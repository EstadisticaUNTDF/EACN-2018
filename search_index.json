[
["index.html", "Estadística Avanzada para Ciencias Naturales Capítulo 1 Reglamento 1.1 Asistencia a clases, participación y evaluación de pares 1.2 Entrega de Ejercicios 1.3 Laboratorios 1.4 Cuestionario de Comprensión 1.5 Parciales 1.6 Cronograma", " Estadística Avanzada para Ciencias Naturales Dr. Luciano Selzer 2018-05-22 Capítulo 1 Reglamento Para aprobar la asignatura el alumno deberá tener obtener una nota de cursada mínina de 60%, aprobar los parciales. La nota se calcula ponderando los siguientes items. Actividad Ponderación Asistencia a clases, participación y evaluación de pares 5% Entrega de Ejercicios 10% Cuestionario de Comprensión 10% Laboratorios en R 15% Parcial 1 30% Parcial 2 30% Total 100% 1.1 Asistencia a clases, participación y evaluación de pares Se espera que los alumnos vayan a clases y participen activamente de la misma, con preguntas y respuestas. La asistencia y participación es una parte pequeña pero no insignificante de la nota de cursada. Si bien puede pedirse a algún alumno en particular que responda una pregunta o resuelva un ejercicio, se espera que sean participativos sin tener que ser llamados. A través del cuatrimestre también se pedirá a los alumnos que completen algunas evaluaciones de pares. Estas serán usadas para asegurar que todos los miembros del equipo contribuyan al éxito del grupo y poder resolver los posibles problemas de manera temprana. 1.2 Entrega de Ejercicios Al principio de cada unidad se harán un pequeño número de ejercicios de forma manual. El objetivo de los problemas es ayudar a desarrollar una comprensión profunda de los temas y preparar a los alumnos para los parciales y el proyecto. Se evaluará la precisión y la completitud. Para recibir la nota deben mostrar todo el trabajo. Se espera que los alumnos colaboren, y se alienta, pero cada uno debe entregar su propio trabajo. Si se detectan copias ambos recibirán 0 para ese grupo de problemas. La nota menor no se tomará en cuenta. 1.3 Laboratorios Cada laboratorio se completará de forma individual. Se puede realizar consultas entre alumnos y se alienta la colaboración. Pero la entrega de informes se debe realizar de forma individual. Es importante que el informe sea completamente reproducible. Se puede asegurar de ello usando el botón knit. Deben entregar el archivo .Rmd y el .html. La nota menor no se tomará en cuenta. 1.4 Cuestionario de Comprensión Al final de la unidad se harán cuestionarios para evaluar la comprensión que tiene el alumno sobre el tema. El mismo se hará en la plataforma de Moodle, y tendrán una semana para realizarla. Luego de ese tiempo tendrán 0 puntos. Se harán 10 preguntas de elección múltiple. La menor puntuación no será tenida en cuenta. 1.5 Parciales La asignatura contará con 2 parciales prácticos (se considera parcial aprobado con el 60% bien desarrollado y si no se cometen errores conceptuales básicos en lo referente a los temas propios de la asignatura), con sus respectivos recuperatorios; como requerimiento para aprobar la cursada. Además de asistir al 80% de las clases prácticas de acuerdo con lo que estipula la Resolución N° 350/14. La asignatura tiene promoción. Para promocionar es necesario aprobar con más del 80% los parciales sin ir a recuperatorio, y tener una nota de cursada mayor al 80%. Y realizar un trabajo final que consiste en poner en práctica los conocimientos adquiridos. Examen Final: El énfasis estará en los conocimientos teóricos y su interpretación con la aplicación. Se tendrá en cuenta la síntesis que alumno realice con los nuevos conceptos adquiridos. Para ello, el examen final consistirá un trabajo final donde los alumnos deberán analizar e interpretar datos provistos por la asignatura y un examen escrito donde deberán saber qué técnicas usar para resolver diferentes problemas. Los exámenes se rendirán en las fechas previstas en el Calendario Académico de la Facultad 1.6 Cronograma Semana Fecha Día Tema 1 13/3/2018 Martes Visualización 1 15/3/2018 Jueves Análisis Reproducible - Manejo de Datos 2 20/3/2018 Martes Planificación de experimentos. ANOVA de un factor- Supuestos del ANOVA. 2 22/3/2018 Jueves ANOVA: Contrastes- Modelo aleatorio 3 27/3/2018 Martes Métodos no paramétricos para comparar varias muestras 3 29/3/2018 Jueves Jueves Santo 4 3/4/2018 Martes Anova de dos factores. 4 5/4/2018 Jueves Modelo aleatorio. Modelo Mixto 5 10/4/2018 Martes Diseños experimentales 5 12/4/2018 Jueves Pruebas no paramétricas: Análisis, para dos muestras, varias muestras y para varias muestras relacionadas 6 17/4/2018 Martes Repaso Regresión lineal simple 6 19/4/2018 Jueves Regresión lineal múltiple 7 24/4/2018 Martes Representación Matricial ANCOVA 7 26/4/2018 Jueves Modelos Lineales generalizados 8 1/5/2018 Martes Día del Trabajo 8 3/5/2018 Jueves Modelos Lineales generalizados 9 8/5/2018 Martes Parcial 9 10/5/2018 Jueves Modelos Lineales generalizados 10 15/5/2018 Martes Análisis Multivariado: Medidas de distancias y similitudes 10 17/5/2018 Jueves Recuperatorio 11 22/5/2018 Martes Análisis Multivariado: Clusters 11 24/5/2018 Jueves Análisis Multivariado: Análisis de Componentes Principales 12 29/5/2018 Martes Análisis Multivariado: Análisis de Coordenadas Principales 12 31/5/2018 Jueves Análisis Multivariado: Análisis de Redundancia y Análisis de correspondencias canónicas 13 5/6/2018 Martes Análisis Multivariado: Análisis discriminante. 13 7/6/2018 Jueves Análisis Multivariado: Análisis discriminante. 14 12/6/2018 Martes Introducción a Inferencia Multimodelo 14 14/6/2018 Jueves Introducción a GAM 15 19/6/2018 Martes Introducción a GAM 15 21/6/2018 Jueves Parcial 16 26/6/2018 Martes Introducción a Estadística Bayesiana 16 28/6/2018 Jueves Recuperatorio "],
["introduccion-a-r-y-rstudio.html", "Capítulo 2 Introducción a R y RStudio 2.1 RStudio 2.2 Análisis Reproducible 2.3 Integrando código", " Capítulo 2 Introducción a R y RStudio En la materia de Estadística Avanzada para Ciencias Naturales vamos a usar métodos estadísticos complejos. Por lo que, en muchos casos, es muy poco práctico realizar los cálculos a mano. Por eso, es necesario usar software estadístico específico para poder trabajar eficientemente. Existen múltiples programas, entre los más conocidos podes mencionar Statistica, Stata, SPSS, SAS, o Infostat, desarrollado en Argentina. Todos ellos son comerciales, y por lo tanto hay que pagar por las licencias de uso. Por otro lado, R es gratis y es usado ampliamente en el mundo. Otra de las ventajas, es la gran comunidad de usuarios y desarrolladores que se ha formado. Lo que hace que esté siendo constantemente actualizado y que las últimas técnicas estadísticas estén, muchas veces, implementadas directamente en R. Sin embargo, una de las dificultades es que hay que escribir comandos para hacer que funcione. Lo que es una desventaja al aprender pero luego se convierte en una ventaja, ya que permite automatizar tareas tediosas y además realizar análisis reproducibles de datos. Otro problema de R es su interfaz muy poco amigable. Por eso se han desarrollado otras interfaces (llamadas entornos de desarrollo integrado o IDE por sus siglas en inglés) que hacen más fácil trabajar con este programa. Hay varias: RStudio, Tinn-R, RKward, etc. Nosotros vamos a usar RStudio por ser la más trabajo tiene encima y está más pulida. Se puede bajar e instalar ambos programas en cualquier computadora. Hay que instalar R descargando desde https://cran.r-project.org/ y seguir las instrucciones del instalador. Para bajar RStudio hay que ir a https://www.rstudio.com/products/rstudio/download/#download y también seguir las instrucciones de instalación. Para las clases tenemos instalado estos programas en un servidor de la Universidad y se puede acceder desde cualquier red (LAN o WiFi) de la sede de Yrigoyen entrando a https://rstudio.untdf.edu.ar. Verán una pantalla de login. Una vez que entren verán una pantalla así. 2.1 RStudio La interfaz de RStudio está dividida en varios paneles, y cada uno tiene varias pestañas. Arriba a la derecha está el espacio de trabajo (Environment), es donde van a aparecer los objetos que creen a medida que trabajan en R. La otra pestaña es el historial (History ), donde quedan guardados todos los comandos que hayan ejecutado. Abajo de estos dos hay un panel con varias pestañas. Archivos (Files), muestra los archivos. Gráficos (Plots) es donde van a aparecer los gráficos que vayamos haciendo. Paquetes (Packages) muestra las librerías que tenemos y sus paquetes instalados y con un tilde los cargados (más adelante vamos a ver que son los paquetes). La ayuda (Help) es donde vamos a poder la ayuda de funciones de R. Y además está las pestaña del Visor (Viewer) que nos muestra una vista de los documentos que creemos. Por el lado izquierdo está la consola, es donde pasa toda la acción. Todo lo que hagamos va a ser escrito como un orden o comando ahí y luego vamos a ver el resultado ahí o si es un gráfico en el panel de gráficos. Cada vez que iniciemos RStudio va a mostrar la consola con un mensaje que indica la versión de R y otros detalles. Debajo de ese mensaje está el prompt. Aquí es donde R espera que se ingresen los comandos. Y para interactuar con R hay que decirle que tiene que hacer. Los comandos y su sintaxis han evolucionado a lo largo de décadas y ahora proveen a los usuarios una manera natural de acceder y procesar datos, aplicar procedimientos estadísticos, etc. Se puede usar R como una calculadora. Podemos poner una cuenta a realizar en el prompt y R nos devolverá el resultado. Por ejemplo, podemos poner: 2 + 2 ## [1] 4 Prueben escribirlo en su consola justo después del “&gt;”. También es posible guardar los resultados en un objeto: x &lt;- 2 + 2 Prueben hacerlo en su consola. En este caso, parece que no pasó nada. No apareció el resultado. Pero si observan en panel del espacio de trabajo verán que hay un nuevo objeto llamado x. Prueben que sucede si escriben x en la consola. 2.2 Análisis Reproducible Una ventaja que tiene R respecto a otros programas estadísticos es que permite reproducir el análisis de los datos. Reproducible significa que a partir de los mismos datos otro analista va a llegar a los mismos resultados. En cambio, replicable es que otro experimentador al repetir el experimento va a llegar a resultados diferentes, que pueden ser o no similares (Figura 2.1) Figura 2.1: Diferencias entre reproducible y replicable. Los programas de interfaz gráfica, que no usan código o no lo proveen, complican la reproducibilidad de los análisis. Esto se debe a que es más complicado de comunicar como se realizó el análisis. La ventaja del código es que queda todo explícito en él. Hay varias formas de trabajar con R. Una es de forma interactiva en la consola. Como cuando pusieron 2 + 2. Esto es muy útil cuando estamos probando si algo funciona. Pero no guardamos el código de esta forma. Aunque, en verdad queda guardado en el orden en que lo ejecutamos en el historial no es útil porque se va sobreescribiendo y va quedando lo que funcionó y lo no que no. Otra forma de usar R es utilizando scripts (archivos con extensión .R). Es indispensable para crear nuevas funciones pero para analizar datos tiene sus desventajas. Ya que, si bien el código se puede comentar anteponiendo # a la línea que queremos comentar, es límitado el formato que podemos usar en el comentario. Además, tendremos que volver a correr el script para ver los resultados si no los guardamos explicitamente en un documento, hoja de cálculo, o imagen. Por último, tenemos los documentos de programación letrada. La programación letrada consiste en mezclar código con texto plano, como en un procesador de texto. En R, hay varias aproximaciones a esto. La que más éxito ha tenido es knitr. To knit es tejer en inglés, lo que hace es “tejer” el documento final con el resultado del código, es decir los análisis que hagamos, y texto explicando que hicimos, porque, y como. Es decir, podemos escribir un paper o informe completo. Para darle formato al texto se usa markdown que permite usar marcas livianas para poner cursivas, negritas o tachado. 2.2.1 Rmarkdown Hay muchas opciones para formatear el texto. La idea detrás de markdown es que se pueda escribir en un procesador de texto sencillo y las marcas sean fáciles de poner y no interrumpan la lectura. Algunos ejemplos: 2.2.1.1 Énfasis *cursiva* **negrita** _cursiva_ __negrita__ 2.2.1.2 Títulos # Título 1 ## Título 2 ### Título 3 2.2.1.3 Listas Lista Desordenada * Item 1 * Item 2 + Item 2a + Item 2b Lista Ordenada 1. Item 1 2. Item 2 3. Item 3 + Item 3a + Item 3b 2.2.1.4 Saltos de línea manuales Termina una línea con dos o más espacios: Las rosas son rojas, las violetas son azules. 2.2.1.5 Vínculos Usa una dirección http simple o agrega un vínculo a una frase: http://example.com [frase vínculada](http://example.com) 2.2.1.6 Imágenes Imágenes en la web o en el mismo directorio de trabajo: ![alt text](http://example.com/logo.png) ![alt text](figures/img.png) Ejercicio 2.1 (Probando markdown) Descarguen un archivo de RMarkdown usando este código en la consola: download.file(&quot;url&quot;, &quot;ejercicio-1.Rmd&quot;) Una vez descargado, abranlo desde el panel Files. Los prácticos en general se harán en un archivo similar a este. En la parte superior encontran el encabezado entre guiones. Ahí deberán poner sus nombres y el nombre del grupo. Ejercicio 2.2 (Personalizando) Cambien en encabezado y pongan sus nombres, el nombre del grupo y la fecha de hoy. Abajo encontraran espacio para ir contestando la preguntas. Una consideración que deben tomar en cuenta es que todo el texto que escriban va a ser considerado como un único párrafo a menos que este separado por una línea en blanco. Por ejemplo, prueben escribir esto en el documento (pueden copiarlo): Mucho antes de que el lector haya llegado a esta parte de mi obra se le habrán ocurrido una multitud de dificultades. Algunas son tan graves, que aun hoy dia apenas puedo reflexionar sobre ellas sin vacilar algo; pero, según mi leal saber y entender, la mayor parte son solo aparentes, y las que son reales no son, creo yo, funestas para mi teoria. Estas dificultades y objeciones pueden clasificarse en los siguientes grupos: 1° Si las especies han descendido de otras especies por suaves gradaciones, ¿por qué no encontramos en todas partes innumerables formas de transicion? ¿Por qué no está toda la naturaleza confusa, en lugar de estar las especies bien definidas según las vemos? 2° ¿Es posible que un animal que tiene, por ejemplo, la conformación y costumbres de un murciélago pueda haber sido formado por modificación de otro animal de costumbres y estructura muy diferentes? ¿Podemos creer que la selecci6n natural pueda producir, de una parte, un órgano insignificante, tal como la cola de la jirafa, que sirve de mosqueador, y, de otra, un órgano tan maravilloso como el ojo? 3° ¿Pueden los instintos adquirirse y modificarse por selección natural? ¿Qué diremos del instinto que lleva a la abeja a hacer celdas y que prácticamente se ha anticipado a los descubrimientos de profundos matemáticos? 4° ¿Cómo podemos explicar que cuando se cruzan las especies son esteriles o producen descendencia esteril, mientras que cuando se cruzan las variedades su fecundidad es sin igual? Luego, hagan clic en el botón Knit que tienen arriba, al lado de un ovillo y una aguja de tejer. El desafío es mantener el formato con cada párrafo separado. La misma consideración se debe tener en cuenta para otros formatos como títulos o listas. Un consejo para ver como va quedando el documento, es tejerlo seguido. Así podremos ver cualquier problema pronto. 2.3 Integrando código En los documentos de RMarkdown se puede integrar bloques de código (de R y otros lenguajes). Para insertar un bloque pueden hacer clic en el botón de “Insert/R” que hay arriba o por el atajo del teclado “Ctrl+Alt+I”. ```{r} Acá va el código ``` Es importante mantener las comillas invertidas tal cual están ya que con ellas se define donde empieza y termina el bloque. Entre las llaves se incluye como se va a ejecutar el código (con R en este caso). También se puede poner nombre al bloque, cosa que es muy recomendable porque sino van a estar nombrados como chunk-#, donde # son números consecutivos. Ahora, imaginen que el chunk-34 de 60 falla. Va a ser un poco tedioso buscarlo, con nombre será más sencillo saber donde estar el fallo. Además se pueden poner otras opciones, como ocultar el código, cambiar el tamaño de figuras, etc. Luego de las llaves, en la línea siguiente, deben introducir el código que quieran ejecutar, siempre teniendo en cuenta de dejar la línea con las comillas invertidas tal cual está. También es buena idea dejar una línea en blanco luego. Ejercicio 2.3 Incluyan un bloque de código y pongan un nombre descriptivo. Luego, escriban una operación matemática simple. Finalmente, tejan el documento. "],
["visualizacion-de-datos.html", "Capítulo 3 Visualización de Datos 3.1 Introducción 3.2 El conjunto de datos mpg 3.3 Gráficos con ggplot 3.4 Mapeando 3.5 Formas geometricas 3.6 Transformaciones Estadísticas 3.7 Ajuste de Posiciones 3.8 Sistemas de Coordenadas 3.9 Personalizando el gráfico", " Capítulo 3 Visualización de Datos Para hacer el tutorial ingresen este código en la consola: download.file(“git.io/informe-visualizacion.Rmd”, destfile = “informe-visualizacion.Rmd”) A continuación abran el archivo informacion-visualizacion.Rmd y completen el infome mientras leen el capítulo. 3.1 Introducción Una de las formas más útiles de visualizar la información es mediante gráficos (aunque si son pocos datos es preferible una tabla). De hecho, el primer paso antes de analizar los datos debe ser hacer un gráfico de los valores que tienen. Un gráfico de dispersión si es bidimensional o un histograma si solo tiene una dimensión. Aunque hay varios sistemas gráficos (base, lattice, ggobi, plotly) vamos a usar ggplot2 por su facilidad de uso y potencia para hacer gráficos complejos a partir de componentes simples. Este paquete sigue una idea que se llama gramática de gráficos propuesta por Wilkinson en donde los gráficos pueden dividirse en cuatro partes: Los datos y como se mapea (aes) esos datos a las diferentes atributos estéticos. Es decir que columna corresponde al eje x, al eje y, forma, color, etc. Las formas geométricas (geom) que representan como se ven los datos. Como puntos, líneas, barras de error, etc. Transformaciones estadísticas de los datos (stats) resumen los datos de forma útil. Por ejemplo, para agregar la media por grupo o una línea de regresión sin haberlos calculado antes. Escalas a las que se mapean los datos (scale). Estas pueden ser escalas de color, forma, etc. Un sistema de coordenadas (coord), que describe como se proyectan estos datos. Por defecto se usa el sistema cartesiano. Pero hay otros disponibles como el polar. Un sistema paneles (facet) que describe como dividir los datos en distintos paneles. Adicionalmente a la gramática, se agrega un sistema de temas que permite modificar la totalidad de elementos que hacen al gráfico como fuentes, líneas de ejes, etc. 3.2 El conjunto de datos mpg En los datos que vienen con el paquete ggplot2 está mpg. Contiene los datos de rendimiento, cilindrada y otros más de algunos modelos de autos. Los datos están una estructura rectangular llamada data frame, cada columna es una variable y cada fila una observación recolectadas por la Agencia de Protección ambiental de EE.UU. library(tidyverse) mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l… f 18 29 p ## 2 audi a4 1.8 1999 4 manual… f 21 29 p ## 3 audi a4 2 2008 4 manual… f 20 31 p ## 4 audi a4 2 2008 4 auto(a… f 21 30 p ## 5 audi a4 2.8 1999 6 auto(l… f 16 26 p ## 6 audi a4 2.8 1999 6 manual… f 18 26 p ## 7 audi a4 3.1 2008 6 auto(a… f 18 27 p ## 8 audi a4 quat… 1.8 1999 4 manual… 4 18 26 p ## 9 audi a4 quat… 1.8 1999 4 auto(l… 4 16 25 p ## 10 audi a4 quat… 2 2008 4 manual… 4 20 28 p ## # ... with 224 more rows, and 1 more variable: class &lt;chr&gt; 3.3 Gráficos con ggplot Podemos hacer un gráfico de la siguiente forma: library(ggplot2) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) Prueben escribiendo el código en la consola. En el gráfico vemos que hay una tendencia a disminuir el rendimiento a medida que aumenta la cilindrada. En la llamada de Hay distintas partes. La llamada a ggplot donde especificamos el nombre de los datos que vamos a usar. Es la que inicializa el gráfico. Pero aquí no especifica nada de como graficarlo. Sin embargo, es necesario empezar siempre por esta función y luego ir agregando capas. Luego agregamos una capa de puntos. Ambos están unidos por un +. Cada vez que deseemos agregar una capa, lo haremos con ese símbolo +. Por otro lado, especificamos el mapeo de las columnas de los datos a las ordenadas y abscisas dentro del argumento mapping. Hemos graficado el tamaño del motor (en litros), displ en las ordenadas y el rendimiento en millas por galón en las abscisas. Siempre que quereamos mapear una columna a alguna parte del gráfico lo hemos de hacer dentro la función aes(), de aestetics que significa ésteticas en inglés. Ejercicio 3.1 a. Prueben correr ggplot(data = mpg). ¿Qué ven? ¿Cuantas columnas y filas hay en mpg? Hagan un gráfico de dispersión entre cty vs hwy Hagan un gráfico de dispersión entre class y fl. ¿Qué ven? ¿Porqué el gráfico no es útil? 3.4 Mapeando En el gráfico anterior vemos que hay unos puntos que no siguen la tendencia general. Aquí están resaltados con rojo. Para saber más el porque de estos puntos podríamos agregar más información al gráfico como por ejemplo el tipo de auto. Una opción es agregar colores. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) Ahora podemos ver, que en general los autos con cilindrada grande son camionetas (pickup) o suv. Y que los que tienen cilindrada grande pero rendimiento mayor son autos deportivos. Agregamos el color mapeando class a la éstetica de color. ggplot le asigna automáticamente un color a cada nivel de class. Y también genera la leyenda apropiada. También podemos mapear el tamaño del punto a class. En este caso recibiremos un warning porque no tiene mucho sentido mapear el tamaño con una variable discreta desordenada. Es decir, que no hay una correspondencia entre el tamaño del punto y la clase. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = class)) ## Warning: Using size for a discrete variable is not advised. También podriamos mapear la clase a la transparencia de los puntos (alpha) o a la forma (shape) # Izquierda ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, alpha = class)) # Derecha ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = class)) Si reproducen el código en sus computadoras verán que que ambos dan advertencias. Así como no tienen mucho sentido mapear el tamaño a algo sin orden intríseco, tampoco lo tiene mapear la transparencia. Por otro lado, noten que en el gráfico de la derecha ¡faltan los puntos de suv! Esto es porque ggplot solo asigna automaticamente hasta 6 símbolos diferentes para los puntos. Si queremos más hay que hacerlo de forma manual. En general, uno mapea una variable a alguna característica del gráfico asociandola dentro de aes(). ggplot se encarga de los detalles de pasar esa asociación a las distintas capas, de generar los niveles apropiados y de hacer la leyenda. De hecho, podemos ver que x e y también son características del gráfico pero en vez de mostrar una leyenda genera las marcas en los ejes. También es posible configurar alguna estética a un valor específico. Como por ejemplo hacer que todos los puntos sean rojos ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = &quot;red&quot;) Acá el color no muestra ninguna información extra. También es posible cambiar el: el color por el nombre que tenga sentido o en hexadecimal. el tamaño de los puntos en mm. la forma del punto según los valores que se muestran acá abajo Figura 3.1: Valores númericos y la forma asociada a cada uno. En R hay 25 formas diferentes. Algunas parecen repetirse pero no es así. Por ejemplo, las formas 0, 15 y 22 son todos cuadrados. Pero las formas del 0-15 tienen el color definido por el borde, usan color para cambiar el color. Del 15 a 18 son formas rellenas que usan fill para cambiar el color del relleno. Y de la forma 21 a 23 son formas con relleno y borde que usan ambas fill y color. Valores númericos y la forma asociada a cada uno. En R hay 25 formas diferentes. Algunas parecen repetirse pero no es así. Por ejemplo, las formas 0, 15 y 22 son todos cuadrados. Pero las formas del 0-15 tienen el color definido por el borde, usan color para cambiar el color. Del 15 a 18 son formas rellenas que usan fill para cambiar el color del relleno. Y de la forma 21 a 23 son formas con relleno y borde que usan ambas fill y color. Ejercicio 3.2 a. ¿Qué está mal con el siguiente código? ¿Porqué no son azules los puntos? Corrijanlo `ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;blue&quot;))` ¿Cuáles son las variables categóricas en mpg? ¿Cuáles son las variables continuas? ¿Cómo puedes ver esta información cuando corres mpg? Pista: escribe ?mpg para leer la documentación del conjunto de datos. O mpg para ver los datos Mapeen una variable continua a color, sizey shape. ¿Cuál es la diferencia en su acción entre variables categóricos y continuas? ¿Qué pasa si mapean la misma variable a varias estéticas? Pista: Por ejemplo class a color y forma de punto. ¿Qué hace la estética stroke? ¿Con qué formas funciona? Pista: usen ?geom_point y busquen el ejemplo dentro de la ayuda. ¿Qué sucede cuando mapean una estética a algo que no sea un nombre de variable como aes(colour = displ &lt; 5)? 3.5 Formas geometricas ¿En que se parecen los gráficos de abajo? Ambos tienen las mismas variables, pero están representados por distintas formas. En el idioma de ggplot cada forma es un geom. Y cada geom es una forma geométrica de representar los datos. Hay muchos geom (más de 30 en el paquete y muchos más en extensiones) y todos empiezan con geom_), por ejemplo: Tabla 3.1: Gráficos comunes con ggplot Gráfico geom Barras geom_col geom_bar Puntos geom_point Cajas y Barras geom_boxplot Histograma geom_histogram Lineas geom_line Barras de Error geom_errorbar Pueden ver más en la ayuda de ggplot en R (usando la pestaña de ayuda o usando help(nombre_de_función) en la consola) o en la documentación online que tiene la ventaja de tener graficados los ejemplos http://ggplot2.tidyverse.org/reference/ . Para hacer los gráficos de arriba usamos: # Izquierda ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) # Derecha ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) Todos los geoms van luego de ggplot y se unen con un +. En ggplot cada forma geométrica es una capa y pueden combinarse varias en un mismo gráfico. Además, todos los geoms tienen un argumento mapping para la estética. Claro que no todos aceptan los mismas argumentos. No tienen sentido ponerle relleno a una línea o cambiar el tipo de línea a un punto. Pero si se puede cambiar el tipo de línea de `geom_smooth: ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv)) Acá geom_smooth separa tres líneas según el valor de drv, que es la tracción. Una línea lisa para las que son 4x4 (4), rayas cortas para tracción delantera (f) y rayas largas para tracción trasera (r). Podemos ver más claramente porque tiene esta forma geom_smooth graficando los puntos de cada grupo: Figura 3.2: Varios geoms pueden usarse en un mismo gráfico. Muchos geoms que resumen la información de varios datos con una sola forma tienen un argumento de éstetica llamado group que agrupa la observaciones que son iguales en una variable. ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, group = drv)) ggplot(data = mpg) + geom_smooth( mapping = aes(x = displ, y = hwy, color = drv), show.legend = FALSE ) Arriba vimos que podiamos usar dos geoms en un mismo gráfico (y podríamos agregar más si quisieramos). Pero al hacerlo hemos duplicado el mapping en los dos geoms: ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv, color = drv)) + geom_point(mapping = aes(x = displ, y = hwy, color = drv)) Podemos evitarlo si ponemos el mapping dentro de la llamada de ggplot: ggplot(data = mpg, mapping= aes(x = displ, y = hwy, linetype = drv, color = drv)) + geom_smooth() + geom_point() Esto funciona porque las capas heredan el mapping de ggplot. Por eso, va a funcionar en todas las capas que pongamos, de forma global. A veces, esto introduce ciertos errores cuando usamos varias fuentes de datos y las variables no están en presentes en todos los conjuntos. Es posible cambiar el mapping de una capa específica y va a ser utilizada solo en esa capa; es decir, de forma de forma local. ggplot(data = mpg, mapping= aes(x = displ, y = hwy)) + geom_smooth() + geom_point(mapping = aes(color = class)) O también podemos definir otro conjunto de datos para el geom: ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(mapping = aes(color = class)) + geom_smooth(data = filter(mpg, class == &quot;subcompact&quot;), se = FALSE) Todavía no vieron filter, pero ya lo verán más adelante. Ejercicio 3.3 a. ¿Cuál geom usarias para dibujar un gráfico de líneas? ¿Un boxplot? ¿Un histograma? ¿Un gráfico de áreas? Corre este código mentalmente y predice cómo se verá el gráfico. Luego, ejecuta el código y comprueba lo que pensaste. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth(se = FALSE) ¿Qué crees que hace show.legend = FALSE? ¿Qué sucede si lo quitas? ¿Por qué crees que lo usé antes en el capítulo? ggplot(data = mpg) + geom_smooth( mapping = aes(x = displ, y = hwy, color = drv), show.legend = FALSE ) ¿Qué hace el argumento se de geom_smooth()? ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth(se = TRUE) ¿Serán diferentes estos dos gráficos? ¿Por qué sí o no? ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() ggplot() + geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy)) 3.6 Transformaciones Estadísticas Pensemos en los gráficos de barras. En ggplot2 se hacen con geom_bar. A primera vista los gráficos de barras parecen simples. En este caso estamos graficando datos de diamantes, del conjunto de datos diamonds que contiene cerca 54000 datos. En el gráfico vemos que hay muchos más diamantes de cortes buenos que regulares. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) En el eje x está puesto el corte, y en el eje y está puesto la cuenta (frecuencia) de cada uno. Pero si vemos el conjunto de datos veremos que esta última ¡no está! Entonces ¿De dónde salió? Algunos geoms grafican los datos puros pero otros aplican transformaciones estadísticas a los datos y crean nuevas variables: Los gráficos de barras, histograms y polígonos de frecuencia cuentan y juntan los datos. geom_smooth usa modelos para mostrar las tendencias de los datos. geom_boxplot crea un sumario de estadísticos robustos para mostrar los datos. El algoritmo usado para calcular las nuevas variables se llama stat. Abajo vemos como funciona stat_count Empieza con los datos ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 geom_bar calcula las nuevas variables usando el stat count que devuelve un nuevo data.frame ## # A tibble: 5 x 3 ## cut count prop ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Fair 1610 1 ## 2 Good 4906 1 ## 3 Very Good 12082 1 ## 4 Premium 13791 1 ## 5 Ideal 21551 1 geom_bar luego usa esos datos para graficar: Podés saber que stat usa cada geom usando la ayuda. Por ejemplo, ?geom_bar usa por defecto stat_count y stat_count usa por defecto geom_bar para mostrar sus resultados y ambos están descriptos en la misma página de ayuda. Podemos ver que es calculado en la sección Computed Variables. Es posible intercambiar geom por su stat. Por ejemplo: ggplot(data = diamonds) + stat_count(mapping = aes(x = cut)) Esto funciona porque cada stat tiene un geom por defecto y cada geom tiene un stat. Lo que significa que podes usar cada geom sin preocuparte por las transformaciones subyacentes. Hay veces que querrás cambiar los valores por defecto: Cuando tengas las variables precomputadas y desees graficarlas. En el código de abajo cambio el stat de geom_bar por stat_identity (identidad). Esto me permite graficar la altura de la variable y a algún valor del conjunto de datos. demo &lt;- tribble( ~cut, ~freq, &quot;Fair&quot;, 1610, &quot;Good&quot;, 4906, &quot;Very Good&quot;, 12082, &quot;Premium&quot;, 13791, &quot;Ideal&quot;, 21551 ) ggplot(data = demo) + geom_bar(mapping = aes(x = cut, y = freq), stat = &quot;identity&quot;) No te preocupes si no entiendes que hace tribble o &lt;-. Todavía no lo hemos visto pero quizá puedas entender que hacen por su contexto Muchos stats computan varias variables y quizás quieras mostrar otra. Abajo, en lugar de graficar la frecuencia o cuenta, grafico la proporción o frecuencia relativa. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop.., group = 1)) Quizás quieras llamar la atención sobre ciertos medidas de resumen que has calculado. Puedes hacer esto con stat_summary. ggplot(data = diamonds) + stat_summary( mapping = aes(x = cut, y = depth), fun.y = median, fun.ymax = max, fun.ymin = min ) Ejercicio 3.4 a. ¿Cuál es el geom asociado por defecto a stat_summary? ¿Cómo podrías reescribir el gráfico anterior para usar la función geom en vez la función stat? ggplot(data = diamonds) + stat_summary( mapping = aes(x = cut, y = depth), fun.y = median, fun.ymax = max, fun.ymin = min ) ¿Qué hace geom_col? ¿En que se diferencia de geom_bar? La mayoría de los geoms y stats vienen en pares que casi siempre son usados juntos. Lee la documentación y haz una lista de todos los pares. ¿Qué tienen en común? ¿Qué variables computa stat_smooth() ¿Qué parámetros controlan su comportamiento? En el gráfico de proprociones, necesitamos poner group = 1. ¿Por qué? En otras palabras ¿Por qué los siguientes gráficos tienen problemas? ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, y = ..prop..)) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = color, y = ..prop..)) 3.7 Ajuste de Posiciones El gráfico anterior revela algo interesante de los gráficos de barras. Tienen relleno (fill) y tienen color. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity)) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, color = clarity)) Es más claro trabajar con el relleno porque es más visible en el gráfico. Pero vemos que las distintas barras están apiladas, lo que dificulta la comparación. Se debe a que la posición de barras es apilada (stack) por defecto. Podemos cambiarla modificando el argumento position de geom_bar(). Entre las otras posiciones que podemos elegir están identidad (identity), esquivar (dodge) y relleno (fill) . Identidad hace que las barras (o otro geoms) caigan unas encimas de otras. No es muy útil porque las barras se superponen y hace muy difícil la interpretación. Se puede mejorar agregando transparencia y usando color y no relleno, pero es complicado de interpetar si la barras se superponen o están apiladas. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, color = clarity), position = &quot;identity&quot;) Esquivar es quizás la más útil junto con relleno. Hace que las barras estén una al lado de la otra. Lo que hace que sea sencillo comparar la altura de estas. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;dodge&quot;) Recién vimos que que con esquivar podemos poner las barras una al lado de otra. Pero, más allá de comparar la cantidad de diamantes en cada uno, ya que la cantidad es muy diferente en cada corte resulta más útil comparar las proporciones de cada una de las claridades. Relleno funciona de forma similar al apilado, pero estadariza cada columna a longitud uno. Entonces se ve las proporciones o frecuencias relativas de cada uno de los niveles de clarity. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;fill&quot;) Hay otros ajustes de posiciones que no son útiles para los gráficos de barras pero son muy útiles para los gráficos de puntos. En el gráfico de dispersión entre displ y hwy hay menos puntos que el total 31 vs 234. Muchos puntos se superponen, por eso vemos menos. Podemos evitarlo añadiendo un poco de movimiento aleatorio a cada punto. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), position = &quot;jitter&quot;) Si bien el gráfico no va a ser exacto, muestra más información que en el caso donde se superponen los puntos. Podes obtener más información el ayuda de cada uno: ?position_dodge, ?position_identity, ?position_fill, ?position_stack, ?position_jitter Ejercicio 3.5 a. ¿Cómo podrías mejorar el siguiente gráfico? ggplot(data = mpg) + geom_point(mapping = aes(x = hwy, y = cty)) ¿Cómo se controla el grado de separación en position_dodge? ¿Cómo se podría controlar el grado de ruido en position_jitter? 3.8 Sistemas de Coordenadas Hasta ahora estuvimos graficando en un sistema de coordenadas cartesianas. Pero es posible cambiarlo, por ejemplo intercambiando el eje x e y. ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() En el primer caso las etiquetas del eje x se superponen, pero en el segundo es fácil verlas. No es la única forma de solucionar este problema. También es posible cambiar el ángulo de las etiquetas para que no se superpongan. Otras veces es mejor reemplazar las coordenadas cartesianas por coordenadas geográficas. arg &lt;- map_data(&quot;world&quot;, region = &quot;Argentina&quot;) ggplot(data = arg, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = &quot;white&quot;, color = &quot;black&quot;) ggplot(data = arg, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = &quot;white&quot;, color = &quot;black&quot;) coord_quickmap() Esto evita que el mapa se deforme, ya que los grados de longitud no miden lo mismo en todas las latitudes. Si van a hacer muchos mapas les recomiendo que vean la extesión ggmap que tiene muchas utilidades para hacer mejores mapas. También existen la coordenadas polares. Un gráfico de torta, que les recomiendo que no lo usen por los problemas de percepción que tiene, es un gráfico de barras apiladas en coordernadas polares. cxc &lt;- ggplot(mtcars, aes(x = factor(cyl))) + geom_bar(width = 1, colour = &quot;black&quot;) cxc + coord_polar() Ejercicio 3.6 a. Inviertan los ejes de un gráfico de barras. ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;dodge&quot;) ¿Cuál es la diferencia entre coord_quickmap() y coord_map() a.¿Para qué sirve la función labs()? 3.9 Personalizando el gráfico Hay varias maneras de personalizar los gráficos. Por un lado, las estéticas pueden ser personalizadas cambiando las distintas scales (escalas). Para cambiar el eje x se usa scale_x_* donde * es el tipo de dato que tiene el eje: si es númerico se usa continuous y si es categórico se usa discrete. Se pueden cambiar muchas cosas: el título del eje (name), el lugar de las marcas (breaks), las etiquetas de las marcas (labels), y muchas más opciones. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy))+ scale_x_continuous(name = &quot;Cilindrada (l)&quot;, breaks = 1:7, labels = c(&quot;uno&quot;, &quot;dos&quot;, &quot;tres&quot;, &quot;cuatro&quot;, &quot;cinco&quot;, &quot;seis&quot;, &quot;siete&quot;)) Un atajo para modificar los nombres de los ejes es usar la función labs(), pero solo se pueden modificar los nombres de los ejes y nada más. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy))+ labs(x = &quot;Cilindrada (l)&quot;, y = &quot;Millas por galón en Autopista&quot;) También se puede modificar los colores que se asignan. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class))+ scale_color_brewer(&quot;Clase&quot;) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class))+ scale_color_brewer(&quot;Clase&quot;, palette = &quot;RdYlBu&quot;) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class))+ scale_color_viridis_d(&quot;Clase&quot;) Hay muchas más opciones disponibles, ya que como dice el dicho: “Para gustos, los colores”. Si quieren conocerlas te recomiendo que lean la ayuda de cada una o visiten el sitio de ggplot2. Vemos que hay patrón común con las escalas, todas empiezan por scale, luego sigue por lo que se quiere modificar: el eje, x o y; el color, color; relleno, fill; la forma, shape; el tipo de línea linetype, etc. Cada uno de las estéticas tiene su escale. Luego, salvo alguna excepción, siguen por el tipo de dato o en el caso de los colores por el método de creación del color. Vale la pena agregar que cada escale tiene su versión manual para un control total de la apariencia. Por otro lado están los elementos del gráfico que modifican la apariencia general del gráfico. El tipo y tamaño de letra, el color del fondo, el grosor de la líneas de los ejes, la dirección de marcas, la dirección del texto, y ¡todo lo demás!. Todo esto está unido a lo que es el tema (theme) del gráfico. Se pueden guardar las modificaciones para usarla facilmente y ya vienen algunas opciones en ggplot y hay más en el paquete ggthemes y otros. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class)) + theme_classic() Para modificar algún elemento en particular usamos la función theme() al final del gráfico. Dentro de la llamada a theme modificamos el argumento que queremos cambiar usando la función element_*(). ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = fl)) + theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) Sí, es bastante complicado. Pero por suerte se puede guardar y reutilizar. x_45 &lt;- theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = fl)) + x_45 También es posible cambiar la posición de la leyenda o eliminarla completamente. ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = fl)) + theme(legend.position = &quot;bottom&quot;) ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = fl)) + theme(legend.position = &quot;none&quot;) Ejercicio 3.7 a. ¿Qué sucede si modificas algún elemento particular del gráfico con theme() y luego aplicas un tema en particular como theme_dark() Pon la leyenda arriba. ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = fl)) + theme(legend.position = &quot;&quot;) Cambia el tipo de forma a puntos abiertos ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class)) + scale_ #completar "],
["manejo-de-datos.html", "Capítulo 4 Manejo de datos 4.1 Seleccionando datos 4.2 Seleccionando columnas 4.3 Agregando columnas 4.4 Operaciones por grupos 4.5 Formato Ancho y Formato Largo 4.6 Por su cuenta", " Capítulo 4 Manejo de datos Antes de comenzar bajen el archivo donde realizarán su informe reproducible. En la consola copien este código: download.file(url = &quot;git.io/informe-manejo&quot;, destfile = &quot;informe-manejo-de-datos.Rmd&quot;) Pueden abrirlo desde la pestaña de archivos, a la derecha. Cambien el nombre por el suyo en el encabezado y mientras leen este capítulo respondan las preguntas. Una parte muy importante del análisis de datos, es el manejo de ellos. Como seleccionar columnas, filtrar datos, y realizar operaciones sobre ellos. Vamos a usar el paquete dplyr y tidyr para el manejo. Los paquetes extienden la funcionalidad de R agregando nuevas funciones. library(&quot;dplyr&quot;) load(&quot;data/nombres-1980-1999.RData&quot;) Revisemos el código. Con library(&quot;dplyr&quot;) cargamos el paquete dplyr. Luego, leemos el archivo que contiene los datos y le asignamos el nombre nombres. Si no le asignasemos ningún nombre, se leerían los datos, imprimiendose en la consola y luego se borrarían de la memoría. Para revisar su contenido podemos escribir el nombre del objeto o usar la función glimpse glimpse(nombres) Ejercicio 4.1 Escriban el nombre del objeto o usen la función glimpse para ver que tiene dentro el objeto nombres. ¿Cuantas columnas tiene y como se llaman? ¿Que tipo de dato tiene cada columna? En R existen diversos tipos de dato, en estos datos solo hay 2: entero (integer) y carácter (character). El primero son números enteros y el segundo es texto. Con el primero se puede hacer operaciones matemáticas y con el segundo otro tipo de operaciones, pero no matemáticas. Es importante comprobar que los tipos de datos se correspondan con lo que esperamos. Si no los resultados pueden no ser los correctos o dar errores. Por ejemplo, el tipo de dato númerico puede ser leído como chr y no podremos calcular la media. 4.1 Seleccionando datos Muchas veces solo nos interesa un subconjunto de datos. Una forma de seleccionar datos es usando la función filter(). nombres %&gt;% filter(nombre == &quot;Luciano&quot;) %&gt;% filter(anio == 1984) Acá empezamos a ver varias cosas nuevas. Primero tenemos el símbolo %&gt;% conocido en inglés como pipe, la traducción más correcta al español es tubo. Lo que hace este símbolo es enviar la salida de la operación a la izquierda a la función de la derecha. Prueben poner cada comando en orden y ver cual es la salida. Esto es: nombres Luego, nombres %&gt;% filter(nombre == &quot;Luciano&quot;) La función filter() filtra un conjunto de datos según los valores de la columna/s que seleccionemos cuyos valores sean igual a Luciano en este caso. Y luego filtramos la columna anio solo los años que sean iguales a 1984. Ejercicio 4.2 Prueben cambiar el nombre por el suyo y el año por su año de nacimiento. El operador que usamos para la igualdad es ==. Este operador, de igualdad, es parte de la familia de operadores lógicos, o booleanos en terminología de ciencias de la información. Son lógicos porque van a comparar valores y dar como resultado verdadero (TRUE) o falso (FALSE). En la Tabla 4.1 podemos ver la lista de operadores lógicos. Tabla 4.1: Operadores Lógicos en R. Operador Descripción &lt; menor que &lt;= menor o igual que &gt; mayor que &gt;= mayor o igual que == exactamente igual a != no igual a !x no x x | y x O y x &amp; y x E y isTRUE(x) comprueba si x es verdadero Los primeros cinco son bastante sencillos y los han estado usando desde la primaria. Así que vamos a explicar en más profundidad los otros. El símbolo != va a devolver TRUE cuando los valores sean diferentes al que pusimos. Por ejemplo: # x una secuencia de 1 a 10 x &lt;- 1:10 # Todos los valores distintos a 5 x != 5 ## [1] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE Ejercicio 4.3 (Nombres comunes) ¿Cómo filtrarían los nombres raros excluyéndolos del conjunto de datos? Guarden el resultado como nombres_comunes y calculen el total por nombre. Nota: Por coherencia, definamos nombres raros como los que son menos de 100. Otro operador muy útil es el de negación ! que invierte las comparaciones, convierte los falsos en verdaderos y los verdaderos en falsos. Siguiendo nuestro ejemplo: !x != 5 ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE Es un ejemplo trivial, que podría haber sido resuelto más sencillamente usando ==. Pero es muy útil cuando queremos seleccionar todos los datos que no cumplan un conjuto de condiciones. Lo que nos lleva al operador | (O) y el operador &amp; (Y). El primero va a devolver verdadero cuando al menos uno de los valores sea verdadero. Por ejemplo: TRUE | TRUE ## [1] TRUE TRUE | FALSE ## [1] TRUE FALSE | TRUE ## [1] TRUE FALSE | FALSE ## [1] FALSE Por otro lado, el operador lógico Y &amp; solo devuelve verdadero cuando ambos valores son verdaderos. TRUE &amp; TRUE ## [1] TRUE TRUE &amp; FALSE ## [1] FALSE FALSE &amp; TRUE ## [1] FALSE FALSE &amp; FALSE ## [1] FALSE Los operadores se evalúan en el orden que aparecen a menos que haya paréntesis, entonces se evalúa primero dentro del paréntesis y luego fuera. Ejercicio 4.4 ¿Qué resultado darán las siguientes evaluaciones? Piensen que resultado tendría que dar y luego comprueben lo que piensan con lo que les devuelve R. TRUE | FALSE | TRUE TRUE | FALSE &amp; TRUE TRUE | (FALSE &amp; TRUE) TRUE != FALSE &amp; TRUE !(TRUE | FALSE) &amp; TRUE Estos dos últimos operadores son muy importantes porque nos permiten comprobar distintas condiciones. Por ejemplo, no hay un operador para seleccionar todos los valores entre a y b (siendo a y b dos números cualesquiera). Podemos hacerlo combinando por un lado, x &gt; a y x &lt; b ¿Y cómo debemos combinar estas dos comparaciones? ¿Usando el operador &amp; o el |? Queremos los valores que cumplen con ambas condiciones, que sean mayores que a y menores que b, por lo tanto debemos usar el operador &amp;. # Si a = 3 y b = 6 ( x &gt; 3 ) &amp; ( x &lt; 6) ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE Estos valores corresponden a la posición de los valores que cumplen o no con la condición. Usando corchetes [] podemos seleccionar solo los verdaderos x[( x &gt; 3 ) &amp; ( x &lt; 6)] ## [1] 4 5 La función filter() hace algo similar para conjuntos de datos (data.frames o tibbles). Ejercicio 4.5 Anteriormente usamos dos operaciones de filter() para seleccionar el nombre y el año. Pero es posible usar solo una con los operadores lógicos que vimos. Intenten hacerlo. Finalmente está isTRUE() que devuelve TRUE cuando el objeto es TRUE lo que suena bastante obvio. Pero es parte de una familia que permite comprobar si un objeto es del tipo esperado. Por ejempo: is.numeric() comprueba que el objeto es un vector con algún tipo de número. Otra forma de seleccionar datos es por posición. Es decir, seleccionar los primeras diez filas: nombres_comunes %&gt;% slice(1:10) ## # A tibble: 10 x 3 ## nombre anio cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Aaron 2012 152 ## 2 Aaron 2013 167 ## 3 Aaron 2014 200 ## 4 Abel 1982 102 ## 5 Abel 1989 103 ## 6 Abel 1990 102 ## 7 Abigail 1991 132 ## 8 Abigail 1992 120 ## 9 Abigail 1993 165 ## 10 Abigail 1994 198 O seleccionar las primeras 10 filas que corresponden números primos: nombres_comunes %&gt;% slice(c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29)) ## # A tibble: 10 x 3 ## nombre anio cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Aaron 2013 167 ## 2 Aaron 2014 200 ## 3 Abel 1989 103 ## 4 Abigail 1991 132 ## 5 Abigail 1995 182 ## 6 Abigail 1997 167 ## 7 Abigail 2011 152 ## 8 Abigail 2013 276 ## 9 Abril 1996 235 ## 10 Abril 2012 534 También es posible eliminar las filas según posición: nombres_comunes %&gt;% slice(-(1:10)) #Tengan en cuenta los parentesis extra ## # A tibble: 6,532 x 3 ## nombre anio cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Abigail 1995 182 ## 2 Abigail 1996 159 ## 3 Abigail 1997 167 ## 4 Abigail 1998 210 ## 5 Abigail 1999 174 ## 6 Abigail 2010 171 ## 7 Abigail 2011 152 ## 8 Abigail 2012 272 ## 9 Abigail 2013 276 ## 10 Abigail 2014 278 ## # ... with 6,522 more rows Ejercicio 4.6 1. ¿Qué sucede si olvidan los paréntesis en el código de arriba? 2. Seleccionen las últimas 10 filas. 4.2 Seleccionando columnas La función para seleccionar columnas es seletc(). Hay muchas formas de seleccionar columnas. La más obvia es por nombre de la columna: nombres_comunes %&gt;% select(nombre, cantidad) ## # A tibble: 6,542 x 2 ## nombre cantidad ## &lt;chr&gt; &lt;int&gt; ## 1 Aaron 152 ## 2 Aaron 167 ## 3 Aaron 200 ## 4 Abel 102 ## 5 Abel 103 ## 6 Abel 102 ## 7 Abigail 132 ## 8 Abigail 120 ## 9 Abigail 165 ## 10 Abigail 198 ## # ... with 6,532 more rows También es posible seleccionar varias columnas usando secuencias: nombres_comunes %&gt;% select(nombre:cantidad) ## # A tibble: 6,542 x 3 ## nombre anio cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Aaron 2012 152 ## 2 Aaron 2013 167 ## 3 Aaron 2014 200 ## 4 Abel 1982 102 ## 5 Abel 1989 103 ## 6 Abel 1990 102 ## 7 Abigail 1991 132 ## 8 Abigail 1992 120 ## 9 Abigail 1993 165 ## 10 Abigail 1994 198 ## # ... with 6,532 more rows De la misma forma se puede eliminar columnas usando el signo -. nombres_comunes %&gt;% select(-anio) ## # A tibble: 6,542 x 2 ## nombre cantidad ## &lt;chr&gt; &lt;int&gt; ## 1 Aaron 152 ## 2 Aaron 167 ## 3 Aaron 200 ## 4 Abel 102 ## 5 Abel 103 ## 6 Abel 102 ## 7 Abigail 132 ## 8 Abigail 120 ## 9 Abigail 165 ## 10 Abigail 198 ## # ... with 6,532 more rows Se pueden renombrar columnas nombres_comunes %&gt;% select(año = anio) ## # A tibble: 6,542 x 1 ## año ## &lt;int&gt; ## 1 2012 ## 2 2013 ## 3 2014 ## 4 1982 ## 5 1989 ## 6 1990 ## 7 1991 ## 8 1992 ## 9 1993 ## 10 1994 ## # ... with 6,532 more rows Pero se eliminan las no seleccionadas. Se puede renombrar sin tener que seleccionar el resto usando la función rename nombres_comunes %&gt;% select(año = anio) ## # A tibble: 6,542 x 1 ## año ## &lt;int&gt; ## 1 2012 ## 2 2013 ## 3 2014 ## 4 1982 ## 5 1989 ## 6 1990 ## 7 1991 ## 8 1992 ## 9 1993 ## 10 1994 ## # ... with 6,532 more rows Hay muchas más formas de seleccionar columnas, pueden referirse a la ayuda ?select, ?select_at y también a este excelente [tutorial][https://suzan.rbind.io/2018/01/dplyr-tutorial-1/] (en inglés). 4.3 Agregando columnas Otra operación muy común es agregar nuevas columnas o variables. Por ejemplo al transformar los datos es siempre mala idea sobreescribir los datos originales. Para esta operación sirve la función mutate(). Dado un data frame computa una valor para cada fila. Por ejemplo: nombres %&gt;% mutate(log_cantidad = log10(cantidad)) ## # A tibble: 121,926 x 4 ## nombre anio cantidad log_cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Aaaraon 2013 1 0 ## 2 Aadil 1992 1 0 ## 3 Aakash 1985 2 0.301 ## 4 Aaminah 2011 1 0 ## 5 Aanisa 2013 1 0 ## 6 Aaran 1988 2 0.301 ## 7 Aarav 2011 2 0.301 ## 8 Aariel 1985 1 0 ## 9 Aaron 1980 2 0.301 ## 10 Aaron 1981 11 1.04 ## # ... with 121,916 more rows Cualquier operación que funcione con vectores funciona con mutate(). También funciona se pueden modificar columnas si el nombre que asignamos ya está usado dentro de nuestro data frame. 4.4 Operaciones por grupos Muchas veces van a necesitar calcular por grupos: la suma, media, varianza, etc. Por ejemplo, calcular el número total de personas con cada nombre. Podrían hacerlo de esta forma: nombres_comunes %&gt;% filter(nombre == &quot;Luciano&quot;) %&gt;% summarise(total = sum(cantidad)) ## # A tibble: 1 x 1 ## total ## &lt;int&gt; ## 1 13244 Y repetirlo cambiando el nombre para cada uno de los nombres. Por su puesto, esta forma de hacer las cosas es muy incómoda y propensa a errores. Hay una forma más fácil y es usando group_by(). Un ejemplo: # No intenten hacerlo en sus computadoras # Los datos tienen más de 3 millones de registros y va a tomar un tiempo nombres_comunes %&gt;% group_by(nombre) %&gt;% summarise(total = sum(cantidad)) ## # A tibble: 561 x 2 ## nombre total ## &lt;chr&gt; &lt;int&gt; ## 1 Aaron 519 ## 2 Abel 307 ## 3 Abigail 2656 ## 4 Abril 4163 ## 5 Adolfo 688 ## 6 Adrian 4186 ## 7 Adriana 2734 ## 8 Agostina 10925 ## 9 Agustin 38169 ## 10 Agustina 34866 ## # ... with 551 more rows Como pueden ver estos datos distan bastante de estar limpios ya que hay muchos errores de entrada de datos, como nombres todo en mayúsculas, versiones del mismo nombre con tilde y sin tilde, etc. Para evitar todo ese “ruido”, podríamos filtrar los nombres raros que son mayoría de las entradas. Además de usar la función summarise() se puede usar la función mutate que va a hacer que queden la misma cantidad de casos. Por ejemplo, calcular el número acumulado de personas con el mismo nombre en a través de los años: nombres_comunes %&gt;% group_by(nombre) %&gt;% arrange(anio, .by_group = TRUE) %&gt;% mutate(acumulado = cumsum(cantidad)) ## # A tibble: 6,542 x 4 ## # Groups: nombre [561] ## nombre anio cantidad acumulado ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Aaron 2012 152 152 ## 2 Aaron 2013 167 319 ## 3 Aaron 2014 200 519 ## 4 Abel 1982 102 102 ## 5 Abel 1989 103 205 ## 6 Abel 1990 102 307 ## 7 Abigail 1991 132 132 ## 8 Abigail 1992 120 252 ## 9 Abigail 1993 165 417 ## 10 Abigail 1994 198 615 ## # ... with 6,532 more rows Acá hay una función nueva, arrange(). Lo que hace. Esta función ordena de manera creciente (0-9, a-z) un conjunto de datos. Por ejemplo: nombres_comunes %&gt;% arrange(cantidad) ## # A tibble: 6,542 x 3 ## nombre anio cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Adolfo 1981 101 ## 2 Ailin 1993 101 ## 3 Alexia 2014 101 ## 4 Amparo 1999 101 ## 5 Anabella 1980 101 ## 6 Armando 1993 101 ## 7 Celina 1985 101 ## 8 Constanza 1983 101 ## 9 Dolores 1987 101 ## 10 Enrique 1983 101 ## # ... with 6,532 more rows Si queremos que sea decreciente (9-0, z-a), hay que agregar la función desc(). Por ejemplo: nombres_comunes %&gt;% arrange(desc(cantidad)) ## # A tibble: 6,542 x 3 ## nombre anio cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Benjamin 2013 4964 ## 2 Benjamin 2012 4726 ## 3 Camila 1994 4326 ## 4 Benjamin 2014 4286 ## 5 Isabella 2013 3589 ## 6 Martina 2014 3566 ## 7 Isabella 2014 3547 ## 8 Camila 1995 3539 ## 9 Martina 2013 3535 ## 10 Isabella 2012 3398 ## # ... with 6,532 more rows También se pueden poner varios criterios para que ordene según ellos. Por ejemplo, por cantidad y luego por orden alfabético. nombres_comunes %&gt;% arrange(cantidad, nombre) ## # A tibble: 6,542 x 3 ## nombre anio cantidad ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Adolfo 1981 101 ## 2 Ailin 1993 101 ## 3 Alexia 2014 101 ## 4 Amparo 1999 101 ## 5 Anabella 1980 101 ## 6 Armando 1993 101 ## 7 Celina 1985 101 ## 8 Constanza 1983 101 ## 9 Dolores 1987 101 ## 10 Enrique 1983 101 ## # ... with 6,532 more rows Ejercicio 4.7 (Orden de totales) Ordenen el resultado del total de nombres que calcularon en el ejercicio 4.3 4.5 Formato Ancho y Formato Largo Los datos en general vienen en uno de dos formatos: Formato Ancho: cada fila se corresponde a varias observaciones, parte de la información está en el nombre de las columnas. Formato Largo: cada fila corresponde a una única observación. Tabla 4.2: Ejemplo de formato ancho. Cada fila corresponde a un individuo denotado por el nombre y cada columna corresponde al puntaje obtenido en una prueba bajo distintos tiempos. Name 50 100 150 200 250 300 350 Carla 1.2 1.8 2.2 2.3 3 2.5 1.8 Mace 1.5 1.1 1.9 2 3.6 3 2.5 Lea 1.7 1.6 2.3 2.7 2.6 2.2 2.6 Karen 1.3 1.7 1.9 2.2 3.2 1.5 1.9 La tabla anterior es un ejemplo de formato ancho. Es cómoda para leer para nosotros pero no es cómoda para trabajar para las computadoras. Por ejemplo, ¿Cómo hacen para indicar que columna es la variable independiente y cual es la de respuesta? No se puede porque la variable indepediente es ¡el nombre de la columna! La forma para poder graficarlo, o analizarlo facilmente es poner esto datos en formato largo. Así quedará una columna para el nombre, otra con los tiempos y otra con el puntaje. Tabla 4.3: Ejemplo de formato largo. Cada fila corresponde a un observación denotado por el nombre, el tiempo y el puntaje. Name time score Carla 50 1.2 Mace 50 1.5 Lea 50 1.7 Karen 50 1.3 Carla 100 1.8 Mace 100 1.1 Lea 100 1.6 Karen 100 1.7 Carla 150 2.2 Mace 150 1.9 Lea 150 2.3 Karen 150 1.9 Carla 200 2.3 Mace 200 2 Lea 200 2.7 Karen 200 2.2 Carla 250 3 Mace 250 3.6 Lea 250 2.6 Karen 250 3.2 Carla 300 2.5 Mace 300 3 Lea 300 2.2 Karen 300 1.5 Carla 350 1.8 Mace 350 2.5 Lea 350 2.6 Karen 350 1.9 De esta forma, será fácil indicar que columna corresponde al eje de las ordenadas, cual al de las abscisas para hacer un gráfico o cual es la variable independiente y cual la dependiente en una regresión. Hay una función que nos permite llevar los datos en formato ancho a formato largo: gather() (recoger). Tiene tres argumntos: data el objeto, key el nombre de la nueva columna que contendrá la identificación del dato, es decir los viejos nombres de columna, y value la nueva columna que contendrá los valores. Por ejemplo: gather(data = race, key = time, value = score) ## Warning: attributes are not identical across measure variables; ## they will be dropped ## time score ## 1 Name Carla ## 2 Name Mace ## 3 Name Lea ## 4 Name Karen ## 5 50 1.2 ## 6 50 1.5 ## 7 50 1.7 ## 8 50 1.3 ## 9 100 1.8 ## 10 100 1.1 ## 11 100 1.6 ## 12 100 1.7 ## 13 150 2.2 ## 14 150 1.9 ## 15 150 2.3 ## 16 150 1.9 ## 17 200 2.3 ## 18 200 2 ## 19 200 2.7 ## 20 200 2.2 ## 21 250 3 ## 22 250 3.6 ## 23 250 2.6 ## 24 250 3.2 ## 25 300 2.5 ## 26 300 3 ## 27 300 2.2 ## 28 300 1.5 ## 29 350 1.8 ## 30 350 2.5 ## 31 350 2.6 ## 32 350 1.9 ¡Pero que ha pasado aquí! No es el mismo resultado que en la Tabla 4.3. Por defecto, la función gather() recoge todas las columnas del objeto. Para evitar que lo haga con todas hay que indicar con un signo menos. race_largo &lt;- gather(data = race, key = time, value = score, -Name) race_largo ## Name time score ## 1 Carla 50 1.2 ## 2 Mace 50 1.5 ## 3 Lea 50 1.7 ## 4 Karen 50 1.3 ## 5 Carla 100 1.8 ## 6 Mace 100 1.1 ## 7 Lea 100 1.6 ## 8 Karen 100 1.7 ## 9 Carla 150 2.2 ## 10 Mace 150 1.9 ## 11 Lea 150 2.3 ## 12 Karen 150 1.9 ## 13 Carla 200 2.3 ## 14 Mace 200 2.0 ## 15 Lea 200 2.7 ## 16 Karen 200 2.2 ## 17 Carla 250 3.0 ## 18 Mace 250 3.6 ## 19 Lea 250 2.6 ## 20 Karen 250 3.2 ## 21 Carla 300 2.5 ## 22 Mace 300 3.0 ## 23 Lea 300 2.2 ## 24 Karen 300 1.5 ## 25 Carla 350 1.8 ## 26 Mace 350 2.5 ## 27 Lea 350 2.6 ## 28 Karen 350 1.9 También funciona indicar cuales son las columnas que debe recoger escribiendo su nombre. gather(data = race, key = time, value = score, `50`, `100`, `150`, `200`, `250`, `300`, `350`) ## Name time score ## 1 Carla 50 1.2 ## 2 Mace 50 1.5 ## 3 Lea 50 1.7 ## 4 Karen 50 1.3 ## 5 Carla 100 1.8 ## 6 Mace 100 1.1 ## 7 Lea 100 1.6 ## 8 Karen 100 1.7 ## 9 Carla 150 2.2 ## 10 Mace 150 1.9 ## 11 Lea 150 2.3 ## 12 Karen 150 1.9 ## 13 Carla 200 2.3 ## 14 Mace 200 2.0 ## 15 Lea 200 2.7 ## 16 Karen 200 2.2 ## 17 Carla 250 3.0 ## 18 Mace 250 3.6 ## 19 Lea 250 2.6 ## 20 Karen 250 3.2 ## 21 Carla 300 2.5 ## 22 Mace 300 3.0 ## 23 Lea 300 2.2 ## 24 Karen 300 1.5 ## 25 Carla 350 1.8 ## 26 Mace 350 2.5 ## 27 Lea 350 2.6 ## 28 Karen 350 1.9 Claro que en este caso es mucho más largo hacerlo de esta forma y además hay que delimitar cada número con acentos graves “`” porque no es un nombre válido en R. Los nombres válidos son aquellos que empiezan con letras o puntos y no contienen signos de operaciones matemáticas ni espacios dentro. Para poner los datos en formato ancho está la función spread() (expandir). Los argumentos son los mismos que tiene gather(). spread(data = race_largo, key = time, value = score) ## Name 100 150 200 250 300 350 50 ## 1 Carla 1.8 2.2 2.3 3.0 2.5 1.8 1.2 ## 2 Karen 1.7 1.9 2.2 3.2 1.5 1.9 1.3 ## 3 Lea 1.6 2.3 2.7 2.6 2.2 2.6 1.7 ## 4 Mace 1.1 1.9 2.0 3.6 3.0 2.5 1.5 4.6 Por su cuenta Lean los datos de load(url(&quot;git.io/calidad-del-aire-2017.RData&quot;)) Son datos de calidad del aire de la ciudad de Buenos Aires ¿Qué columnas hay y cuantos datos encuentran? ¿En que tipo de formato está? ¿Largo o ancho? Cambien los datos a formato largo. Pista: Usen gather para llegar a FECHA, HORA, Columna, Valor, luego Usen la función separate para separar el lugar del tipo de variable. Finalmente, con spread llevenlo a un formato más adecuado para trabajar. Calculen el promedio por lugar para las distintas variables. Pista: el formato más largo que crearon como paso intermedio arriba hace que el trabajo sea más corto. Ordenen los lugares por la contaminación con material particulado. Grafiquen cada uno de los datos de cada lugar, por fecha. Usen facetas para cada variable y un color distinto para cada lugar. Seleccionen los datos de parque Centenario. Grafiquen los datos de parque Centenario. "],
["anova.html", "Capítulo 5 ANOVA 5.1 Algunos conceptos importantes 5.2 Diseño de Estudios de ANOVA 5.3 Planificación De Experimentos 5.4 Usos Del ANOVA 5.5 MODELO I DE ANOVA. NIVELES DEL FACTOR FIJOS 5.6 Comprobación de los Supuestos 5.7 Transformaciones 5.8 Formulación Del Modelo I De ANOVA. 5.9 Partición De La Suma De Cuadrados Total 5.10 Grados De Libertad 5.11 Cuadrados Medios 5.12 Prueba F para la Igualdad de las Medias de los Niveles del Factor 5.13 Formulación Alternativa Del Modelo I 5.14 Prueba Para La Igualdad De Las Medias De Los Niveles Del Factor 5.15 Análisis De Los Efectos Del Nivel Del Factor 5.16 Planificación Del Tamaño Muestral 5.17 Modelo II De ANOVA: Niveles Del Factor Aleatorios", " Capítulo 5 ANOVA 5.1 Algunos conceptos importantes Factor: Un factor es una variable independiente a ser estudiada en una investigación. Ejemplo: Temperatura, Dieta Nivel: El nivel de un factor es una forma particular de ese factor. Ejemplo: Temperatura: 0ºC, 10ºC y 20ºC. Dieta: Con aditivos proteicos y Sin aditivos proteicos Estudios Uní y Multifactoriales: Estudios de un factor, únicamente un factor es de interés. En estudios Multifactoriales, dos o más factores son investigados simultáneamente. Ejemplo: Un factor: cantidades de suplemento de proteínas de una clase determinada. Más de un factor: cantidades y clases de suplementos de proteínas. Factores Experimentales y de Clasificación: En cualquier investigación basada sobre datos observacionales, los factores bajo estudio son factores de clasificación. Un factor de clasificación corresponde a la característica de las unidades bajo estudio y no las que están bajo control del investigador, no pueden ser manipuladas experimentalmente. Por otro lado, un factor experimental es aquel donde los niveles del factor son asignados al azar a las unidades experimentales. Factores cualitativos y cuantitativos: Un factor cualitativo es aquel donde los niveles difieren con respecto a un atributo cualitativo. Por otro lado, un factor cuantitativo es aquel que es descrito por una cantidad numérica sobre una escala. Tratamientos: Es el procedimiento cuyo efecto se mide y se compara con otros tratamientos. En estudios unifactoriales un tratamiento corresponde a un nivel de un factor. En estudios Multifactoriales, un tratamiento corresponde a una combinación de niveles de factores. 5.2 Diseño de Estudios de ANOVA Elección del tratamiento: La elección de los tratamientos a ser incluidos en una investigación es básicamente una decisión del investigador. En una investigación científica, los tratamientos incluidos deberían poder suministrar conocimientos sobre el mecanismo subyacente al fenómeno bajo estudio. Definición del tratamiento: Al seleccionar un conjunto de tratamientos, es importante definir cada tratamiento cuidadosamente y considerarlo con respecto a cada uno de los demás tratamientos para asegurarse, en lo posible, que el conjunto dé respuestas eficientes relacionadas con los objetivos del experimento. Tratamiento Control o Testigo: Un tratamiento control consiste en la aplicación de procedimientos idénticos a las unidades experimentales que aquellos usados con los otros tratamientos, excepto por los efectos bajo investigación. Un tratamiento control es requerido cuando la efectividad general de los tratamientos bajo estudio no es conocida, o cuando la efectividad general de los tratamientos es conocida pero no es consistente bajo todas las condiciones. Unidad experimental o unidad básica de estudio: Es la unidad de material a la cual se aplica un tratamiento. Es la mínima unidad de muestreo. No siempre coincide con la unidad de muestreo. Ejemplo: se aplica un tratamiento en una maceta y se muestrean tres hojas de cada maceta. Observación individual: Son las mediciones que se hacen en cada una de las unidades experimentales. Muestra: Es el conjunto de observaciones individuales, se expresa en términos de observaciones individuales y no de unidades experimentales, es la única información que uno posee. 5.3 Planificación De Experimentos Las inferencias que pueden hacerse, a partir de los resultados de un experimento, dependen de la forma en que fue hecho el experimento. Es una buena práctica hacer un proyecto de los propósitos de cualquier experimento. Este proyecto constará de tres partes: Enumeración de las finalidades: debe incluir una determinación del campo sobre el cual se harán las generalizaciones, o, en otras palabras, la población respecto de la cual se espera hacer inferencias. descripción del experimento: Se ha usado el término tratamiento para denominar los diferentes procesos cuyos efectos van a ser medidos y comparados. En la selección de los tratamientos es importante definir claramente cada uno de ellos y entender el papel que jugará para alcanzar los objetivos del experimento. bosquejo del método de análisis de los resultados: Las características del experimento que deben ser tenidas en cuenta en la enumeración de finalidades son: el número de repeticiones, los tipos de material experimental que se van a usar, las mediciones que se van a hacer. Finalmente, el bosquejo debería describir, con algún detalle, el método propuesto para sacar conclusiones de los resultados. 5.4 Usos Del ANOVA Los estudios de un solo factor son utilizados para comparar efectos de diferentes niveles de un factor, para determinar el “mejor” nivel del factor y la semejanza. En estudios multifactoriales, el ANOVA es empleado para determinar si los diferentes factores interactúan, que factores son claves; cuales combinaciones de factores son las “mejores”, etc. 5.5 MODELO I DE ANOVA. NIVELES DEL FACTOR FIJOS 5.5.1 Distinción Entre Modelos I Y II de ANOVA El modelo I de ANOVA se aplica en casos tales como una comparación de un número determinado de tratamientos, y donde las conclusiones se restringen a aquellos niveles del factor incluidos en el estudio. También se conoce como modelo de efectos fijos. El modelo II de ANOVA se aplica a un tipo diferente de situación, donde las conclusiones se extenderán a una población de niveles del factor del cual los niveles bajo estudio son una muestra. Es decir que se trata de un modelo de efectos aleatorios. 5.5.2 Ideas Básicas Los elementos básicos del modelo I de ANOVA para un estudio de un factor son muy simples. Correspondiendo a cada nivel del factor, hay una distribución de probabilidades de respuestas. El modelo I de ANOVA supone: Cada una de las distribuciones en probabilidades es normal Cada distribución en probabilidad tiene la misma varianza (desviación estándar). Las observaciones para cada nivel del factor son observaciones aleatorias de la correspondiente distribución y son independientes de las observaciones de cualquier otro nivel del factor. Figura 5.1: Densidad de distribuciones de cuatro distribuciones normales con igual varianza y distinta media La Figura 5.1 ilustra estas condiciones: la normalidad de la distribución en probabilidades y la variabilidad constante. Las distribuciones en probabilidad difieren sólo con respecto a sus medias. El análisis de los datos de las muestras de las distribuciones en probabilidades de los niveles de los factores se desarrolla usualmente en dos pasos: Determinar si las medias de los niveles de los factores son las mismas. Si las medias de los niveles del factor no son las mismas, examinar como difieren y cuales son las consecuencias de las diferencias. 5.6 Comprobación de los Supuestos Los modelos de ANOVA son razonablemente robustos, aunque se produzcan ciertos alejamientos del supuesto de normalidad. 5.6.1 Prueba para igualdad de varianzas 5.6.1.1 Prueba de Bartlett Las hipótesis son \\[ \\begin{aligned} H_{0} &amp;: \\sigma_{1}^{2} = \\sigma_{2}^{2} = \\ldots = \\sigma_{i}^{2} = \\ldots = \\sigma_{I}^{2} \\\\ H_{a} &amp;: \\text{no todos}\\ \\text{los}\\ \\sigma_{i}^{2}\\ \\text{son}\\ \\text{iguales} \\end{aligned} \\] Sean \\(S_{1}^{2},\\ldots,S_{I}^{2}\\)indican las varianzas muestrales de \\(I\\) poblaciones normales, y \\(Gl_{i}\\) indica los grados de libertad asociados con la varianza muestral \\(S_{i}^{2}\\). Bartlett ha demostrado que una función de \\(\\left\\lbrack \\ln\\left( C\\text{MD} \\right) - ln(MGD) \\right\\rbrack\\), (\\(M\\text{GD}\\): media geométrica pesada; \\(C\\text{MD}\\): cuadrado medio dentro) para grandes tamaños muestrales, sigue aproximadamente la distribución \\(\\chi^{2}\\) con \\((I-1)\\) grados de libertad cuando las varianzas poblacionales son iguales. La prueba estadística es: \\[ B = \\frac{GL_{t}}{C}\\left\\lbrack \\ln\\left( CM_D \\right) - ln(M\\text{MGD}) \\right\\rbrack \\] , donde \\[ C = 1 + \\frac{1}{3\\left( I - 1 \\right)}\\left\\lbrack \\left( \\sum_{i = 1}^{I}\\frac{1}{GL_{i}} \\right) - \\frac{1}{GL_{T}} \\right\\rbrack \\] El término C es siempre mayor que 1. La prueba estadística se reduce a: \\[ B = \\frac{1}{C}\\left\\lbrack (GL_{t})\\ln\\left( CM_D \\right) - \\sum_{i = 1}^{I}\\left( GL_{i} \\right)lnS_{i}^{2} \\right\\rbrack \\] se calcula el estadístico \\(B\\). La regla de decisión es: Si \\(B &lt; \\chi_{(1 - \\alpha;I - 1)}^{2}\\), no se rechaza \\(H_{0}\\) Si \\(B &gt; \\chi_{(1 - \\alpha;I - 1)}^{2}\\), se rechaza \\(H_{0}\\) Esta aproximación se considera apropiada cuando los grados de libertad son mayores o iguales que cuatro. Cuando la prueba se usa para un modelo de ANOVA de un factor se tiene: \\(GL_{i} = n_{i} - 1\\) y \\(GL_{T} = \\sum_{i = 1}^{I}\\left( n_{i} - 1 \\right) = N - I\\) La prueba de Bartlett es bastante sensible a la falta de normalidad. Si las varianzas muestrales son menores que la unidad, sus logaritmos serán negativos. Por lo tanto, es conveniente utilizar un código multiplicativo para hacer las varianzas mayores que la unidad. Este código no afecta en modo alguno a la prueba estadística. 5.6.1.2 Prueba de Levene Modificada Las hipótesis son \\[ \\begin{aligned} H_{0} &amp;:\\sigma_{1}^{2} = \\sigma_{2}^{2} = \\ldots = \\sigma_{i}^{2} = \\ldots = \\sigma_{I}^{2}\\\\ H_{a} &amp;:\\text{no todos}\\ \\text{los}\\ \\sigma_{i}^{2}\\ \\text{son}\\ \\text{iguales} \\end{aligned} \\] Primero se calcula la desviación absoluta de las \\(Y_ij\\) observaciones de sus respectivas medianas del nivel del factor \\(\\tilde{Y_{i}}\\) \\[ d_{ij} = \\left| Y_{ij} - \\tilde{Y_{i}} \\right| \\] Entonces la prueba de Levene determina si los valores esperados de las desviaciones absolutas son iguales. Si las varianzas son iguales entonces los valores esperados de las desviaciones absolutas también serán iguales. La prueba de Levene usa el estadístico \\(F^{*}\\) \\[ F_{L}^{*} = \\frac{CM_{ET}}{CM_D} \\] donde \\[ CM_{ET} = \\frac{\\sum n_{i}\\left( \\overline{d}_{i\\bullet}\\ - \\overline{d}_{\\bullet\\bullet} \\right)^{2}}{I - 1} \\] \\[ CM_D = \\frac{\\sum\\sum\\left( \\overline{d}_{ij}\\ - \\overline{d}_{i\\bullet}\\right)^{2}}{N - 1} \\] \\[ \\overline{d}_{i\\bullet} = \\sum_{j}^{}\\frac{d_{ij}}{n_{i}} \\] \\[ \\overline{d}_{\\cdot \\cdot } = \\frac{\\sum_{}^{}{\\sum_{}^{}{d_{ij}}}}{N} \\] Si las varianzas son iguales y los tamaños muestrales no son extremadamente pequeños, \\(\\mathbf{F}_{\\mathbf{L}}^{\\mathbf{*}}\\) sigue aproximadamente una distribución \\(F\\) con \\((I - 1)\\) y \\((N - I)\\) grados de libertad. 5.6.2 Prueba de Kolmogorov - Smirnov (modificación de Lilliefors) para estudiar Normalidad Dada una muestra aleatoria, se calcula su media y su varianza muestral, luego se calculan los datos normalizados \\(Z_{i}\\). Se ordenan los datos de menor a mayor, se calculan las frecuencias acumuladas observadas, las esperadas para los \\(Z_{i}\\), y luego se calculan las diferencias, en valor absoluto entre las frecuencias acumuladas observadas y las esperadas. Se define \\(D_{max} = max\\left| F_{i} - \\hat{F_{i}} \\right|\\) este estadístico se compara con el valor de tablas \\(d_{max}\\) al nivel de significación \\(\\alpha\\). 5.6.3 Residuos El residuo \\(\\varepsilon_{ij}\\) es definido como la diferencia entre el valor observado y el ajustado: \\[ \\varepsilon_{ij} = y_{ij} - \\overline{y}_{i\\bullet} \\] Así, un residuo representa la desviación de una observación individual de la respectiva media estimada del nivel del factor. A veces es útil trabajar con los residuos estandarizados, que se expresan como: \\[ \\varepsilon_{\\ _{ij}}^{\\otimes} = \\frac{\\varepsilon_{ij} - \\overline{\\varepsilon}}{\\sqrt{CM_D}} \\] Los residuos “semistudentizados”, los residuos “studentizados”, y los residuos “studentizados borrados” son a menudo útiles para diagnosticar los alejamientos del modelo de ANOVA. Los residuos “semistudentizados” se calculan como: \\[ \\varepsilon_{\\ _{ij}}^{*} = \\frac{\\varepsilon_{ij}}{\\sqrt{CM_D}} \\] Los residuos “studentizados” se calculan como: \\[ r_{ij} = \\frac{\\varepsilon_{ij}}{S\\left( \\varepsilon_{ij} \\right)} \\] donde \\[ S\\left( \\varepsilon_{ij} \\right) = \\sqrt{\\frac{CM_D\\left( n_{i} - 1 \\right)}{n_{i}}} \\] Finalmente, los residuos “studentizados borrados” se calculan \\[ t_{ij} = \\varepsilon_{ij}\\left\\lbrack \\frac{N - I - 1}{SC_D\\left( 1 - \\frac{1}{n_{i}} \\right)\\varepsilon_{ij}^{2}} \\right\\rbrack^{\\frac{1}{2}} \\] 5.6.4 Gráficos de Residuos Estos gráficos son muy importantes para el diagnóstico de problemas con el modelo. Incluye: Residuos vs. las medias de tratamientos: Dado que las valores ajustados de cada nivel de factor se corresponde a la media, todos los valores de los residuales de ese nivel se alinearán en sobre esa media (Figura 5.2-a). Si no hay problemas con el modelo, entonces los residuales deberían tener la misma dispersión. 2.Residuos vs. el tiempo u otra secuencia: Si los datos fueron tomados de forma aleatoria no debería verse un patrón definido (Figura 5.2-b). Gráficos de puntos de los residuos: Este gráfico es similar al primero. Denuevo, en todos lo niveles del factor los residuales deberían tener la misma dispersión alrededor del cero(Figura 5.2-c). Además, dado que están graficados los residuales estandarizados, cualquier residual mayor 3 debería ser investigado por ser muy extremo. Gráficos de probabilidad normal de los residuos: También llamado gráfico cuantil-cuantil o qqplot (Figura 5.2-d). Aquí se grafican los cuantiles de una normal teórica vs los cuantiles muestrales. Idealmente, deberían seguir una línea recta de pendiente 1 y ordenada 0. Figura 5.2: Gráficos de residuales para modelos de ANOVA. a - residuales vs valores predichos. b - residuales vs orden de toma de datos. c - residuales vs niveles del factor. d - gráfico de probabilidad normal. 5.6.4.1 Diagnóstico de los alejamientos de los supuestos del Modelo de ANOVA Heterogeneidad de Varianzas: El Modelo de ANOVA requiere que los términos del error tengan varianzas constantes para todos los niveles del factor. Cuando los tamaños de las muestras son iguales o no difieren mucho, esta suposición puede ser estudiada usando los residuos, los residuos “studentizados” o los residuos “semistudentizados”. Gráficos de los residuos vs. las medias de los niveles del factor o los gráficos de puntos de los residuos son útiles. Cuando los tamaños de las muestras difieren mucho, los residuos “studentizados” deberían ser usados en estos gráficos. La constancia de la varianza del error se ve en estos gráficos pues los puntos tienen aproximadamente la misma dispersión alrededor del cero para cada nivel. La Figura 5.3 muestra un caso en el que las varianzas de los errores no son constantes. En este caso los términos del error del nivel c del factor tienen una varianza mayor que los otros dos niveles del factor. Figura 5.3: Residuales vs Valores Ajustados o predichos. Este gráfico muestra que los residuales uno de los niveles muestra mayor dispersión que el resto de los datos. Cuando los tamaños de las muestras, para los diferentes niveles del factor son grandes, los histogramas de los residuos para cada tratamiento, son una manera efectiva de examinar la constancia de la varianza de los términos del error. Falta de independencia en los términos del error: En todos aquellos casos en que los datos son obtenidos en una secuencia de tiempo, un gráfico de secuencia de residuos es aconsejable para examinar si los términos del error están correlacionados. La Figura 5.4 muestra un caso en el cual los residuos aparecen altamente correlacionados. Esto puede pasar porque el operario tiende a sobreestimar a medida que pasa el tiempo o también porque los equipos se descalibran. Figura 5.4: Residuales vs Orden. Los residuales muestran falta de independencia al haber una correlación entre ellos. La siguiente Figura 5.5 muestra un caso donde la varianza decrece con el tiempo. Figura 5.5: Residuales vs Orden. Los residuales muestran que la varianza decrece, ya que al principio son mayores y al final son menores Cuando los datos son ordenados en alguna a otra secuencia lógica, tal como una secuencia geográfica, también debe verificarse si existe correlación entre los términos del error de acuerdo a este orden. Otros usos del análisis de residuos: Este tipo de análisis se puede usar para detectar “outliers”. También es útil para determinar si modelo de ANOVA de un factor es el adecuado; pues puede determinar la omisión de alguna variable importante que explica las observaciones. También puede ser usado para determinar la falta de normalidad de los términos del error. Esto se realiza graficando los cuantiles de los residuales observados vs los esperados. 5.7 Transformaciones 5.7.1 Transformaciones para estabilizar las Varianzas Varianza proporcional a \\(\\mu_{i}\\) : El estadístico muestral \\(\\mathbf{S}_\\mathbf{i}^\\mathbf{2}\\mathbf{/}{\\overline{\\mathbf{Y}}}_{\\mathbf{i}}\\) tenderá a ser constante. Este tipo de situaciones a menudo se encuentra cuando la variable observada es un número entero. Para estos casos, una transformación raíz cuadrada es útil para estabilizar la varianza: \\(Y^{&#39;} = \\sqrt{Y}\\)o \\(Y^{&#39;} = \\sqrt{Y + \\frac{1}{2}}\\) o\\(Y^{&#39;} = \\sqrt{Y} + \\sqrt{Y + 1}\\) Desviación estándar proporcional a \\(\\mu_{i}\\): \\(\\mathbf{S}_\\mathbf{i}\\mathbf{/}\\overline{\\mathbf{Y}_\\mathbf{i}}\\)tiende a ser contante para los diferentes niveles del factor. Una transformación útil para estabilizar la varianza es la transformación logarítmica: \\(Y^{&#39;} = \\log Y\\)o\\(Y^{&#39;} = \\log{(Y + 1)}\\) Desviación estándar proporcional a \\(\\mu_{i}^{2}\\): \\(\\mathbf{S}_\\mathbf{i}^{\\mathbf{2}}\\mathbf{/}\\overline{\\mathbf{Y}}_{\\mathbf{i}}^{\\mathbf{2}}\\)En este caso tiende a ser constante. la transformación apropiada es la recíproca: \\[ Y^{&#39;} = \\frac{1}{Y} \\] La variable dependiente es una proporción: Una transformación apropiada para este caso es la transformación angular o arcoseno: \\[ Y^{&#39;} = \\text{arcsen}\\sqrt{Y} \\] 5.7.2 Transformaciones para corregir la falta de normalidad La transformación que ayuda a corregir la heterogeneidad de varianzas usualmente también es efectiva para hacer que las distribuciones de los términos del error sean más normales. 5.7.3 Efectos Del Alejamiento De Los Supuestos Del Modelo 5.7.3.1 Normalidad Para el modelo I de ANOVA, la falta de normalidad no es importante, en tanto ese alejamiento no sea extremo. La kurtosis es más importante que la asimetría en términos de efectos sobre las inferencias (Figura 5.6). La prueba F es poco afectada por la falta de normalidad, ya sea en términos del nivel de significación o de la potencia de la prueba. Para el Modelo II de ANOVA, la falta de normalidad tiene serias implicaciones. Figura 5.6: Funciones de densidad para curvas asimétrica, mesocúrtica, leptocúrtica, platicúrtica 5.7.3.2 Heterogeneidad de varianzas Para el modelo de efectos fijos, la prueba de F es ligeramente afectada si los tamaños muestrales son iguales o no difieren mucho. La prueba de F y los análisis relacionados son robustos frente a la heterogeneidad de varianzas cuando los tamaños muestrales son aproximadamente iguales. Para el modelo de efectos aleatorios, la heterogeneidad de varianzas puede tener efectos pronunciados sobre las inferencias acerca de los componentes de la varianza, aun con tamaños muestrales iguales. 5.7.3.3 Independencia de los términos del error La falta de independencia puede tener serios efectos sobre las inferencias en el análisis de la varianza, para el modelo de efectos fijos y para el de efectos aleatorios. 5.8 Formulación Del Modelo I De ANOVA. Denotaremos por I el número de niveles del factor bajo estudio, y denotaremos cualquiera de estos niveles por el subíndice \\(i\\ (i\\ = \\ 1,\\ 2,\\ \\ldots,\\ I)\\). El número de casos para el i-ésimo nivel del factor es simbolizado por \\(n_{i}\\), y el número total de casos en el estudio es denotado por \\(N\\), donde: \\[ N = \\sum_{i = 1}^{I}n_{i} \\] Además, \\(Y_{ij}\\) denotará la j-ésima observación para el i-ésimo nivel del factor. Dado que el número de casos para el i-ésimo nivel del factor es denotado por \\(n_{i}\\), tendremos \\(j\\ = \\ 1,\\ 2,\\ \\ldots,\\ n_{i}\\). El modelo I de ANOVA se puede plantear como sigue: \\[ y_{ij} = u_{i} + \\varepsilon_{ij} \\] donde: \\(y_{ij}\\) es el valor de la j-ésima observación para el i-ésimo nivel del factor o tratamiento. \\(\\mu_{i}\\) es un parámetro \\(\\varepsilon_{ij}\\) son variables independientes \\(N(0,\\sigma^{2})\\) \\(i = 1,\\ 2,\\ldots,I;j = 1,\\ 2,\\ \\ldots,\\ n_{i}\\) 5.8.1 Características importantes del modelo El valor observado de \\(Y\\) en el j-ésimo ensayo del i-ésimo nivel del factor o tratamiento es la suma de dos componentes: a) un término constante \\(\\mu_{i}\\), y b) un término del error aleatorio \\(\\varepsilon_{ij}\\). Dado que \\(E\\left( \\varepsilon_{ij} \\right) = 0\\), se sigue que: \\[ E\\left( Y_{ij} \\right) = \\mu_{i} \\] Dado que \\(\\mu_{i}\\) es una constante, se sigue que: \\[ V\\text{ar}\\left( Y_{ij} \\right) = V\\text{ar}\\left( \\varepsilon_{ij} \\right) = \\sigma^{2} \\] Así como cada \\(\\varepsilon_{ij}\\) esta normalmente distribuido, también lo está cada \\(Y_{ij}\\). Se asume que los términos del error son independientes El modelo de ANOVA puede ser re-enunciado como: \\[ Y_{ij}\\sim N(\\mu_{i},\\sigma^{2}) \\] 5.8.2 Interpretación De Las Medias De Los Niveles Del Factor Datos observacionales: la media del nivel del factor \\(\\mu_{i}\\) corresponde a las medias para las diferentes poblaciones del nivel del factor. Datos Experimentales: la media del nivel del factor \\(\\mu_{i}\\) representa la media de la respuesta que debería obtenerse si el i-ésimo tratamiento fuera aplicado a todas las unidades en la población de las unidades experimentales sobre las cuales se harán las inferencias. 5.8.3 Ajustando El Modelo Supongamos que tenemos \\(I\\) tratamientos o niveles de un factor y que aplicamos cada uno de ellos a un grupo de unidades experimentales.Los datos se podrían consignar de la siguiente forma:  Tratamientos \\(T_{1}\\) \\(T_{2}\\) \\(\\ldots\\) \\(T_{i}\\) \\(\\ldots\\) \\(T_{I}\\) \\(y_{1\\mathbf{1}}\\) \\(y_{2\\mathbf{1}}\\) \\(\\ldots\\) \\(y_{i1}\\) \\(\\ldots\\) \\(y_{I1}\\) \\(y_{1\\mathbf{2}}\\) \\(y_{2\\mathbf{2}}\\) \\(\\ldots\\) \\(y_{i2}\\) \\(\\ldots\\) \\(y_{I2}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\ddots\\) \\(\\vdots\\) \\(\\ldots\\) \\(\\vdots\\) \\(y_{1j}\\) \\(y_{2j}\\) \\(\\ldots\\) \\(y_{i\\mathbf{j}}\\) \\(\\ldots\\) \\(y_{I\\mathbf{j}}\\) \\(\\sum_{j = \\mathbf{1}}^{n_{i}}y_{i\\mathbf{j}}\\) \\(\\sum_{j = \\mathbf{1}}^{n_{1}}y_{1j}\\) \\(\\sum_{j = \\mathbf{1}}^{n_{2}}y_{2j}\\) \\(\\ldots\\) \\(\\sum_{j = \\mathbf{1}}^{n_{i}}y_{i\\mathbf{j}}\\) \\(\\ldots\\) \\(\\sum_{j = \\mathbf{1}}^{n_{I}}y_{I\\mathbf{j}}\\) \\(\\sum_{j = \\mathbf{1}}^{I}y_{i\\mathbf{j}}\\) \\(n_{1}\\) \\(n_{2}\\) \\(\\ldots\\) \\(n_{i}\\) \\(\\ldots\\) \\(n_{I}\\) \\(\\sum_{i = \\mathbf{1}}^{I}n_{i} = N\\) \\(\\overline{y_{i\\bullet}}\\) \\(\\overline{y_{1.}}\\) \\(\\overline{y_{2.}}\\) \\(\\ldots\\) \\(\\overline{y_{i\\bullet}}\\) \\(\\ldots\\) \\(\\overline{y_{I.}}\\) \\(\\frac{\\sum_{i\\mathbf{j}}^{N}y_{i\\mathbf{j}}}{N} = \\overline{y_{\\bullet\\bullet}}\\) \\(S_{1}^{2}\\) \\(S_{2}^{2}\\) \\(\\ldots\\) \\(S_{i}^{2}\\) \\(\\ldots\\) \\(S_{I}^{2}\\) donde \\(T_{i}\\) es el tratamiento o nivel del factor \\(i\\); con \\(i\\ = \\ 1,\\ 2,\\ldots,I\\) \\(y_{ij}\\) es la observación sobre la unidad experimental\\(\\ j\\) con el tratamiento \\(i\\); $ j  =  1,2, , n_{i}$. \\(N\\) tamaño de la muestra \\({\\overline{y}}_{i \\bullet}\\) es la media muestral de cada tratamiento. \\(n_{i}\\) es el número de observaciones con el tratamiento \\(i\\) \\({\\overline{y}}_{\\bullet \\bullet}\\) es la media total, para todas las observaciones. \\(S_{i}^{2}\\) es la varianza muestral para el tratamiento \\(i\\) 5.8.4 Estimadores De Mínimos Cuadrados De acuerdo al criterio de mínimos cuadrados la suma de los cuadrados de las desviaciones de las observaciones alrededor de sus valores esperados puede ser minimizada con respecto a los parámetros. Para un modelo de ANOVA, tenemos que: \\[ E\\left( Y_{ij} \\right) = \\mu_{i} \\] Así, la cantidad a ser minimizada es: \\[ \\sum_{i}^{}{\\sum_{j}^{}\\left( y_{ij} - \\mu_{i} \\right)^{2}} \\] Esta expresión se puede escribir como: \\[ \\sum_{j}^{}\\left( y_{1j} - \\mu_{1} \\right)^{2} + \\sum_{j}^{}\\left( y_{2j} - \\mu_{2} \\right)^{2} + \\ldots + \\sum_{j}^{}\\left( y_{Ij} - \\mu_{I} \\right)^{2} \\] La media muestral minimiza una suma de desviaciones al cuadrado \\[\\begin{equation} \\hat{\\mu_{i}} = \\overline{y}_{i\\bullet} \\tag{5.1} \\end{equation}\\] 5.8.4.1 Comentarios Los estimadores de mínimos cuadrados (5.1) son también estimadores de máxima verosimilitud para el error normal (\\(\\varepsilon_{ij}\\)) del modelo de ANOVA. Para derivar el estimador de mínimo cuadrados de \\(u_{i}\\) , es necesario minimizar, con respecto a \\(u_{i}\\), el i-ésimo componente de la suma de cuadrados en: \\[ \\sum_{j}^{}\\left( y_{ij} - \\mu_{i} \\right)^{2} \\] Diferenciando con respecto a \\(\\mu_{i}\\), se obtiene: \\[ \\frac{\\partial\\sum_{j}^{}\\left( y_{ij} - \\mu_{i} \\right)^{2}}{\\partial\\mu_{i}} = \\sum_{}^{}{- 2\\left( y_{ij} - \\mu_{i} \\right)} \\] Esta derivada se iguala a cero y se reemplaza el parámetro \\(\\mu_{i}\\) por su estimador: \\[ - 2\\sum_{j = 1}^{n_{i}}\\left( y_{ij} - \\mu_{i} \\right) = 0 \\] \\[ \\sum_{j = 1}^{n_{i}}y_{ij} = n_{i}\\hat{\\mu_{i}} \\] \\[ \\hat{\\mu_{i}} = \\overline{Y}_{i\\bullet} \\] 5.9 Partición De La Suma De Cuadrados Total La variabilidad total de las observaciones \\(y_{ij}\\), sin usar la información sobre los niveles del factor, es medida en términos de la desviación de cada observación \\(y_{ij}\\) alrededor de la media total \\({\\overline{y}}_{\\bullet\\bullet}\\): \\[ y_{ij} - {\\overline{y}}_{\\bullet\\bullet} \\] Cuando se utiliza la información sobre los niveles del factor, las desviaciones son aquellas de cada observación \\(y_{ij}\\) alrededor de su respectiva media estimada \\({\\overline{y}}_{\\text{i}\\bullet}\\): \\[ y_{ij} - {\\overline{y}}_{\\text{i}\\bullet} \\] La diferencia entre la desviación total y la desviación anterior refleja la diferencia entre la media estimada del nivel del factor y la media total: \\[ {(y}_{ij} - {\\overline{y}}_{\\bullet\\bullet}) - (y_{ij} - {\\overline{y}}_{i}) = {\\overline{y}}_{\\text{i}\\bullet} - {\\overline{y}}_{\\bullet\\bullet} \\] Así, la desviación total \\(y_{ij} - {\\overline{y}}_{\\bullet\\bullet}\\) puede ser vista como la suma de dos componentes: La desviación de la media estimada del nivel del factor alrededor de la media total. La desviación de \\(y_{ij}\\) alrededor de la media de su nivel del factor. Esta desviación es simplemente el residuo \\(\\varepsilon_{ij}\\) . Elevando al cuadrado se obtiene: \\[ \\sum_{i}^{}{\\sum_{j}^{}\\left( y_{ij} - {\\overline{y}}_{\\bullet\\bullet} \\right)^{2}} = \\sum_{i}^{}{n_{i}\\left( {\\overline{y}}_{\\text{i}\\bullet} - {\\overline{y}}_{\\bullet\\bullet} \\right)^{2}} + \\sum_{i}^{}{\\sum_{j}^{}\\left( y_{ij} - {\\overline{y}}_{\\text{i}\\bullet} \\right)^{2}} \\] El primer miembro de igualdad representa la variabilidad total de las \\(y_{ij}\\) observaciones y es denotado como la suma de cuadrados total (\\(SCT\\)): \\[ SC_T = \\sum_{i}^{}{\\sum_{j}^{}\\left( y_{ij} - {\\overline{y}}_{\\bullet\\bullet} \\right)^{2}} \\] El primer término del segundo miembro de la igualdad será indicado como \\(SCE\\), la suma de cuadrados entre tratamientos: \\[ SC_E = \\sum_{i}^{}{n_{i}\\left( y_{\\text{i}\\bullet} - {\\overline{y}}_{\\bullet\\bullet} \\right)^{2}} \\] El segundo término se indica como \\(SCD\\), la suma de cuadrados dentro de tratamientos o la suma de cuadrados del error. \\[ SC_D = \\sum_{i}^{}{\\sum_{j}^{}\\left( y_{ij} - {\\overline{y}}_{\\text{i}\\bullet} \\right)^{2}} = \\sum_{i}^{}{\\sum_{j}^{}\\varepsilon_{ij}^{2}} \\] Así, podemos escribir: \\[ SCT\\ = \\ SCE\\ + \\ SCD \\] La suma total de los cuadrados para el modelo de análisis de la varianza se compone en consecuencia de dos partes. 5.9.1 Fórmulas computatorias \\[ \\begin{matrix} SC_T = \\sum_{i}^{}{\\sum_{j}^{}{y_{ij}^{2} - N{\\overline{y}}_{\\bullet\\bullet}^{2}}} \\\\ SC_E = \\sum_{i}^{}{n_{i}y_{\\text{i}\\bullet}^{2} - N{\\overline{y}}_{\\bullet\\bullet}^{2}} \\\\ SC_D = \\sum_{i}^{}{\\sum_{j}^{}{y_{ij}^{2} - n_{i}{\\overline{y}}_{\\text{i}\\bullet}^{2}}} = SC_T - SC_E \\\\ \\end{matrix} \\] 5.10 Grados De Libertad Correspondiendo a la descomposición de la suma de cuadrados total, se puede obtener los grados de libertad asociados. La \\(SCT\\) tiene \\((N -\\ 1)\\) grados de libertad asociados. Hay en conjunto N desviaciones \\(Y_{ij} - {\\overline{Y}}_{\\bullet\\bullet}\\) , pero un grado de libertad se pierde debido a que las desviaciones no son independientes a causa de que la suma de ellas debe ser cero. \\(\\sum_{i}^{}{\\sum_{j}^{}\\left(y_{ij} - {\\overline{y}}_{\\bullet\\bullet} \\right)} = 0\\) La \\(SCE\\) (entre tratamientos) tiene \\((I -\\ 1)\\) grados de libertad asociados. Hay I desviaciones de las medias de los niveles de los factores \\({\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{\\bullet\\bullet}\\), pero un grado de libertad se pierde porque las desviaciones no son independientes a causa de que la suma pesada debe ser cero. \\(\\sum_{i}^{}{n_{i}\\left( y_{\\text{i}\\bullet} - {\\overline{y}}_{\\bullet\\bullet} \\right) = \\ 0}\\). La \\(SCD\\) tiene \\((N -\\ I)\\) grados de libertad asociados. Esto puede verse considerando el componente de la \\(SCD\\) para el i-ésimo nivel del factor: \\[ \\sum_{j}^{}\\left( y_{ij} - {\\overline{y}}_{\\text{i}\\bullet} \\right)^{2} \\] La expresión es equivalente a la suma de cuadrados total considerando sólo el i-ésimo nivel del factor. Así, hay \\(n_{i}\\ - \\ 1\\) grados de libertad asociados con esta suma de cuadrados. De esta forma la \\(SCD\\) es una suma de sumas de cuadrados, los grados de libertad asociados son la suma de los grados de libertad de sus términos: \\[ (n_{1}\\ - \\ 1)\\ + \\ (n_{2}\\ - \\ 1)\\ + \\ \\ldots\\ + \\ (n_{I}\\ - \\ 1)\\ = \\ N-\\ I \\] Los grados de libertad, al igual que la suma de cuadrados, son aditivos. 5.11 Cuadrados Medios Los cuadrados medios se obtienen dividiendo la suma de cuadrados por sus grados de libertad asociados. Se tiene: \\[ \\begin{matrix} CM_E = \\frac{SC_E}{I - 1} \\\\ \\\\ CM_D = \\frac{SC_D}{N - I} \\\\ \\end{matrix} \\] \\(CM_E\\), es el cuadrado medio entre los tratamientos. \\(CM_D\\), es el cuadrado medio dentro de los tratamientos o del error. 5.11.1 Esperanza de los Cuadrados Medios Los valores esperados del \\(CM_D\\) y \\(CM_E\\) pueden ser vistos como: \\[ E\\left( CM_D \\right) = \\sigma^{2} \\] \\[\\begin{equation} E\\left( CM_E \\right) = \\sigma^{2} + \\frac{\\sum_{}^{}{n_{i}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2}}}{I - 1} \\tag{5.2} \\end{equation}\\] donde \\[ \\mu_{\\bullet} = \\frac{\\sum_{}^{}{n_{i}\\mu_{i}}}{N} \\] El \\(CM_D\\) es un estimador insesgado de la varianza del error llamado \\(\\varepsilon_{ij}\\), tanto si las medias \\(u_{i}\\) son iguales como si no. Cuando todas las medias \\(\\mu_i\\) de los niveles del factor son iguales y por lo tanto iguales a la media pesada \\(\\mu_{\\bullet}\\), entonces\\(\\ E(CM_E) = \\sigma^{2}\\) dado que el segundo término se vuelve cero. Cuando las medias de los niveles del factor no son iguales, el \\(CM_E\\) tiende en promedio a ser mayor que el \\(CM_D\\), dado que el segundo término de la Ecuación (5.2) será positivo. Esto es intuitivamente razonable, como se ilustra en la figura para cuatro tratamientos. En la situación planteada se asume que todos los tamaños muestrales son iguales, o sea\\(\\ n_{i} = n\\). Cuando todos los \\(\\mu_{i}\\) son iguales, entonces todos lo \\(\\overline{Y}_{i\\bullet}\\) siguen la misma distribución en el muestreo, con una media\\(\\mu_{c}\\) y una varianza \\(\\sigma^{2}/n\\). Cuando las \\(\\mu_{i}\\) no son iguales, por otro lado, las\\(\\ {\\overline{Y}}_{\\text{i}\\bullet}\\) siguen diferentes distribuciones en el muestreo, cada una con la misma variabilidad \\(\\sigma^{2}/n\\) pero centradas sobre medias diferentes \\(\\mu_{i}\\) (Figura 5.1). En consecuencia, los \\({\\overline{Y}}_{\\text{i}\\bullet}\\) tenderán a diferir unos de otros tanto si los \\(\\mu_i\\) difieren como si son iguales, y en consecuencia la \\(SCE\\) tenderá a ser mayor cuando las medias de los niveles de los factores no son las mismas que cuando ellas son iguales. Esta propiedad de la \\(SCE\\) es utilizada en la construcción de la prueba estadística para determinar si las medias de los niveles del factor son iguales o no. Si la \\(SCE\\) y la \\(SCD\\) son de la misma magnitud, esto sugiere que las medias µi de los niveles del factor son iguales. Si la \\(SCE\\) es substancialmente mayor que la \\(SCD\\), esto sugeriría que los \\(\\mu_i\\) no son iguales. 5.11.2 Comentarios Para encontrar el valor esperado del \\(CM_D\\), se ve que puede ser expresado como sigue: \\[ \\begin{matrix} CM_D\\ = \\frac{1}{N - I}\\sum_{i}^{}{\\sum_{j}^{}\\left( Y_{ij} - {\\overline{Y}}_{\\text{i}\\bullet} \\right)^{2}} \\\\ = \\frac{1}{N - I}\\sum_{i}^{}\\left\\lbrack \\left( n_{i} - 1 \\right)\\frac{\\sum_{j}^{}\\left( Y_{ij} - {\\overline{Y}}_{\\text{i}\\bullet} \\right)^{2}}{\\left( n_{i} - 1 \\right)} \\right\\rbrack \\\\ \\end{matrix} \\] Indicamos la varianza muestral de las observaciones para el i-ésimo nivel del factor como \\(s_{i}^{2}\\): \\[ s_{i}^{2} = \\frac{\\sum_{j}^{}\\left( Y_{ij} - {\\overline{Y}}_{\\text{i}\\bullet} \\right)^{2}}{n_{i} - 1} \\] Por lo tanto, el \\(CM_D\\) puede ser expresado de la siguiente forma: \\[ CM_D = \\frac{1}{N - I}\\sum_{i}^{}{\\left( n_{i} - 1 \\right)s_{i}^{2}} \\] Dado que la varianza muestral es un estimador insesgado de la varianza poblacional, la cual es \\(\\sigma^{2}\\) para todos los niveles del factor, se obtiene: \\[ \\begin{aligned} E\\left( CM_D \\right)&amp; = \\frac{1}{N - I}\\sum_{i}^{}{\\left( n_{i} - 1 \\right)E\\left( s_{i}^{2} \\right)} \\\\ &amp; = \\frac{1}{N - I}\\sum_{i}^{}{\\left( n_{i} - 1 \\right)\\sigma^{2}} \\\\ &amp; = \\sigma^{2} \\\\ \\end{aligned} \\] Se puede derivar el valor esperado de la \\(CM_E\\) para el caso especial en que todos los tamaños muestrales ni son los mismos, o sea \\(n_i = n\\). El resultado general para este caso especial: \\[ E\\left( CM_E \\right) = \\sigma^{2} + \\frac{n\\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2}}{I - 1}\\text{ cuando}\\ n_{i} = n \\] De esta forma, cuando todos los tamaños muestrales de los niveles del factor son \\(n\\), el \\(CM_E\\) se vuelve: \\[ CM_E = \\frac{n\\sum_{}^{}\\left( {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{\\bullet\\bullet} \\right)^{2}}{I - 1}\\text{ cuando }n_{i} = n \\] Para derivar el \\(E(CM_D)\\), se considera el modelo: \\[ Y_{ij}\\ = \\ u_{i}\\ + \\ \\varepsilon_{ij} \\] Promediando el \\(Y_{ij}\\) para el i-ésimo nivel del factor, se obtiene: \\[ {\\overline{Y}}_{i \\bullet} = \\mu_{i} + {\\overline{\\varepsilon}}_{i \\bullet} \\] donde \\({\\overline{\\varepsilon}}_{i \\bullet}\\) es el promedio de los \\(\\varepsilon_{ij}\\) para el i-ésimo nivel del factor: \\[ {\\overline{\\varepsilon}}_{\\text{i}\\bullet} = \\frac{\\sum_{j}^{}\\varepsilon_{ij}}{n} \\] Promediando los \\(Y_{ij}\\) sobre todos los niveles del factor, se obtiene: \\[ {\\overline{Y}}_{\\bullet\\bullet} = \\mu_\\bullet + {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\] donde \\(\\mu_{\\bullet}\\) para \\(n_{i} = n\\): \\[ \\mu_{\\bullet} = \\frac{n\\sum_{}^{}\\mu_{i}}{\\text{nI}} = \\frac{\\sum_{}^{}\\mu_{i}}{I}\\text{ donde}\\ n_{i} = n \\] y \\({\\overline{\\varepsilon}}_{\\bullet\\bullet}\\) es el promedio de todos los \\(\\varepsilon_{ij}\\) : \\[ {\\overline{\\varepsilon}}_{\\bullet\\bullet} = \\frac{\\sum_{}^{}{\\sum_{}^{}\\varepsilon_{ij}}}{\\text{nI}} \\] Cuando los tamaños muestrales son iguales, se tiene: \\[ {\\overline{Y}}_{\\bullet \\bullet} = \\frac{\\sum_{}^{}Y_{\\text{i}\\bullet}}{I}\\ \\ \\ \\ {\\overline{\\varepsilon}}_{\\bullet \\bullet} = \\frac{\\sum_{}^{}\\varepsilon_{\\text{i}\\bullet}}{I} \\] Operando se obtiene: \\[ {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{\\bullet \\bullet} = \\left( \\mu_{i} + {\\overline{\\varepsilon}}_{\\text{i}\\bullet} \\right) - \\left( \\mu_{\\bullet} + {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right) = \\left( \\mu_{i} - \\mu_{\\bullet} \\right) + \\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} - {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right) \\] Elevando al cuadrado y sumando sobre los niveles del factor, se obtiene: \\[\\sum_{}^{}\\left( {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{\\bullet \\bullet} \\right)^{2} = \\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2} + \\sum_{}^{}\\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} - {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right)^{2} + 2\\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet \\bullet} \\right)\\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} - {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right)\\] Se desea encontrar el \\(E\\left\\{ \\sum_{}^{}\\left( {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{\\bullet \\bullet} \\right)^{2} \\right\\}\\), y por lo tanto se necesita encontrar el valor esperado de cada uno de los términos de la derecha: Dado que \\(\\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2}\\) es una constante, su valor esperado es: \\[ E\\left\\{ \\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2} \\right\\} = \\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2} \\] Antes de encontrar el valor esperado del segundo término de la derecha, consideremos la expresión: \\[ \\frac{\\sum_{}^{}\\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} - \\overline{\\varepsilon}_{\\bullet\\bullet} \\right)^{2}}{I - 1} \\] Esto es una varianza muestral, dado que \\({\\overline{\\varepsilon}}_{\\bullet\\bullet}\\) es la media muestral de los I términos \\({\\overline{\\varepsilon}}_{\\text{i}\\bullet}\\). Se sabe que la varianza muestral es un estimador insesgado de la varianza de la variable, en este caso de \\({\\overline{\\varepsilon}}_{\\text{i}\\bullet}\\). Pero \\({\\overline{\\varepsilon}}_{\\text{i}\\bullet}\\) es la media de n términos independientes del error \\(\\varepsilon_{ij}\\). Así: \\[ \\text{Var}\\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} \\right) = \\frac{\\text{Var}\\left( \\varepsilon_{ij} \\right)}{n} = \\frac{\\sigma^{2}}{n} \\] Por lo tanto: \\[ E\\left\\{ \\frac{\\sum_{}^{}\\left( {\\overline{\\varepsilon}}_{i\\bullet} - {\\overline{\\varepsilon}}_{\\bullet\\bullet} \\right)^{2}}{I - 1} \\right\\} = \\frac{\\sigma^{2}}{n} \\] en consecuencia: \\[ E\\left\\{ \\sum_{}^{}\\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} - {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right)^{2} \\right\\} = \\frac{\\left( I - 1 \\right)\\sigma^{2}}{n} \\] Dado que tanto \\({\\overline{\\varepsilon}}_{\\text{i}\\bullet}\\) como \\({\\overline{\\varepsilon}}_{\\bullet\\bullet}\\) son medias de los \\(\\varepsilon_{ij}\\) , los cuales tiene un valor esperado, se sigue que: \\[ E\\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} \\right) = 0\\ E\\left( {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right) = 0 \\] por tanto: \\[ E\\left\\{ 2\\sum_{}^{}{\\left( \\mu_{i} - \\mu_{\\bullet} \\right)\\left( {\\overline{\\varepsilon}}_{i\\bullet} - {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right)} \\right\\} = 2\\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)E\\left( {\\overline{\\varepsilon}}_{\\text{i}\\bullet} - {\\overline{\\varepsilon}}_{\\bullet \\bullet} \\right) = 0 \\] Ya se ve que: \\[ E\\left\\{ \\sum_{}^{}\\left( {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{\\bullet\\bullet} \\right)^{2} \\right\\} = \\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2} + \\frac{\\left( I - 1 \\right)\\sigma^{2}}{n} \\] Entonces: \\[ \\begin{matrix} \\begin{split} E\\left( CM_E \\right)&amp; = E\\left\\{ \\frac{n\\sum_{}^{}\\left( {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{\\bullet \\bullet} \\right)^{2}}{I - 1} \\right\\} = \\frac{n}{I - 1}\\left\\lbrack \\sum_{}^{}{\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2} + \\frac{\\left( I - 1 \\right)\\sigma^{2}}{n}} \\right\\rbrack \\\\ &amp; = \\sigma^{2} + \\frac{n\\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2}}{I - 1} \\\\ \\end{split} \\\\ \\\\ \\end{matrix} \\] TABLA DE ANÁLISIS DE LA VARIANZA ANOVA de un factor Fuente de variación SC GL CM E(CM) Entre tratamientos \\({\\sum_{i}^{}{n_{i}\\left( y_{\\text{i}\\bullet} - {\\overline{y}}_{\\bullet\\bullet} \\right)}}^{2}\\) \\(I\\ - \\ 1\\) \\(CM_E = \\frac{SC_E}{I - 1}\\) \\(\\sigma^{2} + \\frac{1}{I - 1}\\sum_{}^{}n_{i}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2}\\) Error (dentro de tratamientos) \\(\\sum_{i}^{}{\\sum_{j}^{}\\left( y_{ij} - {\\overline{y}}_{\\text{i}\\bullet} \\right)^{2}}\\) \\(N- I\\) \\(CM_D = \\frac{SC_D}{N - I}\\) \\(\\sigma^{2}\\) Total \\(\\sum_{i}^{}{\\sum_{j}^{}\\left( y_{ij} - {\\overline{y}}_{\\bullet\\bullet} \\right)^{2}}\\) \\(N - 1\\) 5.12 Prueba F para la Igualdad de las Medias de los Niveles del Factor Las conclusiones alternativas a ser consideradas son: \\[ \\begin{aligned} H_{0} &amp;: u_{1} = u_{2} = \\ldots = u_{I}\\\\ H_{a} &amp;: \\text{no todos los } \\mu_{i}\\ \\text{son iguales} \\end{aligned} \\] 5.12.1 Prueba Estadística La prueba estadística a ser usada para elegir entre las hipótesis planteadas, es: \\[ F^{*} = \\frac{CM_E}{CM_D} \\] La prueba apropiada es de una cola a la derecha. 5.12.2 Distribución de \\(\\mathbf{F}^{\\mathbf{*}}\\) Cuando todas las medias de los tratamientos son iguales, cada observación \\(Y_{ij}\\) tiene el mismo valor esperado. En vista de la aditividad de la suma de cuadrados y de los grados de libertad, del teorema de Cochran se sigue que: Cuando \\(H_{0}\\) se verifica, \\(SCE/\\sigma^{2}\\) y \\(SCD/\\sigma^{2}\\) son variables distribuidas como \\(\\chi^{2}\\) independientes. Por lo tanto: cuando \\(H_{0}\\) se verifica, \\(F^{*}\\) se distribuye como \\(F_{\\left( I - \\ 1 \\right)\\left( \\ N\\ - \\ I \\right)}\\). Si \\(H_{a}\\) se verifica, esto es, si los \\(\\mu_{i}\\) no son todos iguales, \\(F^{*}\\) no sigue una distribución \\(F\\). Es más, sigue una distribución compleja llamada distribución \\(F_{ no\\ central}\\). 5.12.3 Regla De Decisión Dado que se sabe que \\(F^{*}\\) se distribuye como \\(F_{\\left( I - \\ 1 \\right)\\left( \\ N\\ - \\ I \\right)}\\). cuando se verifica \\(H_{0}\\) y que grandes valores de \\(F^{*}\\) llevan a concluir \\(H_{a}\\) , la regla de decisión para controlar el nivel de significación \\(\\alpha\\) es: Si \\(F^{*} \\leq F_{(1 - \\alpha;I - 1;\\ N - I)}\\) no se rechaza \\(H_{0}\\). Si \\(F^{*} &gt; F_{(1 - \\alpha;I - 1;\\ N - I)}\\) se rechaza \\(H_{0}\\). donde \\(F^{*} \\leq F_{(1 - \\alpha;I - 1;\\ N - I)}\\) es el percentil del \\(\\left( 1 - \\alpha \\right) \\times 100\\) de la distribución de \\(F\\). 5.12.4 Comentario Si hay sólo dos niveles del factor esto es \\(I = 2\\), se ve fácilmente que la prueba empleando \\(F^{*}\\) es equivalente a la prueba de “\\(t\\)” a dos colas para dos poblaciones. La prueba de \\(F\\) tiene \\((1,\\ N - 2)\\) grados de libertad, y la prueba “\\(t\\)” tiene \\((n_1 + n_2 -2)\\) o \\((N-2)\\) grados de libertad, así ambas pruebas conducen a regiones críticas equivalentes. Para comparar las medias de dos poblaciones, la prueba de “\\(t\\)” debe preferirse. 5.13 Formulación Alternativa Del Modelo I MODELO I DE ANOVA - MODELO DE LOS EFECTOS DEL FACTOR Con esta formulación las medias de los tratamientos son expresadas de un modo equivalente por medio de la identidad: \\[ \\mu_{i} \\equiv \\mu_{\\bullet} + \\left( \\mu_{i} - \\mu_{\\bullet} \\right) \\] donde \\(u_{\\bullet}\\) es una constante. Se denotará la diferencia: \\[ (u_{i} - u_{\\bullet}) = \\alpha_{i}\\ \\] esto implica que: \\(u_{i} = u_{\\bullet} + \\alpha_{i}\\) La diferencia \\(u_{i} = u_{\\bullet} + \\alpha_{i}\\) es llamada el efecto del i-ésimo nivel del factor. El modelo I de ANOVA puede ser expresado como sigue: \\[ Y_{ij} = u_{\\bullet} + \\alpha_{i} + \\varepsilon_{ij} \\] donde: \\(u_{\\bullet}\\) es una componente constante común a todas las observaciones. \\(\\alpha_{i}\\) es el efecto del i-ésimo nivel del factor (constante para cada nivel del factor) \\(\\varepsilon_{ij}\\) son variables independientes que se distribuyen \\(N(0,\\ \\sigma^{2})\\) \\[ i = 1,\\ 2,\\ldots,\\ I;\\ j = 1,\\ 2,\\ldots,\\ n_{i}\\ \\] El modelo de ANOVA es llamado el modelo de los efectos del factor pues se expresa en términos de los efectos del factor \\(\\alpha_{i}\\) en distinción del modelo de las medias de las celdas, el cual se expresa en términos de las medias de los tratamientos. El modelo de los efectos del factor es un modelo lineal, como su modelo equivalente de las medias de las celdas. 5.13.1 Definición de \\(\\mathbf{\\mu}_{\\mathbf{\\bullet}}\\) Medias no pesadas: A menudo, una definición de \\(\\mu_{\\bullet}\\) como un promedio no pesado para todas las medias de los niveles del factor \\(\\mu_{i}\\) puede ser útil: \\[ \\mu_{\\bullet} = \\frac{\\sum_{i = 1}^{I}\\mu_{i}}{I} \\] Esta definición implica que \\[ \\sum_{i = 1}^{I}\\alpha_{i} = 0 \\] pues: \\[ \\sum_{}^{}\\alpha_{i} = \\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right) = \\sum_{}^{}\\mu_{i} - I\\mu_{\\bullet} \\] y \\[ \\sum_{}^{}\\mu_{i} = I\\mu_{\\bullet} \\] Así la definición de la constante general \\(\\text{μ.}\\) implica una restricción sobre los \\(\\mu_{i}\\), en este caso que su suma debe ser cero. Medias pesadas: La constante \\(\\text{μ.}\\) también puede definirse como un promedio pesado de las medias de los niveles del factor \\(\\mu_{i}\\): \\[ \\mu_{\\bullet} = \\sum_{i = 1}^{I}{f_{i}\\mu_{i}\\ } \\] donde los \\(f_{i}\\) son pesos definidos tales que \\(\\sum f_{i} = 1\\) La restricción sobre los \\(\\alpha_{i}\\) es entonces: \\[ \\sum_{i = 1}^{I}{f_{i}\\alpha_{i}\\ } = 0 \\] La elección de los pesos \\(f_{i}\\) puede depender de la significación de las medidas resultantes de los efectos de los niveles del factor. Por ejemplo, los pesos se pueden dar de acuerdo a: a) una medida conocida de importancia o b) de acuerdo al tamaño muestral. Cuando los tamaños muestrales son iguales se usa una media no pesada. 5.14 Prueba Para La Igualdad De Las Medias De Los Niveles Del Factor Dado que el modelo de efectos del factor es equivalente al modelo de las medias de las celdas, la prueba para igualdad de las medias de los niveles del factor es la misma prueba estadística \\(F^{*}\\). La única diferencia está en el planteo de las hipótesis. Para el modelo de las medias de las celdas las hipótesis son: \\[ \\begin{aligned} H_{0} &amp;:\\ u_{1} = u_{2} = \\ldots = u_{I}\\\\ H_{a} &amp;:\\ \\text{no todos los}\\ u_{i}\\ \\text{son iguales} \\end{aligned} \\] Para el modelo de los efectos del factor, estas mismas hipótesis en términos de los efectos del factor son: \\[ \\begin{aligned} H_{0} &amp;:\\ \\alpha_{1} = \\alpha_{2} = \\ldots = \\alpha_{I} = 0\\\\ H_{a} &amp;:\\ \\text{no todos los}\\ \\alpha_{i}\\ \\text{son iguales} \\end{aligned} \\] 5.15 Análisis De Los Efectos Del Nivel Del Factor Si la prueba de F lleva a la conclusión de que las medias de los niveles del factor µi difieren, se sigue que hay una relación entre el factor y la variable dependiente. En este caso, un análisis cuidadoso de la naturaleza de los efectos de los niveles del factor es usualmente emprendido. Esto se hace de dos maneras: Un análisis directo de los efectos de los niveles de interés del factor usando técnicas de estimación. Pruebas estadísticas con respecto a los efectos de los niveles del factor de interés. 5.15.1 Gráficos de las estimaciones de las medias de los niveles del factor Se dispone de dos tipos de gráficos (1) una línea, la que es apropiada tanto si los tamaños de las muestras \\(n_{i}\\) son iguales como si no; y (2) un gráfico de probabilidad normal, la que es apropiada si los tamaños de las muestras \\(n_{i}\\) no son iguales. 5.15.2 Estimación de los efectos de los niveles del factor Las estimaciones de los efectos de los niveles del factor usualmente empleadas incluyen: Estimación de la media de un nivel del factor Estimación de la diferencia entre dos medias de dos niveles de un factor. Estimación de un contraste entre las medias de los niveles del factor. Estimación de una combinación lineal de las medias de los niveles del factor. 5.15.2.1 Estimación de la media del nivel del factor Un estimador insesgado de la media del nivel del factor µi, fue obtenido como: \\[ {\\hat{\\mu}}_{i} = {\\overline{Y}}_{i\\bullet} \\] Este estimador tiene media y varianza: \\[ \\begin{aligned} E\\left( {\\overline{Y}}_{i\\bullet} \\right) &amp;= \\mu_{i} \\\\ \\\\ \\text{Var}\\left( {\\overline{Y}}_{i\\bullet} \\right) &amp;= \\frac{\\sigma^{2}}{n_{i}} \\\\ \\end{aligned} \\] El último resultado se sigue pues \\({\\overline{Y}}_{i}. = \\mu_{i} + {\\overline{\\varepsilon}}_{\\text{i}\\bullet}\\), la suma de una constante a una media de \\(n_{i}\\) términos independientes \\(\\varepsilon_{ij}\\), cada uno de los cuales tiene una varianza \\(\\sigma^{2}\\). En consecuencia \\({\\overline{Y}} _{\\text{i}\\bullet}\\) está normalmente distribuido pues los términos del error \\(\\varepsilon_{ij}\\) son variables aleatorias normales e independientes. La varianza estimada de \\({\\overline{Y}}_{\\text{i}\\bullet}\\) se simboliza \\(S^{2}({\\overline{Y}}_{\\text{i}\\bullet})\\) \\[ S^{2} = \\frac{CM_D}{n_{i}} \\] Se puede demostrar que \\(\\frac{{\\overline{Y}}_{i\\bullet} - \\mu_{i}}{\\sqrt{\\frac{CM_D}{n_{i}}}}\\), se distribuye como \\(t_{N - I}\\). Se sigue que los límites del intervalo de confianza del \\((1 - \\alpha)\\) para \\(u_{i}\\) son: \\[ {\\overline{Y}}_{i\\bullet} \\pm t_{\\left( 1 - \\frac{\\alpha}{2};N - I \\right)}\\sqrt{\\frac{CM_D}{n_{i}}} \\] 5.15.2.2 Estimación de la diferencia entre dos medias de niveles del factor Frecuentemente dos tratamientos o niveles de un factor son comparados por estimación de la diferencia D entre las dos medias de los niveles del factor, o sea, \\(u_{i}\\) y \\(u_{i&#39;}\\): \\[ D = u_{i} - u_{i&#39;} \\] Tal comparación entre dos medias de niveles del factor será llamada comparación de a pares. Un estimador puntual es: \\[ \\overset{\\land}{D} = {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{i&#39;\\bullet} \\] Este estimador puntual es insesgado: \\[ E\\left( \\overset{\\land}{D} \\right) = \\mu_{i} - \\mu_{i&#39;} \\] Dado que \\({\\overline{Y}}_{\\text{i}\\bullet}\\) y \\({\\overline{Y}}_{i&#39;.}\\) son independientes, la varianza es: \\[ \\text{Var}\\overset{\\land}{\\left( D \\right)} = Var\\left( {\\overline{Y}}_{\\text{i}\\bullet} \\right) + Var\\left( {\\overline{Y}}_{i&#39;.} \\right) = \\sigma^{2}\\left( \\frac{1}{n_{i}} + \\frac{1}{n_{i&#39;}} \\right) \\] La varianza estimada está dada por: \\[ S^{2}\\left( \\overset{\\land}{D} \\right) = CM_D\\left( \\frac{1}{n_{i}} + \\frac{1}{n_{i&#39;}} \\right) \\] \\(\\overset{\\land}{D}\\) está normalmente distribuido por ser una combinación lineal de variables independientes normales. Se sigue, de estas características, que: \\(\\frac{\\overset{\\land}{D} - D}{S\\left( \\overset{\\land}{D} \\right)}\\sim t_{N - I}\\) para el modelo de ANOVA De esta manera un intervalo de confianza de \\((1 - \\alpha)\\) para \\(D\\) está dado por: \\[ \\overset{\\land}{D} \\pm t_{\\left( 1 - \\frac{\\alpha}{2};N - I \\right)}\\sqrt{CM_D\\left( \\frac{1}{n_{i}} + \\frac{1}{n_{i&#39;}} \\right)} \\] 5.15.2.3 Estimación de contrastes Un contraste es una comparación que involucra a dos o más medias e incluye al caso anterior de comparación de la diferencia entre un par de medias. Un contraste se simboliza como f y es una combinación lineal de las medias de los niveles del factor µi donde los coeficientes ci suman cero: \\[ f = \\sum_{i = 1}^{I}c_{i}\\mu_{i}\\text{ donde}\\sum_{i = 1}^{I}c_{i} = 0 \\] Ejemplos: Supongamos que estamos estudiando 4 niveles de un factor \\(f = \\mu_{1} - \\mu_{2}\\); aquí \\(c_{1} = 1\\); \\(c_{2} = - 1\\); \\(c_{3} = 0\\) y \\(c_{4} = 0\\); y \\(\\sum_{i = 1}^{4}c_{i} = 0\\). \\(f = \\frac{\\mu_{1} + \\mu_{2}}{2} - \\frac{\\mu_{3} + \\mu_{4}}{2}\\); aquí \\(c_{1}\\ = \\frac{1}{2}\\); \\(c_{2} = \\frac{1}{2}\\); \\(c_{3} = - \\frac{1}{2}\\) y \\(c_{4} = - \\frac{1}{2}\\); y \\(\\sum_{i = 1}^{4}c_{i} = 0\\). \\(f = \\frac{\\mu_{1} + \\mu_{3}}{2} - \\frac{\\mu_{2} + \\mu_{4}}{2}\\); aquí \\(c_{1} = \\frac{1}{2}\\); c2 = -1/2; \\(c_{3} = \\frac{1}{2}\\) y \\(c_{4} = - \\frac{1}{2}\\); y \\(\\sum_{i = 1}^{4}c_{i} = 0\\). Un estimador insesgado de un contraste \\(f\\) es: \\[ \\overset{\\land}{f} = \\sum_{i = 1}^{I}c_{i}{\\overline{Y}}_{\\text{i}\\bullet} \\] Dado que los \\({\\overline{Y}}_{\\text{i}\\bullet}\\) son independientes, la varianza de \\(\\overset{\\land}{f}\\) es: \\[ \\text{Var}\\left( \\overset{\\land}{f} \\right) = \\sum_{i = 1}^{I}c_{i}^{2}\\text{Var}\\left( {\\overline{Y}}_{i} \\right) = \\sum_{i = 1}^{I}c_{i}^{2}\\left( \\frac{\\sigma^{2}}{n_{i}} \\right) = \\sigma^{2}\\sum_{i = 1}^{I}\\frac{c_{i}^{2}}{n_{i}} \\] Un estimador insesgado de esta varianza es: \\[ S^{2}\\left( \\overset{\\land}{f} \\right) = CM_D\\sum_{i = 1}^{I}\\frac{c_{i}^{2}}{n_{i}} \\] \\(\\overset{\\land}{f}\\) está normalmente distribuida pues es una combinación lineal de variables independientes normalmente distribuidas. Se puede demostrar que: \\(\\frac{\\overset{\\land}{f} - f}{\\sqrt{CM_D\\sum_{i = 1}^{I}\\frac{c_{i}^{2}}{n_{i}}}}\\sim t_{\\left( N - I \\right)}\\) para el modelo de ANOVA 5.15.3 Comparaciones múltiples Planeados o A Priori: Se proponen antes de ver los resultados del experimento, pueden ser significativos aunque el ANOVA no dé significativo. No Planeados o A Posteriori: Se plantean a la vista de los resultados, se hacen sólo si el ANOVA da significativo. 5.15.3.1 Planeados LSD: el criterio de la prueba para examinar si existen diferencias significativas entre medias se llama diferencia mínima significativa y se simboliza \\(\\text{LSD}\\): \\[ \\begin{aligned} \\text{LSD}_{\\alpha}&amp; = t_{\\left( \\alpha\\left( 2 \\right);N - I \\right)}\\sqrt{CM_D\\left( \\frac{1}{n_{i}} + \\frac{1}{n_{i^{&#39;}}} \\right)} \\\\ &amp; = t_{\\left( \\alpha\\left( 2 \\right);N - I \\right)}\\sqrt{CM_D\\left( \\frac{2}{n} \\right)}\\text{ para }n_{i} = n\\ \\forall\\ i \\\\ \\end{aligned} \\] Se calcula\\(\\left| {\\overline{Y}}_{\\text{i}\\bullet} - {\\overline{Y}}_{i&#39;.} \\right|\\) y este valor se compara con \\(\\text{LSD}\\), en caso de que el primero sea mayor las diferencias son significativas. Método de Bonferroni o método “t”: Este método es aplicable ya sea que los tamaños muestrales sean iguales o no; o si se hacen comparaciones de a pares o contrastes. Un contraste \\[ f = \\sum_{i = 1}^{I}{c_{i}\\mu_{i\\bullet}} \\] estimo por medio de \\[ \\overset{\\land}{f} = \\sum_{i = 1}^{I}{c_{i}{\\overline{y}}_{i\\bullet}} \\] \\[ H_{0f}:\\ f = 0 \\] \\[ \\begin{aligned} \\text{Var}\\left( \\overset{\\land}{f} \\right) &amp;= \\text{Var}\\left( \\sum_{}^{}{c_{i}{\\overline{y}}_{i\\bullet}} \\right) \\\\ &amp;= \\sum_{}^{}c_{i}^{2}\\text{Var}\\left( {\\overline{y}}_{i\\bullet} \\right) \\\\ &amp;= \\sum_{}^{}c_{i}^{2}\\frac{\\overset{\\land}{\\sigma^{2}}}{n} \\\\ &amp;= \\overset{\\land}{\\sigma^{2}}\\sum_{i}^{}\\frac{c_{i}^{2}}{n_{i}} \\\\ &amp;= CM_D\\sum_{i}^{}\\frac{c_{i}^{2}}{n_{i}} \\\\ \\Rightarrow \\text{ES}\\left( \\overset{\\land}{f} \\right) &amp;= \\sqrt{CM_D\\sum_{i}^{}\\frac{c_{i}^{2}}{n_{i}}} \\\\ \\end{aligned} \\] Intervalo de confianza \\[ \\begin{matrix} \\overset{\\land}{f} \\pm t_{\\alpha(2);N - I}\\sqrt{CM_D\\sum_{i}^{}\\frac{c_{i}^{2}}{n_{i}}} \\\\ \\text{Si}\\ 0 \\in \\left\\lbrack \\ \\right\\rbrack \\Rightarrow no\\ rechazo\\ H_{0} \\\\ \\varepsilon = \\frac{\\overset{\\land}{f}}{\\sqrt{CM_D\\sum_{i}^{}\\frac{c_{i}^{2}}{n_{i}}}} \\\\ \\text{VC} = t_{\\alpha(2);N - I} \\\\ \\text{Si }\\varepsilon &gt; \\text{VC} \\Rightarrow rechazo\\ H_{0} \\\\ \\end{matrix} \\] \\(m\\) contrastes \\(f_{1},\\ f_{2},\\ \\ldots,\\ f_{m}\\) Se fija el número de contrastes a realizarse, junto con el experimento, m contrastes. Se toma como nivel para cada contraste \\(\\frac{\\alpha}{m} = \\alpha_{i} \\Rightarrow \\sum_{i}^{}\\alpha_{i} = \\alpha\\). Intervalo de confianza \\[ {\\overset{\\land}{f}}_{i} \\pm t_{\\frac{\\alpha}{m}(2);N - I}\\sqrt{CM_D\\sum_{i}^{}\\frac{c_{i}^{2}}{n_{i}}} \\] 5.15.3.2 No Planeados: Método de Scheffé: Este método da, para cada contraste, intervalos de confianza de la forma: \\[ \\overset{\\land}{f} \\pm \\text{VC}\\ \\text{ES}\\left( \\overset{\\land}{f} \\right) \\] donde \\[ \\begin{matrix} \\overset{\\land}{f} = \\sum_{i}^{}{c_{i}{\\overline{y}}_{i\\bullet}} \\\\ \\text{VC} = \\sqrt{\\left( I - 1 \\right)F_{I - 1;N - I;\\alpha}} = S \\\\ \\text{ES}\\left( \\overset{\\land}{f} \\right) = \\sqrt{CM_D\\sum_{i}^{}\\frac{c_{\\ _{i}}^{2}}{n_{i}}} \\\\ \\end{matrix} \\] Si usamos el estadístico \\(\\varepsilon = \\frac{\\overset{\\land}{f}}{\\text{ES}\\left( \\overset{\\land}{f} \\right)}\\) rechazo \\(H_{0}\\) sí \\(\\varepsilon &gt; S\\) Método de Tukey Utiliza el método de rangos studentizado. Supongamos que se tiene \\(r\\) observaciones independientes \\(Y_{1},\\ \\ldots,\\ Y_{r}\\) de una distribución normal con media \\(u\\) y varianza \\(\\sigma^{2}\\). Llamamos w al rango de estas observaciones; así: \\[ w = max(Y_{i}) - min(Y_{i}) \\] Supongamos que se tiene una estimación \\(S^{2}\\) de la varianza \\(\\sigma^{2}\\) la cual está basada sobre ν grados de libertad. El cociente \\(w/s\\) es llamado rango studentizado y se denota: \\[ q\\left( r,\\nu \\right) = \\frac{w}{S} \\] \\[ \\begin{matrix} \\varepsilon = \\frac{{\\overline{y}}_{\\text{i}\\bullet\\text{max}} - {\\overline{y}}_{\\text{i}\\bullet\\text{min}}}{S_{{\\overline{y}}_{\\bullet\\bullet}}}\\sim q_{I;N - I} \\\\ S_{{\\overline{y}}_{\\bullet\\bullet}} = \\sqrt{\\frac{CM_D}{n}} \\\\ \\end{matrix} \\] Sólo se puede usar si los \\(n = n_{i}\\ \\forall\\text{\\ i}\\). Si \\(\\text{n\\ } \\neq \\ n_{i}\\) se usa \\(n = min(n_{i}\\ ,\\ n_{j})\\) Si el \\(\\varepsilon &gt; q_{I;N - I;\\alpha\\ }\\), rechazo \\(H_{0}\\). 5.15.3.3 Ni planeados ni no planeados Contrastes Ortogonales: son contrastes tales que: \\[\\begin{equation} \\begin{aligned} f = \\sum{c_{i}\\mu_{i}}\\\\ \\sum {c_{i}} &amp;= 0 \\\\ \\sum{c_{i}^{j}c_{i}^{j&#39;}} &amp;= 0\\ \\forall j \\neq j&#39; \\end{aligned} \\end{equation}\\] donde \\(j\\) y \\(j&#39;\\) son contrastes diferentes. Para aplicar estos contrastes se supone que los \\(n_{i} = n\\ \\forall i\\) ; y que el número de contrastes ortogonales es el mismo que los grados de libertad entre, es decir el número de tratamientos menos uno. Ejemplo: Supongamos que tenemos cuatro niveles de un factor T1 T2 T3 T4 Entonces sólo se pueden hacer 3 contrastes. Sean, por ejemplo: \\[ \\begin{aligned} f_{1} &amp;= \\mu_{1} - \\frac{\\mu_{2}}{3} - \\frac{\\mu_{3}}{3} - \\frac{\\mu_{4}}{3} \\\\ f_{2} &amp;= \\mu_{3} - \\mu_{4} \\\\ f_{3} &amp;= \\mu_{2} - \\frac{\\mu_{3}}{2} - \\frac{\\mu_{4}}{2} \\\\ c_{1} &amp;= \\begin{pmatrix} 1 &amp; - \\frac{1}{3} &amp; - \\frac{1}{3} &amp; - \\frac{1}{3} \\\\ \\end{pmatrix} \\\\ c_{2} &amp;= \\begin{pmatrix} 0 &amp; 0 &amp; 1 &amp; - 1 \\\\ \\end{pmatrix} \\\\ c_{3} &amp;= \\begin{pmatrix} 0 &amp; 1 &amp; - \\frac{1}{2} &amp; - \\frac{1}{2} \\\\ \\end{pmatrix} \\\\ \\end{aligned} \\] Se puede comprobar que cada uno es un contraste porque \\(\\sum{c_i}=0\\) y si hacemos la multiplicación de a pares se comprueba su ortogonalidad: \\[ \\begin{aligned} \\sum{c_i^1c_i^2} &amp;= 1 &amp;\\times &amp; 0 &amp; + &amp; -\\frac{1}{3} &amp; \\times &amp; 0 &amp; + &amp; -\\frac{1}{3} &amp;\\times &amp; 1 &amp; + &amp; -\\frac{1}{3} &amp; \\times &amp; -1 &amp;= 0 \\\\ \\sum{c_i^1c_i^3} &amp;= 1 &amp;\\times &amp; 0 &amp; + &amp; -\\frac{1}{3} &amp; \\times &amp; 1 &amp; + &amp; -\\frac{1}{3} &amp;\\times &amp; -\\frac{1}{2} &amp; + &amp; -\\frac{1}{3} &amp; \\times &amp; -\\frac{1}{2} &amp;= 0\\\\ \\sum{c_i^2c_i^3} &amp;= 0 &amp;\\times &amp; 0 &amp; + &amp; 0 &amp; \\times &amp; 1 &amp; + &amp; 1 &amp;\\times &amp; -\\frac{1}{2} &amp; + &amp; -1 &amp; \\times &amp; -\\frac{1}{2} &amp;= 0 \\end{aligned} \\] Esto es equivalente a tener una matriz ortogonal de coeficientes. Para estudiar la significación de los contrastes se debe encontrar un estadístico y compararlo con algún valor crítico. La idea es descomponer la \\(SCE\\) en \\(\\text{SC}\\) independientes cada un grado de libertad. En el ejemplo la \\(SCE\\) se descompone en: \\[ SCE\\ = \\ SC_{1,\\ 2,\\ 3,\\ 4}\\ + \\ SC_{3,\\ 4}\\ + \\ SC_{2,3,4} \\] El procedimiento para calcular cada una de las sumas de cuadrados es: \\[ \\begin{matrix} SC_{f_{i}} = \\frac{{\\overset{\\land}{f}}_{i}^{2}}{\\frac{\\sum_{}^{}c_{i}^{2}}{n}} = \\frac{{\\overset{\\land}{f}}_{i}^{2}n}{\\sum_{}^{}c_{i}^{2}} \\\\ \\text{donde}\\ {\\overset{\\land}{f}}_{i} = \\sum_{}^{}c_{i}{\\overline{Y}}_{\\text{i}\\bullet} \\\\ \\end{matrix} \\] El cociente \\[ \\frac{SC_{f_{i}}}{CM_D}\\sim F_{\\ _{1,N - I;\\alpha}} \\] Ejemplo 1.- Comparaciones Se midió el contenido de nitrógeno tres suelos. ## ## Study: nitro_aov ~ &quot;trt&quot; ## ## LSD t Test for nitro ## ## Mean Square Error: 274.3452 ## ## trt, means and individual ( 95 %) CI ## ## nitro std r LCL UCL Min Max ## A 275.250 20.04816 8 263.0717 287.4283 242 300 ## B 293.625 13.79376 8 281.4467 305.8033 282 320 ## C 288.625 15.19340 8 276.4467 300.8033 264 314 ## ## Alpha: 0.05 ; DF Error: 21 ## Critical Value of t: 2.079614 ## ## least Significant Difference: 17.22271 ## ## Treatments with the same letter are not significantly different. ## ## nitro groups ## B 293.625 a ## C 288.625 ab ## A 275.250 b ⇒ Existen diferencias entre los sitios A y B en lo que a contenido de nitrógeno se refiere. Ejemplo 2.- Comparaciones a posteriori Figura 5.7: Presion de cuatro especies de ratas. media +- error estándar, n = 10. Test de Sheffé ## ## Study: ratas_aov ~ &quot;especie&quot; ## ## Scheffe Test for presion ## ## Mean Square Error : 9.25 ## ## especie, means ## ## presion std r Min Max ## A 84.5 3.922867 10 79 92 ## B 88.0 2.748737 10 84 92 ## C 91.1 2.378141 10 87 95 ## D 88.8 2.898275 10 85 93 ## ## Alpha: 0.05 ; DF Error: 36 ## Critical Value of F: 2.866266 ## ## Minimum Significant Difference: 3.988455 ## ## Means with the same letter are not significantly different. ## ## presion groups ## C 91.1 a ## D 88.8 a ## B 88.0 ab ## A 84.5 b Test de Tukey ## ## Study: ratas_aov ~ &quot;especie&quot; ## ## HSD Test for presion ## ## Mean Square Error: 9.25 ## ## especie, means ## ## presion std r Min Max ## A 84.5 3.922867 10 79 92 ## B 88.0 2.748737 10 84 92 ## C 91.1 2.378141 10 87 95 ## D 88.8 2.898275 10 85 93 ## ## Alpha: 0.05 ; DF Error: 36 ## Critical Value of Studentized Range: 3.808798 ## ## Minimun Significant Difference: 3.663185 ## ## Treatments with the same letter are not significantly different. ## ## presion groups ## C 91.1 a ## D 88.8 a ## B 88.0 ab ## A 84.5 b Polinomios ortogonales   Df Sum Sq Mean Sq F value Pr(&gt;F) especie 3 224.6 74.87 8.094 0.0003005 especie: A vs C-D 1 198 198 21.41 4.672e-05 especie: A vs B 1 0.1333 0.1333 0.01441 0.9051 especie: C vs D 1 26.45 26.45 2.859 0.09948 Residuals 36 333 9.25 NA NA Tabla: Modelo de Análisis de la Varianza Existen diferencias entre A vs C –D y no existen entre C y D. Ejercicio: Comprobar si los contrastes son ortogonales. Prueba de homogeneidad de varianzas   Df F value Pr(&gt;F) group 3 0.7086 0.5532 36 NA NA Tabla: Prueba de Levene para homogeneidad de varianzas (centro = mediana).   Sum Sq Df F value Pr(&gt;F) especie 224.6 3 8.094 0.0003005 Residuals 333 36 NA NA Tabla: ANOVA (Tipo II) Medias marginales estimadas especie lsmean SE df lower.CL upper.CL A 84.5 0.9618 36 82.55 86.45 B 88 0.9618 36 86.05 89.95 C 91.1 0.9618 36 89.15 93.05 D 88.8 0.9618 36 86.85 90.75 Pruebas post hoc Prueba contrast estimate SE df t.ratio p.value Tukey A - B -3.5 1.4 36 -2.57 0.06553 Tukey A - C -6.6 1.4 36 -4.85 0.00013 Tukey A - D -4.3 1.4 36 -3.16 0.01606 Tukey B - C -3.1 1.4 36 -2.28 0.12197 Tukey B - D -0.8 1.4 36 -0.59 0.93501 Tukey C - D 2.3 1.4 36 1.69 0.34314 Bonferroni A - B -3.5 1.4 36 -2.57 0.08604 Bonferroni A - C -6.6 1.4 36 -4.85 0.00014 Bonferroni A - D -4.3 1.4 36 -3.16 0.01908 Bonferroni B - C -3.1 1.4 36 -2.28 0.17213 Bonferroni B - D -0.8 1.4 36 -0.59 1 Bonferroni C - D 2.3 1.4 36 1.69 0.59688 LSD A - B -3.5 1.4 36 -2.57 0.01434 LSD A - C -6.6 1.4 36 -4.85 2e-05 LSD A - D -4.3 1.4 36 -3.16 0.00318 LSD B - C -3.1 1.4 36 -2.28 0.02869 LSD B - D -0.8 1.4 36 -0.59 0.56009 LSD C - D 2.3 1.4 36 1.69 0.09948 Scheffe A - B -3.5 NA NA NA 0.1041 Scheffe A - C -6.6 NA NA NA 4e-04 Scheffe A - D -4.3 NA NA NA 0.0301 Scheffe B - C -3.1 NA NA NA 0.1779 Scheffe B - D -0.8 NA NA NA 0.9506 Scheffe C - D 2.3 NA NA NA 0.4253 5.16 Planificación Del Tamaño Muestral Diseño De Estudios De ANOVA La planificación de los tamaños muestrales es una parte integral del diseño en un estudio de ANOVA. Se asumirá que todos los niveles del factor tienen el mismo tamaño muestral 5.16.1 Potencia De La Prueba F La potencia de la prueba \\(F\\) es la probabilidad de rechazar \\(H_{0}\\) cuando \\(H_{0}\\) es falsa, o también se puede pensar como la probabilidad de no rechazar \\(H_{a}\\) cuando \\(H_{a}\\) es cierta. Específicamente la potencia está dada por la siguiente expresión: \\[ P = P\\left( F^{*} &gt; F_{\\left( \\alpha;I - 1,N - I \\right)}\\left| \\phi \\right.\\ \\right) \\] donde \\(\\mathbf{\\phi}\\) es un parámetro de no-centralidad, que es una medida de cuan distintas son las \\(\\mu_i\\): \\[ \\phi = \\frac{1}{\\sigma}\\sqrt{\\frac{\\sum_{}^{}{n_{i}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2}}}{I}} \\] y \\[ \\mu_{\\bullet} = \\frac{\\sum_{}^{}{n_{i}\\mu_{i}}}{N} \\] Cuando todos los tamaños muestrales son iguales, el parámetro ϕ es: \\[ \\phi = \\frac{1}{\\sigma}\\sqrt{\\frac{n\\sum_{}^{}\\left( \\mu_{i} - \\mu_{\\bullet} \\right)^{2}}{I}}\\text{con}\\ n = n_{i} \\] donde: \\[ \\mu_{} = \\frac{\\sum_{}^{}\\mu_{i}}{I} \\] Para determinar la potencia, se necesita utilizar la distribución F no-centrada, dado que ésta es la distribución muestral de \\(F^{*}\\) cuando \\(H_{a}\\) es cierta. Los cálculos son bastantes complejos, pero se han preparado gráficos que permiten determinar la potencia relativamente fácil. Estos son los gráficos de Pearson-Hartley de la potencia de la prueba \\(F\\). La curva a utilizar depende del número de niveles del factor, del tamaño muestral y del nivel de significación empleado en la regla de decisión. Estos gráficos se usan de la siguiente forma: Cada página se refiere a diferentes \\(\\nu_{1}\\), los grados de libertad del numerador de \\(F^{*}\\). Para el modelo de ANOVA \\(\\nu_{1} = I1\\). Dos niveles de significación, indicados por \\(\\alpha\\), son usados en los gráficos, \\(\\alpha = 0.05\\) y \\(\\alpha = 0.01\\). Hay dos escalas de \\(X\\), dependiendo de cual es el nivel de significación empleado. De esta forma, el grupo de curvas de la izquierda corresponde a \\(\\alpha = 0.05\\) y el de la derecha a \\(\\alpha = 0.01\\). Hay curvas separadas para diferentes valores de \\(\\nu_{2}\\), los grados de libertad del denominador de \\(F^{*}\\). Para el modelo de ANOVA \\(\\nu_{2} = NI\\). Las curvas son indicadas de acuerdo al valor de \\(\\nu_{2}\\), en la parte superior del gráfico. Dado que sólo son usados en la tabla valores seleccionados de \\(\\nu_{2}\\), es necesario interpolar para valores intermedios de \\(\\nu_{2}\\) La escala de \\(X\\) representa a \\(\\phi\\), el parámetro no-central. La escala de \\(Y\\) da la potencia \\(1 - \\beta\\), donde \\(\\beta\\) es la probabilidad de cometer el error de tipo II. Ejemplo 3.- Consideremos el caso donde \\(\\nu_{1} = 2\\), \\(\\nu_{2} = 10\\), \\(\\phi = 10\\) y \\(\\alpha = 0.05\\). Si buscamos en la tabla la potencia es \\(1 - \\beta = 0.983\\) aproximadamente. Una forma alternativa para determinar la potencia es especificar la mínima diferencia que se desea detectar entre las medias de las dos poblaciones más diferentes. Designaremos a esta diferencia mínima detectable \\(\\delta\\), calculamos entonces: \\[ \\phi = \\sqrt{\\frac{n\\delta^{2}}{2IS^{2}}} \\] Ejemplo 4.- Supongamos que especificamos que trabajaremos con \\(n = 10\\), y que deseamos detectar, entre cuatro tratamientos, diferencias entre las medias de al menos 4 unidades. De un estudio piloto se sabe que \\(S^{2} = 7.5888\\). Trabajamos con \\(\\alpha = 0.05\\). \\(I\\ = \\ 4\\nu_{1} = 3\\) \\(n = 10\\) \\(\\nu_{2} = 36\\) \\(\\delta = 4.0\\) \\(S^{2} = 7.5888\\) \\[ \\begin{aligned} \\phi &amp;= \\sqrt{\\frac{n\\delta^{2}}{2IS^{2}}}\\\\ \\phi &amp;= \\sqrt{\\frac{10\\left( 4.0 \\right)^{2}}{2\\left( 4 \\right)\\left( 7.5888 \\right)}}\\\\ \\phi &amp;= \\sqrt{2.6355}\\\\ \\phi &amp;= 1.62 \\end{aligned} \\] En la tabla obtenemos una potencia igual a \\(0.72\\); lo que implica una probabilidad de cometer el error de tipo II del \\(28\\%\\). Si bien es deseable estimar la potencia antes de realizar el ANOVA, es útil, también, preguntarse con que potencia se ha realizado un ANOVA. Esto es especialmente interesante si la \\(H_{0}\\) no se ha rechazado, pues entonces es deseable saber cuan bien la prueba detecta las diferencias entre las medias de la población. Calculamos \\(\\phi\\), de la siguiente forma: \\[ \\phi = \\sqrt{\\frac{\\left( I - 1 \\right)\\left( CM_E - S^{2} \\right)}{IS^{2}}} \\] Ejemplo 5.- Los datos de la tabla corresponden a una muestra recogida de tres poblaciones de aves geográficamente aisladas. Se midió la longitud del pico con una precisión de un décimo de mm; obteniéndose los siguientes datos: Población A B C 4.2 3.8 3.0 3.3 4.1 3.5 2.8 5.0 4.5 4.3 4.6 4.4 3.7 5.1 4.5 3.6 \\[ \\begin{aligned} H_{0}&amp;:\\ u_{A} = u_{B} = u_{C}\\\\ H_{a}&amp;:\\ \\text{No todas las medias de las poblaciones son iguales.} \\end{aligned} \\] Fte. de Variación SC GL CM F P VC Entre Dentro 1.7977143 5.0322857 2 13 0.8988571 0.3870989 2.322 0.137322 3.806 Total 6.83 15 No rechazamos \\(H_{0}\\) con \\(p &gt; 0.05\\) \\[ \\phi = \\sqrt{\\frac{\\left( I - 1 \\right)\\left( CM_E - S^{2} \\right)}{IS^{2}}} = \\sqrt{\\frac{\\left( 3 - 1 \\right)\\left( 0.898871 - 0.387098 \\right)}{3\\left( 0.3870989 \\right)}} = 0.9388053 \\] Consultando la tabla la Potencia es 0.25; por lo cual la probabilidad de cometer error de tipo II es aproximadamente de 0.75. Se puede observar que grandes valores de \\(\\phi\\) están asociados con grandes potencias, de las ecuaciones vistas anteriormente se ve que \\(\\phi\\) se incrementa con: incremento del tamaño muestral; incremento entre las diferencias de las medias de las poblaciones (medida ya sea por \\(CM_E\\) , o por \\(\\sum_{}^{}\\left( \\mu_{i} - \\mu_{} \\right)^{2}\\) o por la mínima diferencia detectable); un bajo número de niveles del factor o de tratamientos; una disminución de la variabilidad dentro de las poblaciones, \\(\\sigma^{2}\\), estimada por \\(S^{2}\\) o el \\(CM_D\\). Ejemplo 6.- Veamos qué pasa con el experimento anterior al aumentar el tamaño de la muestra. POBLACIÓN A B C 3.9 4.6 3.7 3.5 4.1 4.2 4.1 4.5 3.6 4.4 4.4 4.0 4.4 3.7 3.3 4.6 4.6 3.5 3.3 3.9 4.0 3.9 4.6 4.4 4.4 4.5 3.5 3.6 3.7 4.1 3.7 4.1 3.9 3.4 4.2 4.3 Fte. de Variación SC GL CM F p VC Entre 0.9433318 2 0.4716659 3.179 0.05458498 3.285 Dentro 4.89465511 33 0.14832288 Total 5.8379869 35 \\[ \\mathbf{\\phi}\\ \\approx \\ 1.21 \\] La potencia es entonces 0.4 Para el uso de las tablas vistas anteriormente se hace necesario la realización de un experimento. Pero existen tablas que proporcionan los tamaños muestrales adecuados directamente. Este método es aplicable cuando todos los niveles del factor tienen el mismo tamaño muestral, esto es \\(n = n_{i}\\). La planificación del tamaño de la muestra usando estas tablas se hace en términos del parámetro de no-centralidad, para tamaños muestrales iguales. Sin embargo, en lugar de requerir una especificación directa de los niveles de \\(u_{i}\\) para los cuales es importante controlar la probabilidad de cometer el error de tipo II; esta tabla sólo requiere una especificación del rango mínimo de las medias de los niveles del factor para los cuales es importante detectar diferencias entre los \\(u_{i}\\), con alta probabilidad. Este rango mínimo se indica \\(\\Delta\\): \\[ \\Delta = max\\left( u_{i} \\right)min\\left( u_{i} \\right) \\] Las siguientes especificaciones son necesarias para hacer uso de la tabla: El nivel de significación \\(\\alpha\\) La magnitud del rango mínimo \\(\\Delta\\) de los \\(u_{i}\\), la cual es importante detectar con alta probabilidad. La magnitud de \\(\\sigma\\), la desviación estándar de \\(Y\\), debe también ser especificada para entrar en la tabla en términos del cociente: \\(\\frac{\\Delta}{\\sigma}\\) El nivel de \\(\\beta\\). Entrar en la tabla en términos de \\(1 - \\beta\\). Cuando se usa la tabla están disponibles cuatro niveles de \\(\\alpha\\ (0.2;0.1;0.05\\ y\\ 0.01)\\). También hay cuatro niveles de β a través de la potencia. La tabla provee tamaños muestrales para estudios \\(\\text{de}\\ I = 2,\\ldots,10\\) niveles del factor o tratamientos. Ejemplo 7.- 1) Supongamos que se quiere con un rango mínimo \\(\\Delta = 3\\), para comparar cuatro tratamientos. Se sabe por estudios anteriores que \\(\\sigma\\) es aproximadamente igual a 2. Los niveles para controlar los errores son: \\[ \\alpha = 0.05\\ \\beta = 0.10\\ o\\ P = 1 - \\beta = 0.90 \\] Entramos a la tabla para \\(\\frac{\\Delta}{\\sigma} = \\frac{3}{2} = 1.5\\); \\(\\alpha = 0.05\\); \\(1 - \\beta = 0.9\\) e \\(I = 4\\). Encontramos que \\(n = 14\\). Especificación de \\(\\frac{\\Delta}{\\sigma}\\) directa: El rango mínimo también se puede especificar en términos de unidades de desviación estándar. \\[ \\frac{\\Delta}{\\sigma} = \\frac{k\\sigma}{\\sigma} = k \\] En nuestro ejemplo supongamos que el rango de las medias es k = 2 o más. Supongamos que las otras especificaciones son: \\[ \\alpha = 0.01\\ \\beta = 0.05\\ o\\ 1 - \\beta = 0.95 \\] En la tabla encontramos que \\(n = 9\\). En el ejemplo se hace necesario incrementar el tamaño de la muestra. Para ello nos preguntamos cuál es el tamaño de muestra necesario para, trabajando con \\(\\alpha = 0.05\\), tener una potencia de \\(0.80\\) para detectar diferencias tan pequeñas como \\(0.7\\). Suponemos que \\(S^{2} = 0.3870989\\) es una buena estimación de \\(\\sigma^{2}\\). Entramos a la tabla para \\(\\frac{\\Delta}{\\sigma} = \\frac{0.6}{\\sqrt{0.387089}} \\approx 1\\); \\(\\alpha = 0.05\\); \\(1 - \\beta = 0.8\\) e \\(I = 3\\) Encontramos que \\(n = 21\\). 5.17 Modelo II De ANOVA: Niveles Del Factor Aleatorios Existen situaciones en las cuales los niveles del factor o los tratamientos empleados no tienen un interés en sí mismos, pero constituyen una muestra de la población. El Modelo II de ANOVA está diseñado para este tipo de situaciones. 5.17.1 Modelo Aleatorio de Medias de Celdas. El modelo II de ANOVA para un factor es: \\[ Y_{ij}\\ = \\ u_{i}\\ + \\ \\varepsilon_{ij} \\] donde \\(u_{i}\\) son variables independientes \\(\\sim N\\left( \\mu_{\\bullet},\\sigma_{\\mu}^{2} \\right)\\) \\(\\varepsilon{ij}\\) son variables independientes \\(\\sim N\\left( 0,\\sigma^{2} \\right)\\) \\(u_{i}\\) y \\(\\varepsilon_{ij}\\) son variables aleatorias independientes \\[ i = 1,\\ 2,\\ldots,\\ I;\\ j = 1,\\ 2,\\ \\ldots,\\ n_{i} \\] 5.17.2 Características importantes del Modelo El valor esperado de una observación \\(Y_{ij}\\) es: \\[ E(Y_{ij}) = u_{\\bullet} \\] esto se debe a que: \\[ \\begin{aligned} E\\left( Y_{ij} \\right)&amp; = E\\left( u_{\\bullet}\\ \\right) + \\ E\\left( \\varepsilon_{ij} \\right) \\\\ &amp; = \\ u_{\\bullet}\\ \\ + \\ 0 \\\\ &amp; = \\ u_{\\bullet} \\\\ \\end{aligned} \\] La varianza de \\(Y_{ij}\\), que se indica \\(\\sigma_{Y}^{2}\\), es: \\[ \\text{Var}\\left( Y_{ij} \\right) = \\sigma_{Y}^{2} = \\sigma_{\\mu}^{2} + \\sigma^{2} \\] A causa de que la varianza de Y en este modelo es la suma de dos componentes, este modelo se llama, algunas veces, un modelo de componentes de la varianza. Los \\(Y_{ij}\\) están normalmente distribuidos pues son una combinación lineal de variables independientes, \\(u_{i}\\) y \\(\\varepsilon_{ij}\\), distribuidas normalmente Las \\(Y_{ij}\\) para el modelo aleatorio son sólo independientes si pertenecen a diferentes tratamientos o niveles del factor. Se puede demostrar que la covarianza para cualesquiera dos observaciones \\(Y_{ij}\\) e \\(Y_{{ij}&#39;}\\), para el mismo nivel i con un modelo II es: \\[ Cov(Y_{ij},\\ Y_{ij&#39;}) = \\sigma_{Y}^{2}\\; \\forall\\ j \\neq j \\] El modelo II supone que la covarianza entre cualesquiera dos observaciones para el mismo nivel del factor es constante para todos los niveles del factor. Una vez que los niveles del factor han sido seleccionados, el modelo II asume que dos observaciones cualesquiera para el mismo nivel del factor son independientes pues la media del nivel del factor µi es entonces fijada y las dos observaciones difieren sólo por los términos del error \\(\\varepsilon_{ij}\\). 5.17.3 Cuestiones de Interés Cuando el modelo aleatorio es apropiado, uno no está particularmente interesado en inferencias sobre un \\(u_{i}\\) particular incluido en el estudio, ya sea si es grande o pequeño, pero sí en inferencias acerca de la población completa de \\(mu_{i}\\). Específicamente, el interés a menudo se centra sobre la media de los \\(mu_{i}\\), \\(u_{}\\), y en la variabilidad de los \\(mu_{i}\\) medida por \\(\\sigma_{\\mu}^{2}\\). Dado que \\(\\sigma_{\\mu}^{2}\\) es una medida directa de la variabilidad de los \\(mu_{i}\\), el efecto de esa variabilidad, a menudo, es medido por el cociente: \\[ \\frac{\\sigma_{\\mu}^{2}}{\\sigma_{\\mu}^{2} + \\sigma^{2}} \\] El cociente toma valores entre \\(0\\ (\\sigma^{2} = \\infty)\\) y \\(1\\ (\\sigma^{2} = 0)\\). El denominador es \\(\\sigma_{Y}^{2}\\). En vista de las propiedades 1 y 2, el cociente mide la proporción de la variabilidad total de los \\(Y_{ij}\\) que se debe a la variabilidad en los \\(\\mu_{i\\bullet}\\). 5.17.4 Prueba para \\(\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}\\) = 0 Consideremos como decidir entre \\[ H_{0}:\\ \\sigma_{\\mu}^{2} = 0 \\] \\[ H_{a}:\\ \\sigma_{\\mu}^{2} &gt; 0 \\] \\(H_{0}\\) implica que todos los \\(mu_{i}\\) son iguales, esto es, \\(mu_{i} = u_{\\bullet}\\). \\(H_{a}\\) implica que los \\(mu_{i}\\) difieren. La diferencia entre los dos modelos aparece en los valores esperados de los cuadrados medios. Se puede demostrar de misma forma que lo hemos hecho para el modelo I, que: \\[ E(CM_D) = \\sigma^{2} \\] \\[ E\\left( CM_E \\right) = \\sigma^{2} + \\ n\\sigma_{\\mu}^{2} \\] donde \\[ n = \\frac{1}{I - 1}\\left\\lbrack \\left( \\sum_{}^{}n_{i} \\right) - \\frac{\\sum_{}^{}n_{i}^{2}}{\\sum_{}^{}n_{i}} \\right\\rbrack \\] Sí todos los \\(n_{i} = n\\), entonces \\(n = n\\) Es claro que si \\(\\sigma_{\\mu}^{2} = 0\\), el \\(CM_D\\) y el \\(CM_E\\) tienen el mismo valor esperado \\(\\sigma^{2}\\). Por otro lado \\(E\\left( CM_E \\right) &gt; \\ E(CM_D)\\) dado que \\(n &gt; 0\\) siempre. En consecuencia, grandes valores de la prueba estadística: \\[ F^{*} = \\frac{CM_E}{CM_D} \\] nos llevará a rechazar \\(H_{0}\\). Dado que \\(F^{*}\\) sigue la distribución \\(F\\) cuando \\(H_{0}\\) es verdadera, la regla de decisión es la misma que para el modelo I: Si \\(F^{*} \\leq F_{(1 - \\alpha;I - 1;\\ N - I)}\\) no se rechaza \\(H_{0}\\). Si \\(F^{*} &gt; F^{*} \\leq F_{(1 - \\alpha;I - 1;\\ N - I)}\\) se rechaza \\(H_{0}\\). Ejemplo 8.- Un laboratorio emplea una cierta técnica para determinar el contenido de fósforo en el forraje del ganado bovino. La cuestión planteada es “si las determinaciones de fósforo dependen de las técnicas empleadas para el análisis”. Para contestar esta pregunta se seleccionaron al azar cuatro técnicas con cinco observaciones para la misma tanda de forraje, obteniéndose los siguientes resultados: Técnica 1 Técnica 2 Técnica 3 Técnicas 4 34 37 34 36 36 36 37 34 34 35 35 37 35 37 37 34 34 37 36 35 \\(H_{0}\\): La determinación del contenido de fósforo no difiere entre las técnicas. \\(H_{a}:\\) La determinación del contenido de fósforo difiere entre técnicas. Fte. de Variación SC GL CM F p VC General Ejemplo Entre 9 3 3 2.4 0.10589 3.23886 \\(\\sigma^2 + n´ \\sigma_{\\mu}^{2}\\) \\(\\sigma^2 + 5 \\sigma_{\\mu}^{2}\\) Dentro 20 16 1.25 \\(\\sigma^2\\) \\(\\sigma^2\\) Total 29 19 No se rechaza \\(H_0\\) Niveles del Factor \\(n_{i}\\) Media muestral Varianza muestral 1 5 34.6 0.8 2 5 36.4 0.8 3 5 35.8 1.7 4 5 35.2 1.7 5.17.5 Estimación De \\(\\mathbf{\\mu}_{\\mathbf{\\bullet}}\\) Se sabe que: \\[ E(Y_{ij}) = u_{\\bullet} \\] Así, un estimador insesgado de \\(\\mu_{\\bullet}\\) es: \\[ {\\hat{\\mu}}_{i} = {\\overline{Y}}_{\\bullet\\bullet} \\] Se puede demostrar que la varianza de este estimador es: \\[ S^{2}\\left( {\\overline{Y}}_{\\bullet\\bullet} \\right) = \\frac{\\sigma_{\\mu}^{2}}{I} + \\frac{\\sigma^{2}}{N} = \\frac{n\\sigma_{\\mu}^{2} + \\sigma^{2}}{N} \\] Recordar que \\(N = I\\ n.\\) Se ve que la varianza está formada por dos componentes. Un estimador insesgado de esta varianza es: \\[ S^{2}\\left( {\\overline{Y}}_{\\bullet\\bullet} \\right) = \\frac{CM_E}{N} \\] es un estimador insesgado pues, cuando ni = n: \\[ E\\left( CM_E \\right) = n\\sigma_{\\mu}^{2} + \\sigma^{2} \\] Se puede demostrar que: \\(\\frac{{\\overline{Y}}_{\\bullet\\bullet} - \\mu_{\\bullet}}{S\\left( {\\overline{Y}}_{\\bullet\\bullet} \\right)}\\sim t_{\\left( I - 1 \\right)}\\), cuando \\(n_{i} = n.\\) Así, de la forma usual se obtienen los límites del intervalo de confianza para µ•: \\[ {\\overline{Y}}_{\\bullet\\bullet} \\pm t_{I - 1;\\alpha\\left( 2 \\right)}S\\left( {\\overline{Y}}_{\\bullet\\bullet} \\right) \\] Ejemplo 9.- En el estudio de contenido de fósforo del forraje del ganado bovino. Se tiene: \\[ {\\overline{Y}}_{\\bullet\\bullet} = 35.5\\ CM_E = 3\\ N = 20 \\] Necesitamos \\(t_{3;\\ 0.05\\left( 2 \\right)} = 3.182\\) y \\(S^{2}\\left( {\\overline{Y}}_{\\bullet \\bullet} \\right) = \\frac{3}{20} = 0.15\\), entonces \\(S\\left( {\\overline{Y}}_{\\bullet \\bullet} \\right) = 0.38729833\\); el intervalo de confianza del 95% es: \\[ 34.27 \\leq \\ u_{\\bullet} \\leq 36.73 \\] 5.17.6 Estimación De \\(\\sigma_{\\mu}^2/\\left ( \\sigma_{\\mu}^2+\\sigma^2 \\right )\\) El cociente \\(\\sigma_{\\mu}^2/\\left (\\sigma_{\\mu}^2+\\sigma^2 \\right )\\) revela el alcance del efecto de la varianza entre los \\(mu_{i}\\). Para desarrollar un intervalo de confianza para este cociente, se supone que todos los tamaños muestrales de los niveles del factor son iguales. Comenzaremos obteniendo un intervalo de confianza para el cociente \\(\\frac{\\sigma_{\\mu}^{2}}{\\sigma^{2}}\\). El \\(CM_E\\) y el \\(CM_D\\) son variables aleatorias independientes para el modelo II de ANOVA, lo mismo que para el modelo I. Cuando \\(n_{i} = n\\), se puede demostrar que: \\[ \\frac{CM_E}{n\\sigma_{\\mu}^{2} + \\sigma^{2}} + \\frac{CM_D}{\\sigma^{2}}\\sim F_{I - 1,N - I} \\] Así, se puede escribir la probabilidad: \\[ P\\left( F_{\\left( 1 - \\frac{\\alpha}{2} \\right);I - 1,N - I} \\leq \\frac{CM_E}{n\\sigma_{\\mu}^{2} + \\sigma^{2}} + \\frac{CM_D}{\\sigma^{2}} \\leq F_{\\left( \\frac{\\alpha}{2} \\right);I - 1,N - I} \\right) = 1 - \\alpha \\] Reordenando las desigualdades, se obtienen los siguientes límites \\(S\\) e \\(I\\) para \\(\\frac{\\sigma_{\\mu}^{2}}{\\sigma^{2}}\\) \\[ \\begin{matrix} I = \\frac{1}{n}\\left\\lbrack \\frac{CM_E}{CM_D}\\left( \\frac{1}{F_{\\frac{\\alpha}{2};I - 1,N - I}} \\right) - 1 \\right\\rbrack \\\\ S = \\frac{1}{n}\\left\\lbrack \\frac{CM_E}{CM_D}\\left( \\frac{1}{F_{1 - \\frac{\\alpha}{2};I - 1,N - I}} \\right) - 1 \\right\\rbrack \\\\ \\end{matrix} \\] donde \\(I\\) es el límite inferior y \\(S\\) el superior. Los límites \\(I^{*}\\) y \\(S^{*}\\) para \\(\\frac{\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}}{\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}\\mathbf{+}\\mathbf{\\sigma}^{\\mathbf{2}}}\\) pueden ser obtenidos como sigue: \\[ I^{*} = \\frac{I}{1 + I}S^{*} = \\frac{S}{1 + S} \\] Ejemplo 9 cont.- En nuestro ejemplo \\[ CM_E = 3\\ CM_D = 1.25\\ n = 5\\ I = 4\\ N = 20 \\] Para construir el intervalo de confianza del 95% se necesita: \\[ F_{0.975;\\ 3,\\ 19} = 0.071\\ F_{0.025;\\ 3,\\ 19} = 3.093 \\] De esta manera los límites del 95% para \\(\\frac{\\sigma_{\\mu}^{2}}{\\sigma^{2}}\\) son: \\[ I = \\frac{1}{5}\\left\\lbrack \\frac{3}{1.25}\\left( \\frac{1}{3.093} \\right) - 1 \\right\\rbrack = - 0.077S = \\frac{1}{5}\\left\\lbrack \\frac{3}{1.25}\\left( \\frac{1}{0.071} \\right) - 1 \\right\\rbrack = 6.561 \\] Cuando el límite inferior del intervalo de confianza para \\(\\frac{\\sigma_{\\mu}^{2}}{\\sigma^{2}}\\) es negativo, la práctica usual es considerarlo como 0. Entonces el intervalo de confianza es: \\[ 0 \\leq \\frac{\\sigma_{\\mu}^{2}}{\\sigma^{2}} \\leq 6.561 \\] Finalmente, los límites de confianza para \\(\\frac{\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}}{\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}\\mathbf{+}\\mathbf{\\sigma}^{\\mathbf{2}}}\\) son: \\[ 0\\ \\leq \\text{ }\\frac{\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}}{\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}\\mathbf{+}\\mathbf{\\sigma}^{\\mathbf{2}}} \\leq \\ 0.87 \\] Concluimos que la variabilidad de la media de las determinaciones de fósforo se encuentra entre 0 y 87% de la varianza total. Estimación de σ2 y \\(\\mathbf{\\sigma}_{\\mathbf{\\mu}}^{\\mathbf{2}}\\) Un estimador insesgado para \\(\\sigma^{2}\\) es: \\[ {\\overset{\\land}{\\sigma}}^{2} = CM_D \\] Y el intervalo de confianza se obtiene como: \\[ \\frac{\\left( N - I \\right)S^{2}}{\\chi_{0.025;N - I}^{2}} \\leq \\sigma^{2} \\leq \\frac{\\left( N - I \\right)S^{2}}{\\chi_{0.975,N - I}^{2}} \\] También se puede obtener un estimador insesgado de \\(\\sigma_{\\mu}^{2}\\): \\[ E(CM_D) = \\sigma^{2} \\] \\[ E(CM_E) = \\sigma^{2} + n\\sigma_{\\mu}^{2} \\] Se sigue que: \\[ {\\overset{\\land}{\\sigma}}_{\\mu}^{2} = \\frac{CM_E - CM_D}{n} \\] Ejemplo 9 cont.- \\[ CM_D = 1.25\\; \\chi_{0.975,16}^{2} = 6.908\\; \\chi_{0.025;16}^{2} = 28.845 \\] El intervalo de confianza es: \\[ 0.693 = \\frac{16\\left( 1.25 \\right)}{28.845} \\leq \\sigma^{2} \\leq \\frac{16\\left( 1.25 \\right)}{6.908} = 2.895 \\] Una estimación insesgada de \\(\\sigma_{\\mu}^{2}\\) es: \\[ {\\overset{\\land}{\\sigma}}_{\\mu}^{2} = \\frac{3 - 1.25}{5} = 0.35 \\] 5.17.7 Modelo De Efectos Aleatorios El modelo se puede expresar como: \\[ Y_{ij} = u_{\\bullet} + \\alpha_{i} + \\varepsilon_{ij} \\] donde \\(\\mu_{\\bullet}\\) es una componente constante común a todas las observaciones \\(\\alpha_{i}\\) son variables aleatorias independientes \\(\\sim N(0,\\sigma_{\\mu}^{2})\\) \\(\\varepsilon_{ij}\\) son variables aleatorias independientes \\(\\sim N(0,\\sigma^{2})\\) \\(\\alpha_{i}\\) y \\(\\varepsilon_{ij}\\) son independientes \\[ i = 1,2,\\ldots,I;j = 1,2,\\ldots,n_{i} \\] "],
["problemas-anova-simple.html", "Capítulo 6 Problemas ANOVA Simple 6.1 Problemas", " Capítulo 6 Problemas ANOVA Simple Para analizar datos con ANOVA en R hay que conocer unas pocas funciones como mínimo. aov() ajusta el modelo de ANOVA especificado a los datos. summary() muestra un resumen del resultado, junto con la típica tabla de ANOVA. autoplot() realiza automaticamente los gráficos de diagnóstico más comunes. La función aov() tiene dos argumentos principales. En primer lugar, la formula que define el modelo. Las formulas estadísticas tienen dos partes, una izquierda y una derecha y se separan con el signo ~. La parte izquierda define los términos dependientes, que será una variable el caso de estadística univariada o o varias en estadística multivariada. La parte derecha define los términos explicatorios o independientes. Por ejemplo, y ~ x indica que y depende de la variable x. La otra parte importante es el argumento data que indica donde se encuentran esas variables. Si les aparece un error del tipo object ‘y’ not found es muy probable que hayan especificado mal este argumento o se lo hayan olvidado. Un ejemplo concreto, con los datos de contenido de nitrógeno en tres suelos. La variable dependiente es el nitrógeno, y la explicatoria es el tipo de suelo. Estas corresponden a las columnas nitro y trt respectivamente. library(tidyverse) # Cargar datos nitro &lt;- read_csv(&quot;data/nitrogeno.csv&quot;) nitro ## # A tibble: 8 x 3 ## A B C ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 270 309 281 ## 2 255 295 264 ## 3 278 320 291 ## 4 294 283 285 ## 5 292 285 314 ## 6 300 288 298 ## 7 242 282 298 ## 8 271 287 278 # Poner los datos en formato largo # Una columna para la variable dependiente # Una columna para la variable explicatoria nitro &lt;- nitro %&gt;% gather(trt, nitro) nitro ## # A tibble: 24 x 2 ## trt nitro ## &lt;chr&gt; &lt;int&gt; ## 1 A 270 ## 2 A 255 ## 3 A 278 ## 4 A 294 ## 5 A 292 ## 6 A 300 ## 7 A 242 ## 8 A 271 ## 9 B 309 ## 10 B 295 ## # ... with 14 more rows nitro_aov &lt;- aov(nitro ~ trt, data = nitro) nitro_aov ## Call: ## aov(formula = nitro ~ trt, data = nitro) ## ## Terms: ## trt Residuals ## Sum of Squares 1444.083 5761.250 ## Deg. of Freedom 2 21 ## ## Residual standard error: 16.56337 ## Estimated effects may be unbalanced Por si solo, la impresión de los resultados no da demasiada información. En primer lugar, el la llamada que usamos para calcular el ANOVA. En segundo lugar, cuales son los términos del modelo, junto con sus repectivas suma de cuadados (Sum of Squares) y grados de libertad (Deg. of Freedom). Y finalmente, el error estándar residual o sea \\(\\sqrt{\\hat\\sigma^2}\\). Para obtener la tabla de ANOVA es necesario usar la función summary summary(nitro_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trt 2 1444 722.0 2.632 0.0955 . ## Residuals 21 5761 274.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Como vemos, esto nos devuelve la típica tabla de ANOVA. Además, para saber si el ajuste ha sido adecuado podemos ver los gráficos de residuales library(ggfortify) autoplot(nitro_aov) Por lo que vemos en el los gráficos no hay motivo para preocuparse por la falta de cumplimiento de los supuestos. Pero para estar seguros podemos usar las pruebas que vimos en la teoría: la prueba de bartlett y la prueba de levene. bartlett.test(nitro ~ trt, data = nitro) ## ## Bartlett test of homogeneity of variances ## ## data: nitro by trt ## Bartlett&#39;s K-squared = 1.0316, df = 2, p-value = 0.597 library(car) leveneTest(nitro ~ trt, data = nitro) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.7634 0.4786 ## 21 Como ven ambas funciones trabajan de manera similar a aov(). Ambas aceptan formulas y necesitan del argumento data. Ejercicio 6.1 ¿Qué concluirían si el test de Bartlett rechaza la hipótesis nula y el de Levene no lo hace? 6.0.1 Recordatorio Recuerden que pueden calcular las medias, desvios, etc. por grupo usando la función group_by() y a continuación la función summarise(). Por ejemplo: nitro %&gt;% group_by(trt) %&gt;% summarise( media = mean(nitro), desvio_estandar = sd(nitro), n = n() ) ## # A tibble: 3 x 4 ## trt media desvio_estandar n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 275. 20.0 8 ## 2 B 294. 13.8 8 ## 3 C 289. 15.2 8 Con esta misma secuencia se puede calcular el test de Lilliefors para normalidad. Aunque la secuencia algo más compleja porque el objeto que devuelve este test no es un único número sino que son varios. nitro %&gt;% group_by(trt) %&gt;% summarise(normalidad = list(lillie.test(nitro))) %&gt;% mutate(statistic = map_dbl(normalidad, &quot;statistic&quot;), p.value = map_dbl(normalidad, &quot;p.value&quot;)) ## # A tibble: 3 x 4 ## trt normalidad statistic p.value ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A &lt;S3: htest&gt; 0.173 0.682 ## 2 B &lt;S3: htest&gt; 0.283 0.0580 ## 3 C &lt;S3: htest&gt; 0.144 0.897 Voy a explicar que es lo que hice. Los pasos hasta summarise() son similares a los que se usan para calcular la media, desvío estándar, etc. La función lillie.test devuelve varios números distintos y ¡summarise quiere solo un número! Para resolver este incoveniente hay que envolver todos estos números en otra objeto. Imaginen que cada número está dentro de una caja, si metemos todos estas cajas dentro de un cajón entonces tenemos solo un cajón por cada uno de nuestros niveles. Al cajón de está analogía se lo conoce como lista en R (list) y puede contener cajas de de cualquier tipo ¡Incluso mezcladas!. Ahora, la lista es cómoda para trabajar como estructura intermedia en nuestros calculos, pero no es cómoda para ver los resultados. De cada uno esos cajones queremos extraer la caja con los números que nos interesan, el valor del estadístico y la probabilidad (\\(P(X&gt;x)\\)). Para eso use la función mutate que agrega o cambia el valor de una columna y la función map. Es un poco compleja de explicar ahora como funciona esta última función, pero por ahora solo necesitan saber que la estamos usando para extraer la cajas de los cajones. Entonces, para ponerlo en castellano lo que estoy haciendo se puede traducir como: con los datos de nitro agrupalos por la variable trt; luego resumilos en una nueva variable normalidad, que es el resultado de probar la normalidad con la función lillie.test() de la variable nitro; luego, a ese resultado, agregar la variable statistic que es igual a extraer la caja statistic del cajón normalidad y la variable p.value que es igual a extraer la caja p.value del cajón normalidad. Ejercicio 3.3 1. ¿Cuales son las hipótesis que se prueban en la prueba de Lilliefors? ¿Se rechaza alguna de ellas? 6.1 Problemas Antes de comenzar bajen el archivo donde realizarán su informe reproducible. En la consola copien este código: download.file(&quot;git.io/informe-anova.Rmd&quot;, &quot;informe-anova.Rmd&quot;) Pueden abrirlo desde la pestaña de archivos, a la derecha. Cambien el nombre por el suyo en el encabezado y mientras leen este capítulo respondan las preguntas. Se lleva a cabo una experiencia para poner a prueba el efecto de 6 fertilizantes sobre el crecimiento de la soja, obteniéndose la siguiente tabla de ANOVA: Fte.de.Variación SC GL F p Entre 754.25 5 2.23 0.0644 Dentro 3646.08 54 Calcular la potencia. Enuncie sus conclusiones. ¿Cual es el n necesario para tener una potencia de 90%? Para calcular la potencia en R se puede usar la función power.anova.test() del paquete pwr. Esta función puede calcular tanto la potencia de una prueba como el n necesario para alcanzar cierta potencia, dependiendo de que dato falte completar. Para eso necesita: El número de grupos El n de cada grupo (esto implica que solo da resultados aproximados con datos desbalanceados) la varianza entre grupos. la varianza dentro de los grupos. A partir de la tabla de ANOVA es posible derivar todos los datos. # Número de grupos. Recordar que GL entre = K - 1 g &lt;- 5 + 1 # Número de réplicas por grupo N-I/I n &lt;- (54 + g) / g # Varianza entre grupos CM entre /n bv &lt;- 754.25 / 5 / n # Varianza dentro de grupos CM dentro wv &lt;- 3648.08/ 54 power.anova.test(groups = g, n = n, between.var = bv, within.var = wv) ## ## Balanced one-way analysis of variance power calculation ## ## groups = 6 ## n = 10 ## between.var = 15.085 ## within.var = 67.55704 ## sig.level = 0.05 ## power = 0.6821423 ## ## NOTE: n is number in each group Fueron ensayados dos cebos distintos para estimar si existía una diferencia significativa en función de su consumo por ratones silvestres. Los datos de la tabla se obtuvieron en cinco sitios diferentes por cebo y están expresados como porcentaje de consumo: cebos ## A B ## 1 10 15 ## 2 15 20 ## 3 12 16 ## 4 20 25 ## 5 14 20 Analizar la significación de estas observaciones. ¿Qué transformación es conveniente utlizar teniendo en cuenta el tipo de dato? La potencia de la prueba. Durante el estudio del control del fotoperíodo de la reproducción del alga roja Porphira, se llevó a cabo un experimento para examinar el efecto de la interrupción de largos períodos de oscuridad, mediante un período de iluminación de 30 minutos con luz de diferentes longitudes de onda, y se contaron los esporangios en un volumen fijo de material. Se obtuvieron 4 réplicas para cada una de las cinco longitudes de onda. ## Color N\\xfamero.de.esporangios NA. NA..1 NA..2 ## 1 Azul 7720 7490 7986 7382 ## 2 Verde 7918 7948 7632 8215 ## 3 Amarillo 6495 7101 7412 7006 ## 4 Rojo 4741 4150 5315 4810 ## 5 Infrarrojo 7520 7418 7937 7118 Teóricamente solamente la luz roja tiene efecto sobre el número de esporangios. Realizar un análisis para decidir si hay diferencia entre los efectos de las longitudes de onda. Poner a prueba el supuesto teórico. ¿Puede inferirse algún otro resultado? 6.1.1 Contrastes En R existen varias formas de hacer contrastes. Una de las más prácticas es usar el paquete emmeans que además de estimar la medias marginales también permite realizar comparaciones de a pares. La función eemeans devuelve por defecto las medias marginales junto con los errores estándar, grados de libertad, e intervalos de confianza. alga.em &lt;- emmeans(alga.aov, ~ Color) alga.em ## Color emmean SE df lower.CL upper.CL ## Amarillo 7003.50 175.4702 15 6629.494 7377.506 ## Azul 7644.50 175.4702 15 7270.494 8018.506 ## Infrarrojo 7498.25 175.4702 15 7124.244 7872.256 ## Rojo 4754.00 175.4702 15 4379.994 5128.006 ## Verde 7928.25 175.4702 15 7554.244 8302.256 ## ## Confidence level used: 0.95 También puede ser usada para estimar la comparación de a pares. Por defecto, usa el método de Tukey. Se pueden usar otros métodos como Bonferroni (bonferroni), Scheffé (scheffe), LSD (none), y otro más (para más detalles ver ?summary.emmGrid sección P-value adjustment) pairs(alga.em, adjust = &quot;none&quot;) ## contrast estimate SE df t.ratio p.value ## Amarillo - Azul -641.00 248.1523 15 -2.583 0.0208 ## Amarillo - Infrarrojo -494.75 248.1523 15 -1.994 0.0647 ## Amarillo - Rojo 2249.50 248.1523 15 9.065 &lt;.0001 ## Amarillo - Verde -924.75 248.1523 15 -3.727 0.0020 ## Azul - Infrarrojo 146.25 248.1523 15 0.589 0.5644 ## Azul - Rojo 2890.50 248.1523 15 11.648 &lt;.0001 ## Azul - Verde -283.75 248.1523 15 -1.143 0.2708 ## Infrarrojo - Rojo 2744.25 248.1523 15 11.059 &lt;.0001 ## Infrarrojo - Verde -430.00 248.1523 15 -1.733 0.1036 ## Rojo - Verde -3174.25 248.1523 15 -12.792 &lt;.0001 LSD.test(alga.aov, &quot;Color&quot;, console = TRUE) ## ## Study: alga.aov ~ &quot;Color&quot; ## ## LSD t Test for esporangios ## ## Mean Square Error: 123159.2 ## ## Color, means and individual ( 95 %) CI ## ## esporangios std r LCL UCL Min Max ## Amarillo 7003.50 380.7698 4 6629.494 7377.506 6495 7412 ## Azul 7644.50 267.7679 4 7270.494 8018.506 7382 7986 ## Infrarrojo 7498.25 338.6270 4 7124.244 7872.256 7118 7937 ## Rojo 4754.00 477.0891 4 4379.994 5128.006 4150 5315 ## Verde 7928.25 238.3868 4 7554.244 8302.256 7632 8215 ## ## Alpha: 0.05 ; DF Error: 15 ## Critical Value of t: 2.13145 ## ## least Significant Difference: 528.9242 ## ## Treatments with the same letter are not significantly different. ## ## esporangios groups ## Verde 7928.25 a ## Azul 7644.50 a ## Infrarrojo 7498.25 ab ## Amarillo 7003.50 b ## Rojo 4754.00 c También se pueden graficar los intervalos de confianza de las medias estimadas, y de las comparaciones de entre ellas. plot(alga.em, comparisons = TRUE) Las barras azules son los intervalos de confianza para las medias y las flechas rojas son los intervalos de confianza de las comparaciones entre ellos. Si las flechas no se superponen las diferencias son significativas entre ellos. También es posible obtener lo que se llama compact letter display que es una forma muy práctica de ver comparaciones. cld(alga.em) ## Color emmean SE df lower.CL upper.CL .group ## Rojo 4754.00 175.4702 15 4379.994 5128.006 1 ## Amarillo 7003.50 175.4702 15 6629.494 7377.506 2 ## Infrarrojo 7498.25 175.4702 15 7124.244 7872.256 23 ## Azul 7644.50 175.4702 15 7270.494 8018.506 23 ## Verde 7928.25 175.4702 15 7554.244 8302.256 3 ## ## Confidence level used: 0.95 ## P value adjustment: tukey method for comparing a family of 5 estimates ## significance level used: alpha = 0.05 Los niveles que no comparten números o letras son significativamente distintos. También podemos ver cuales son los coeficientes de los contrastes usados. coef(pairs(alga.em)) ## Color c.1 c.2 c.3 c.4 c.5 c.6 c.7 c.8 c.9 c.10 ## Amarillo Amarillo 1 1 1 1 0 0 0 0 0 0 ## Azul Azul -1 0 0 0 1 1 1 0 0 0 ## Infrarrojo Infrarrojo 0 -1 0 0 -1 0 0 1 1 0 ## Rojo Rojo 0 0 -1 0 0 -1 0 -1 0 1 ## Verde Verde 0 0 0 -1 0 0 -1 0 -1 -1 Ahora, la pregunta que nos hacen es poner a prueba el supuesto teórico de que solo las roja es efectiva. Podemos hacerlo de dos formas distintas. Una por contrastes ortogonales. Es quizás el método más complicado, aunque más poderoso, de hacer. Primero debemos implementar nuestros coeficientes. Los niveles del factor son ordenados por orden alfabético a menos que indiquemos otro orden. Por lo tanto, el orden de los niveles de Color es: Amarillo, Azul, Infrarrojo, Rojo, Creamos una matriz de tamaño I x (I-1). Cada columna es un contraste. contraste.algas &lt;- matrix(c(-1, -1, -1 , 4, -1, -1, -1, -1, 0, 3, -1, -1, 2, 0 , 0, 1, -1, 0, 0, 0), nrow = 5) row.names(contraste.algas) &lt;- levels(alga$Color) colnames(contraste.algas) &lt;- paste(&quot;c&quot;, 1:4, sep = &quot;.&quot;) contraste.algas ## c.1 c.2 c.3 c.4 ## Amarillo -1 -1 -1 1 ## Azul -1 -1 -1 -1 ## Infrarrojo -1 -1 2 0 ## Rojo 4 0 0 0 ## Verde -1 3 0 0 # Comprobar que son ortogonales, fuera de la diagonal debe dar 0 crossprod(contraste.algas) ## c.1 c.2 c.3 c.4 ## c.1 20 0 0 0 ## c.2 0 12 0 0 ## c.3 0 0 6 0 ## c.4 0 0 0 2 Una vez hecho la matriz de coeficientes, la usamos dentro de la función aov especificando el argumento contrasts que debe ser una lista con nombres igual a los variables explicatorias. alga.aov_or &lt;- aov(esporangios ~ Color, data = alga, contrasts = list(Color = contraste.algas)) Luego hay que hacer algo similar para que summary muestre esos contrastes. Especificar el argumento split que también tiene que ser una lista con los nombres de los variables explicatorias, pero dentro de cada uno hay un vector con nombres donde el número indica que contraste es. summary(alga.aov_or, split = list(Color = c(&quot;Rojo vs Todos&quot; = 1, &quot;Verde vs Amarillo, Azul, Infrarrojo&quot; = 2, &quot;Amarillo Azul vs Infrarrojo&quot; = 3, &quot;Amarillo vs Azul&quot; = 4))) ## Df Sum Sq Mean Sq F value ## Color 4 26255709 6563927 53.296 ## Color: Rojo vs Todos 1 24458084 24458084 198.589 ## Color: Verde vs Amarillo, Azul, Infrarrojo 1 894894 894894 7.266 ## Color: Amarillo Azul vs Infrarrojo 1 80968 80968 0.657 ## Color: Amarillo vs Azul 1 821762 821762 6.672 ## Residuals 15 1847388 123159 ## Pr(&gt;F) ## Color 1.09e-08 *** ## Color: Rojo vs Todos 4.67e-10 *** ## Color: Verde vs Amarillo, Azul, Infrarrojo 0.0166 * ## Color: Amarillo Azul vs Infrarrojo 0.4301 ## Color: Amarillo vs Azul 0.0208 * ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Noten las diferencias de resultados entres las comparaciones múltiples. La otra forma es usar comparaciones de a pares pero solo usando tratamientos vs control. En este caso nuestro “control” es el color rojo. Se puede hacer usando emmeans y resulta mucho más sencillo. La función a usar es contrast. Además de indicar el objeto sobre el que hay que realizar los contrastes, también es necesario indicar el método (method), y opcionalmente el número de nivel que corresponde al tratamiento control (ref) contrast(object = alga.em, method = &quot;trt.vs.ctrl&quot;, ref = 4) ## contrast estimate SE df t.ratio p.value ## Amarillo - Rojo 2249.50 248.1523 15 9.065 &lt;.0001 ## Azul - Rojo 2890.50 248.1523 15 11.648 &lt;.0001 ## Infrarrojo - Rojo 2744.25 248.1523 15 11.059 &lt;.0001 ## Verde - Rojo 3174.25 248.1523 15 12.792 &lt;.0001 ## ## P value adjustment: dunnettx method for 4 tests También es posible especificar un contraste o varios usando una lista donde cada item es un contraste: contrast(object = alga.em, list(&quot;Rojo vs Todos&quot; = c(-1, -1, -1, 4, -1), &quot;Amarillo vs Verde&quot; = c(1, 0, 0, 0, -1))) ## contrast estimate SE df t.ratio p.value ## Rojo vs Todos -11058.50 784.7266 15 -14.092 &lt;.0001 ## Amarillo vs Verde -924.75 248.1523 15 -3.727 0.0020 Es importante recordar que el orden de los coeficientes del contraste deben ser igual a que como estan ordenados los niveles del factor. La faciolasis es una enfermedad parasitaria producida por la Fasciola hepatica (trematode hepático). Los trematodes adultos viven en el conducto biliar del huésped, donde segregan cantidades significativas de ciertos aminoácidos, en especial prolina; el huésped presenta, como característica, anemia (reducción en los glóbulos rojos de la sangre). Se tomaron 40 ratas Wistar, sanas de aproximadamente igual peso y edad, se dividieron al azar en 4 grupos de 10 ratas cada uno. Se adaptó un aparato para infundir material directamente al conducto biliar de las ratas mediante una cánula. Las ratas del grupo I recibieron 20 minimoles de prolina disuelta en suero fisiológico, las del grupo II recibieron un cóctel consistente siete aminoácidos (excluyendo prolina) segregados por el trematode, también disuelto en suero fisiológico; el grupo III recibió lo mismo que el II más el agregado de 20 milimoles de prolina (simulando a lo segregado por el trematode) y el grupo IV sólo se trató con suero fisiológico. En todos los casos se tomó como variable el número de glóbulos rojos del huésped, expresados en millones por mm3 de sangre. Los resultados se presentan en la siguiente tabla: GRUPO.I GRUPO.II GRUPO.III 1 20 mmol prolina mezcla aa - prolina mezcla aa + prolina 2 6.07 5.69 5.61 3 5.02 5.54 5.40 4 5.69 5.35 5.26 5 5.43 5.11 4.99 6 5.87 5.94 5.44 7 5.55 5.25 5.13 8 5.64 6.02 5.21 9 5.95 5.64 5.52 10 5.20 5.11 4.79 11 5.40 5.04 4.92 GRUPO.IV 1 suero fisiológico 2 7.35 3 7.11 4 6.99 5 6.72 6 7.16 7 6.85 8 6.94 9 7.25 10 6.51 11 6.65 Plantear y comprobar todos los supuestos para la validez de las pruebas estadísticas utilizadas. ¿Está asociada la reducción del número de glóbulos rojos de la sangre del huésped con la segregación de aminoácidos por el trematode? ¿Está específicamente asociado a la segregación de prolina? Realice un breve comentario sobre el diseño del experimento. Se realiza una experiencia a fin de comparar tres métodos diferentes para determinar el contenido de oxígeno disuelto en el agua de lagos. Se extrae una muestra aleatoria de 18 muestras de agua de un lago, las cuales se dividen al azar en tres grupos de igual tamaño y cada uno de los grupos es asignado al azar a uno de los métodos que se quiere comparar. Se obtienen los siguientes resultados, expresados en mg/litro: Método.1 Método.2 Método.3 1 832 1023 8710 2 324 832 1660 3 550 1318 5495 4 617 1995 3981 5 525 832 2138 6 1349 912 3548 7 501 646 5130 Comprobar las suposiciones del ANOVA Poner a prueba la hipótesis “No hay efecto del método en la determinación de oxígeno en el agua del lago”. Indicar P. Realizar comparaciones entre métodos, utilizando todos los métodos de contraste conocidos e indicar cuáles serían los adecuados a este problema particular. Hallar la potencia de la prueba para alguna hipótesis alternativa. Estimar el tamaño de la muestra (¿de qué?) con la que debería trabajar para tener una potencia del 95%, con una probabilidad de cometer error de Tipo I del 5%. En un estudio sobre viabilidad, se aíslan tres parejas de Drosophila melanogaster en 10 frascos y se hace un recuento del número de huevos al cabo de 8 días. Esta experiencia se repite 4 veces con parejas distintas. Los resultados obtenidos son: mosca ## Serie.1 Serie.2 Serie.3 Serie.4 ## 1 47 28 32 50 ## 2 36 31 41 44 ## 3 22 32 44 67 ## 4 69 45 17 63 ## 5 68 72 96 87 ## 6 57 101 20 74 ## 7 37 55 45 21 ## 8 108 27 55 54 ## 9 29 49 36 91 ## 10 72 36 72 72 ¿Es posible reunir las cuatro series en una sola para efectuar un análisis conjunto de la viabilidad? Trabajar con = 0.05 Hallar la potencia de la prueba realizada cuando se dan ciertas alternativas. Estimar el tamaño de la muestra con que debería trabajar en cada tratamiento para tener una potencia mayor del 95%. "],
["anova-de-dos-factores.html", "Capítulo 7 ANOVA DE DOS FACTORES 7.1 Ventajas de los estudios multifactoriales 7.2 Elementos del Modelo 7.3 Representación gráfica 7.4 Interacción 7.5 MODELO I PARA ESTUDIOS DE DOS FACTORES 7.6 Prueba de F 7.7 Contrastes 7.8 Potencia de la prueba F 7.9 CASO DE UNA OBSERVACIÓN POR TRATAMIENTO 7.10 MODELO II Y MODELO III PARA ESTUDIOS DE DOS FACTORES", " Capítulo 7 ANOVA DE DOS FACTORES 7.1 Ventajas de los estudios multifactoriales 7.1.1 Eficiencia: 7.1.1.1 Cantidad de Información: Permiten estudiar la interacción de los factores. Validez de las decisiones: los experimentos multifactoriales también pueden robustecer la validez de las decisiones. Comentarios: Los análisis multifactoriales permiten una evaluación efectiva de los efectos de la interacción y economiza el número de casos requeridos para el análisis. Experimentos involucrando muchos factores, cada uno con numerosos niveles, se vuelven complejos, costosos e insumen tiempo. 7.2 Elementos del Modelo Ejemplo: Consideremos un estudio de dos factores, en el cual son de interés los efectos del sexo y la edad en el aprendizaje de una tarea. El factor edad lo definimos en términos de sólo tres niveles (adolescente, adulto joven, anciano). La respuesta para un dado tratamiento, en un estudio de dos factores, es indicada por µij, donde i hace referencia al nivel del factor \\(A\\) (\\(i = 1,2,\\ldots,I\\)) y j se refiere al nivel del factor \\(B\\) (\\(j = 1,2,\\ldots,J\\)). Tabla 7.1: Tiempo de aprendizaje (en minutos) en mujeres de y hombres de tres edades. Caso sin interacción y sin efecto del factor Sexo. SEXO EDAD Adolescente (j=1) Adulto joven (j=2) Anciano (j=3) \\(\\mathbf{\\mu_{i\\bullet}}\\) Masculino (\\(i = 1\\)) 9 (\\(\\mu_{11}\\)) 11 (\\(\\mu_{12}\\)) 16 (\\(\\mu_{13}\\)) 12 (\\(\\mu_{1\\bullet}\\)) Femenino (\\(i = 2\\)) 9 (\\(\\mu_{21}\\)) 11 (\\(\\mu_{22}\\)) 16 (\\(\\mu_{23}\\)) 12 (\\(\\mu_{2\\bullet}\\)) \\(\\mu_{\\bullet j}\\) 9 (\\(\\mu_{\\bullet1}\\)) 11 (\\(\\mu_{\\bullet2}\\)) 16 (\\(\\mu_{\\bullet3}\\)) 12 (\\(\\mu_{\\bullet\\bullet}\\)) Indicamos \\[ \\mu_{\\bullet j} = \\frac{\\sum_{i = 1}^{I}\\mu_{ij}}{I} \\] y \\[ \\mu_{i \\bullet} = \\frac{\\sum_{j = 1}^{J}\\mu_{ij}}{J} \\] La media general se indica cómo \\(\\mu_{\\bullet \\bullet}\\), y es definida en las siguientes formas equivalentes: \\[ \\begin{aligned} \\mu_{\\bullet \\bullet} &amp;= \\frac{\\sum_{i = 1}^{I}{\\sum_{j = 1}^{J}\\mu_{ij}}}{ij}\\\\ \\mu_{\\bullet \\bullet} &amp;= \\frac{\\sum_{i = 1}^{I}\\mu_{i \\bullet}}{I}\\\\ \\mu_{\\bullet \\bullet} &amp;= \\frac{\\sum_{j = 1}^{J}\\mu_{\\bullet j}}{J} \\end{aligned} \\] 7.2.1 Efectos principales Definimos el efecto principal del factor \\(A\\) al i-ésimo nivel, como: \\[ \\alpha_{i} = \\mu_{i \\bullet} - \\mu_{\\bullet \\bullet} \\] En el ejemplo: \\[ \\alpha_{1} = \\mu_{1 \\bullet} - \\mu_{\\bullet \\bullet} = 912 = - 3 \\] De forma similar, el efecto principal del j-ésimo nivel del facto B se define: \\[ \\beta_{j} = \\mu_{\\bullet j} - \\mu_{\\bullet \\bullet} \\] En el ejemplo: \\[ \\beta_{1} = \\mu_{\\bullet 1} - \\mu_{\\bullet \\bullet}\\bullet \\bullet = 1212 = 0 \\] Se sigue que: \\[ \\begin{matrix} \\sum_{i}^{}\\alpha_{i} = 0 &amp; \\sum_{j}^{}\\beta_{j} = 0 \\\\ \\end{matrix} \\] Así, la suma de los efectos principales para cada factor es cero. 7.2.1.1 Aditividad de los efectos de los factores En general, si los efectos son aditivos se tiene: \\[ \\mu_{ij} = \\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j} \\] lo que se puede expresar de forma equivalente, usando la definición de \\(\\alpha_{i}\\) y de \\(\\beta_{j}\\), como: \\[ \\mu_{ij} = \\mu_{\\bullet \\bullet} + \\mu_{i \\bullet} + \\mu_{\\bullet j} \\] En el ejemplo \\[ \\mu_{11} = \\mu_{\\bullet \\bullet} + \\alpha_{I} + \\beta_{j} = 12 + 0 + ( - 3) = 9 \\] Cuando todos los tratamientos pueden ser expresados en esta forma, se dice que los factores principales no interactúan, o que los efectos de los factores son aditivos. 7.3 Representación gráfica Una de las mejores formas para representar este tipo de datos es un gráfico de líneas y puntos. En el eje de las abscisas va una de las variables y en el eje de las ordenadas la variable de respuesta. Se grafica cada observación o media como punto y se unen los puntos de los niveles que son iguales para la otra variable. Este tipo de gráfico permite ver si las lineas son paralelas. Si lo son indica que los efectos de los factores son aditivos. La falta de paralelismo puede estar indicando que hay interacción. (ref:interaccion1) Gráfico de interacción para las medias del tiempo de aprendizaje. Los datos graficados corresponden a y d Tabla 7.1, b y e Tabla 7.2, c y f Tabla 7.3, Figura 7.1: (ref:interaccion1) Tabla 7.2: Tiempo de aprendizaje (en minutos) en mujeres de y hombres de tres edades. Caso sin interacción y efecto del factor Sexo. SEXO EDAD Adolescente (\\(j = 1\\)) Adulto joven (\\(j = 2\\)) Anciano (\\(j = 3\\)) \\(\\mu_{i\\bullet}\\) Masculino (\\(i = 1\\)) 11 (\\(\\mu_{11}\\)) 13 (\\(\\mu_{12}\\)) 18 (\\(\\mu_{13}\\)) 14 (\\(\\mu_{1 \\bullet}\\)) Femenino (\\(i = 2\\)) 7 (\\(\\mu_{21}\\)) 9 (\\(\\mu_{22}\\)) 14 (\\(\\mu_{23}\\)) 10 (\\(\\mu_{2 \\bullet}\\)) \\(\\mu_{\\bullet j}\\) 9 (\\(\\mu_{\\bullet 1}\\)) 11 (\\(\\mu_{\\bullet 2}\\)) 16 (\\(\\mu_{\\bullet 3}\\)) 12 (\\(\\mu_{\\bullet \\bullet}\\)) 7.4 Interacción \\[ \\mu_{ij} = \\mu_{\\bullet\\bullet} + \\alpha_i + \\beta_j \\] Si esto se cumple, los efectos serán aditivos; de lo contrario los factores interactúan. Tabla 7.3: Tiempo de aprendizaje (en minutos) en mujeres de y hombres de tres edades. Caso con interacción. SEXO EDAD Adolescente (\\(j = 1\\)) Adulto joven (\\(j = 2\\)) Anciano (\\(j = 3\\)) \\(\\mu_{i\\bullet}\\) Masculino (\\(i = 1\\)) 9 (\\(\\mu_{11}\\)) 12 (\\(\\mu_{12}\\)) 18 (\\(\\mu_{13}\\)) 13 (\\(\\mu_{1 \\bullet}\\)) 1( \\(\\alpha_{1}\\)) Femenino (\\(i = 2\\)) 9 (\\(\\mu_{21}\\)) 10 (\\(\\mu_{22}\\)) 14 (\\(\\mu_{23}\\)) 11 (\\(\\mu_{2 \\bullet}\\)) -1 (\\(\\alpha_{2}\\)) \\(\\mu_{\\bullet j}\\) 9 (\\(\\mu_{\\bullet 1}\\)) 11 (\\(\\mu_{\\bullet 2}\\)) 16 (\\(\\mu_{\\bullet 3}\\)) 12 (\\(\\mu_{\\bullet \\bullet}\\)) -3 (\\(\\beta_{1}\\)) -1 (\\(\\beta_{2}\\)) 4 (\\(\\beta_{3}\\)) Para el ejemplo de la tabla, es claro que los efectos de los factores interactúan, por ejemplo: \\[ \\mu_{\\bullet \\bullet} + \\alpha_{1} + \\beta_{1} = 12 + 1 + ( - 3) = 10 \\] mientras que \\(\\mu_{11} = 9\\). La diferencia entre la media del tratamiento \\(\\mu_{ij}\\) y el valor (\\(\\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j}\\)) es llamada la interacción del i-ésimo nivel del factor \\(A\\) con el j-ésimo nivel del factor \\(B\\), se simboliza \\(\\left( \\alpha\\beta \\right)_{ij}\\) y la definimos como: \\[ \\left( \\alpha\\beta \\right)_{ij} = \\mu_{ij} - (\\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j}) \\] Reemplazando \\(\\alpha_{i}\\) y \\(\\beta_{j}\\) por su definición, se obtiene la siguiente expresión alternativa: \\[ \\left( \\alpha\\beta \\right)_{ij} = \\mu_{ij} - \\mu_{I \\bullet} - \\mu_{\\bullet j} + \\mu_{\\bullet \\bullet} \\] Por ejemplo, la interacción para: \\[ \\begin{aligned} \\left( \\alpha\\beta \\right)_{13}&amp; = \\mu_{13}\\left( \\mu_{\\bullet \\bullet} + \\alpha_{1} + \\beta_{3} \\right) \\\\ &amp; = 18\\left( 12 + 1 + 4 \\right) \\\\ &amp; = 1 \\\\ \\end{aligned} \\] Tabla 7.4: Efectos de \\(\\alpha \\beta_{ij}\\). \\(j = 1\\) \\(j = 2\\) \\(j = 3\\) Promedio \\(i = 1\\) -1 0 1 0 \\(i = 2\\) 1 0 -1 0 Promedio 0 0 0 0 7.4.1 Interacciones no importantes Muchas veces se detectan interacciones pero estas son pequeñas y no cambian las conclusiones que se pueden sacar sobre el efecto de los factores principales. Tabla 7.5: Tiempo de aprendizaje (en minutos) en mujeres de y hombres de tres edades. Caso con interacción no importante. SEXO EDAD Adolescente (\\(j = 1\\)) Adulto joven (\\(j = 2\\)) Anciano (\\(j = 3\\)) \\(\\mu_{i\\bullet}\\) Masculino (\\(i = 1\\)) 9.75 (\\(\\mu_{11}\\)) 12 (\\(\\mu_{12}\\)) 17.25 (\\(\\mu_{13}\\)) 14 (\\(\\mu_{1 \\bullet}\\)) Femenino (\\(i = 2\\)) 8.25 (\\(\\mu_{21}\\)) 10 (\\(\\mu_{22}\\)) 14.75 (\\(\\mu_{23}\\)) 10 (\\(\\mu_{2 \\bullet}\\)) \\(\\mu_{\\bullet j}\\) 9 (\\(\\mu_{\\bullet 1}\\)) 11 (\\(\\mu_{\\bullet 2}\\)) 16 (\\(\\mu_{\\bullet 3}\\)) 12 (\\(\\mu_{\\bullet \\bullet}\\)) (ref:interaccion-no-importante) Gráfico de interacción para las medias del tiempo de aprendizaje. Los datos graficados corresponden a la Tabla 7.5 Figura 7.2: (ref:interaccion-no-importante) 7.4.1.1 Interacciones transformables y no transformables Efectos de los factores multiplicativos Consideremos el caso donde los efectos de los factores son multiplicativos, en lugar de aditivos: \\(\\mu_{ij} = \\mu_{\\bullet \\bullet}\\alpha_{i}\\beta_{j}\\) Estas interacciones pueden ser eliminadas aplicando la transformación logarítmica: \\[ \\log(\\mu_{ij}) = \\log(\\mu_{\\bullet \\bullet}) + \\log(\\alpha_{i}) + \\log(\\beta_{j}) \\] o sea \\[ {u&#39;}_{ij} = {u&#39;}_{\\bullet \\bullet} + \\alpha_{i}^{&#39;} + \\beta_{j}^{&#39;} \\] \\[ \\begin{matrix} \\mu_{ij}^{&#39;} = log \\mu_{ij} &amp; \\mu_{\\bullet \\bullet}^{&#39;} = log \\mu_{\\bullet \\bullet} &amp; \\alpha_{i}^{&#39;} = \\log\\alpha_{i} &amp; \\beta_{j}^{&#39;} = \\log\\beta_{j} \\end{matrix} \\] Entonces se usa la variable \\(Y&#39; = \\log Y\\) Cuando una simple transformación de Y remueve los efectos de la interacción o los hace poco importantes, decimos que la interacción es transformable. Interacciones multiplicativas Otro ejemplo de interacciones transformables aparece cuando cada efecto de interacción es igual al producto de funciones de los efectos principales. \\[ \\mu_{ij} = \\alpha_{i} + \\beta_{j} + 2\\sqrt{\\alpha_{i}}\\sqrt{\\beta_{j}} \\] o lo que es lo mismo \\[ \\mu_{ij} = \\left( \\sqrt{\\alpha_{i}}\\sqrt{\\beta_{j}} \\right)^{2} \\] Si se aplica la transformación raíz cuadrada, se obtiene: \\[ \\mu_{ij}^{&#39;} = \\alpha_{i}^{&#39;} + \\beta_{j}^{&#39;} \\] donde: \\[ \\begin{matrix} {u^{&#39;}}_{ij} = \\sqrt{\\mu_{ij}} &amp; \\alpha_{i}^{&#39;} = \\sqrt{\\alpha_{i}} &amp; \\beta_{j}^{&#39;} = \\sqrt{\\beta_{i}} \\\\ \\end{matrix} \\] Las transformaciones que convierten las interacciones importantes en no importantes son: cuadrado, raíz cuadrada, logaritmo y recíproca. Tabla 7.6: Ejemplo de transformacion de medias de tratamientos: a) Medias de tratamientos escala original b) Medias de tratamientos después de la transformación \\(\\sqrt{}\\) factor \\(A\\) factor \\(B\\) \\(j = 1\\) \\(j = 2\\) \\(i = 1\\) 16 64 \\(i = 2\\) 49 121 \\(i = 3\\) 64 144 factor \\(A\\) factor \\(B\\) \\(j = 1\\) \\(j = 2\\) \\(i = 1\\) 4 8 \\(i = 2\\) 7 11 \\(i = 3\\) 8 12 7.5 MODELO I PARA ESTUDIOS DE DOS FACTORES El factor \\(A\\) es estudiado en \\(I\\) niveles, y estos no representan una muestra aleatoria de todos los niveles posibles de \\(A\\). De manera equivalente el factor \\(B\\) se estudia sobre \\(J\\) niveles. Todas las \\(IJ\\) combinaciones de los niveles de los factores son incluidas en el análisis. El número de casos para cada uno de los \\(IJ\\) tratamientos es el mismo, lo indicamos con \\(n\\), y es necesario que \\(n &gt; 1\\). Entonces el número total de casos es: \\[ N = IJn \\] 7.5.1 Modelo de las medias de celdas Se puede expresar el modelo de niveles del factor fijos en términos de las medias de los tratamientos. \\[ Y_{ijk} = \\mu_{ij} + \\varepsilon_{ijk} \\] \\[ i = 1,2,\\ldots,I;j = 1,2,\\ldots,J;k = 1,2,\\ldots,n \\] 7.5.1.1 Características importantes del Modelo \\(E(Y_{ijk}) = \\mu_{ij}\\) \\(Var(Y_{ijk}) = Var(\\varepsilon_{ijk}) = \\sigma^{2}\\) \\(Y_ijk\\) son independientes y se distribuyen \\(N\\left( 0,\\sigma^{2} \\right)\\) El modelo de ANOVA es un modelo lineal. El modelo de ANOVA de dos factores es similar al de un factor. Normalidad, independencia de los términos del error y constancia de la varianza para los términos del error son propiedades de ambos modelos. 7.5.2 Modelo de los efectos de los factores Una forma equivalente de enunciar el modelo se obtiene de la definición de interacción: \\[ \\left( \\alpha\\beta \\right)_{ij} = \\mu_{ij}(\\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j}) \\] Reordenando términos se obtiene: \\[ \\mu_{ij} = \\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j} + \\left( \\alpha\\beta \\right)_{ij} \\] donde: \\[ \\mu_{\\bullet \\bullet} = \\frac{\\sum_{i = 1}^{I}{\\sum_{j = 1}^{J}\\mu_{ij}}}{IJ} \\] \\[ \\alpha_{i} = \\mu_{i \\bullet} - \\mu_{\\bullet \\bullet} \\] \\[ \\beta_{j} = \\mu_{\\bullet j} - \\mu_{\\bullet \\bullet} \\] \\[ \\left( \\alpha\\beta \\right)_{ij} = \\mu_{ij} - \\mu_{i \\bullet} - \\mu_{\\bullet j} - \\mu_{\\bullet \\bullet} \\] La media de la celda µij para cualquier tratamiento puede ser vista como la suma de cuatro componentes de los efectos de los factores. Específicamente: Una constante general \\(\\mu_{\\bullet \\bullet}\\). El efecto principal \\(\\alpha_{i}\\) del factor \\(A\\) en el i-ésimo nivel. El efecto principal \\(\\beta_{j}\\) del factor \\(B\\) en el j-ésimo nivel. El efecto de la interacción \\(\\left( \\alpha\\beta \\right)_{ij}\\) Reemplazando \\(\\mu_{ij}\\) en el modelo de las medias de las celdas, se obtiene: \\[ Y_{ijk} = \\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j} + \\left( \\alpha\\beta \\right)_{ij} + \\varepsilon_{ijk} \\] donde: \\(\\mu_{\\bullet \\bullet}\\) es una constante \\(\\alpha_{i}\\) son constantes sujetas a la restricción \\(\\sum\\alpha_{i} = 0\\) \\(\\beta_{j}\\) son constantes sujetas a la restricción \\(\\sum\\beta_{j} = 0\\) \\(\\left( \\alpha\\beta \\right)_{ij}\\) son constantes sujetas a las restricciones \\[ \\begin{matrix} \\sum_{i}^{I}\\left( \\alpha\\beta \\right)_{ij} = 0 &amp; \\sum_{j}^{J}\\left( \\alpha\\beta \\right)_{ij} = 0 \\\\ \\end{matrix} \\] \\(\\varepsilon_{ijk}\\) son independientes y se distribuyen \\(N(0,\\ \\sigma^{2})\\) 7.5.3 ANOVA (MODELO I) 7.5.3.1 Notación \\[ \\overline{Y}_{ij \\bullet} = \\frac{\\sum_{k = 1}^{n}Y_{ijk}}{n} \\] \\[ \\overline{Y}_{i \\bullet \\bullet} = \\frac{\\sum_{j}^{J}{\\sum_{k = 1}^{n}Y_{ijk}}}{\\text{Jn}} \\] \\[ \\overline{Y}_{\\bullet j \\bullet} = \\frac{\\sum_{i}^{I}{\\sum_{k = 1}^{n}Y_{ijk}}}{\\text{In}} \\] \\[ \\overline{Y}_{\\bullet \\bullet \\bullet} = \\frac{\\sum_{i}^{I}{\\sum_{j}^{J}{\\sum_{k = 1}^{n}Y_{ijk}}}}{\\text{IJn}} \\] 7.5.3.2 Ajuste del modelo 7.5.3.2.1 Modelo de las medias de celda El cuadrado a minimizar es el siguiente: \\[ \\sum\\sum\\sum\\left( Y_{ijk} - \\mu_{ij} \\right)^{2} \\] Minimizando se obtiene: \\[ \\hat{\\mu}_{ij} = \\overline{Y}_{ij \\bullet} \\] Los residuos se definen como la diferencia entre los valores observados y los estimados: \\[ \\varepsilon_{ijk} = Y_{ijk} - \\overline{Y}_{ij \\bullet} \\] 7.5.3.2.2 Modelo de los efectos del factor El cuadrado a minimizar es el siguiente: \\[ \\sum\\sum\\sum\\left( Y_{ijk} - \\mu_{\\bullet \\bullet} - \\alpha_{i} - \\beta_{j} - \\left( \\alpha\\beta \\right)_{ij} \\right)^{2} \\] sujeto a las restricciones: \\[ \\begin{matrix} \\sum_{i}^{I}\\alpha_{i} = 0 &amp; \\sum_{j}^{J}\\beta_{j} = 0 &amp; \\sum_{i}^{I}\\left( \\alpha\\beta \\right)_{ij} = 0 &amp; \\sum_{j}^{J}\\left( \\alpha\\beta \\right)_{ij} = 0 \\\\ \\end{matrix} \\] Cuando se minimiza se obtienen los siguientes estimadores de mínimos cuadrados: Parámetro Estimador \\(\\mu_{\\bullet \\bullet}\\) \\({\\overline{Y}_{\\bullet \\bullet \\bullet}}\\) \\(\\alpha_{i} = \\mu_{i \\bullet} - \\mu_{\\bullet \\bullet}\\) \\(\\overline{Y}_{i \\bullet \\bullet} - \\overline{Y}_{\\bullet \\bullet \\bullet}\\) \\(\\beta_{j} = \\mu_{\\bullet j} - \\mu_{\\bullet \\bullet}\\) \\(\\overline{Y}_{\\bullet j \\bullet} - \\overline{Y}_{\\bullet \\bullet \\bullet}\\) \\(\\left( \\alpha\\beta \\right)_{ij} = \\mu_{ij} - \\mu_{i \\bullet} - \\mu_{\\bullet j} + \\mu_{\\bullet \\bullet}\\) \\(\\overline{Y}_{ij \\bullet} - \\overline{Y}_{i \\bullet \\bullet} - \\overline{Y}_{\\bullet j \\bullet} + \\overline{Y}_{\\bullet \\bullet \\bullet}\\) 7.5.3.2.3 Descomposición de la suma de cuadrados total Para una observación, se puede descomponer la desviación con respecto a la media total, en dos partes: \\[ \\begin{matrix} \\underbrace{\\left ( Y_{ijk}-\\overline{Y}_{\\bullet\\bullet\\bullet} \\right )} &amp; = &amp; \\underbrace{\\left ( \\overline{Y}_{ij\\bullet}-\\overline{Y}_{\\bullet\\bullet\\bullet} \\right )}&amp; + &amp; \\underbrace{\\left ( Y_{ijk}-\\overline{Y}_{ij\\bullet} \\right )}\\\\ \\text{Desviacion Total} &amp; &amp; \\text{Desviacion de los tratamientos} &amp; &amp; \\text{Desviacion de las observaciones} \\\\ \\end{matrix} \\] También se puede descomponer la desviación estimada de la media de los tratamientos en: \\[ \\begin{matrix} \\underbrace{\\overline{Y}_{ij\\bullet} - \\overline{Y}_{\\bullet \\bullet \\bullet}}&amp; = &amp; \\underbrace{\\overline{Y}_{i\\bullet\\bullet} - \\overline{Y}_{\\bullet \\bullet \\bullet}} &amp; + &amp; \\underbrace{\\overline{Y}_{\\bullet j \\bullet} - \\overline{Y}_{\\bullet \\bullet \\bullet}} &amp; + &amp; \\underbrace{\\overline{Y}_{ij \\bullet} - \\overline{Y}_{i \\bullet \\bullet} - \\overline{Y}_{\\bullet j \\bullet} + \\overline{Y}_{\\bullet \\bullet \\bullet}} \\\\ \\text{Desviacion de} &amp; &amp; \\text{Efecto principal} &amp; &amp; \\text{Efecto principal} &amp; &amp; \\text{Efecto de} \\\\ \\text{ los tratamientos} &amp; &amp; \\text{A} &amp; &amp; \\text{B} &amp; &amp; \\text{ la interaccion} \\end{matrix} \\] ###TABLA DE ANOVA PARA DOS FACTORES. MODELO I Más de una observación por celda Fuente de variación SC GL CM E(CM) Entre tratamientos \\(SC_{E} = n\\sum_{ij}^{}\\overline{Y}_{ij \\bullet}^{2} - N\\overline{Y}_{\\bullet \\bullet \\bullet}^{2}\\) \\(IJ-1\\) \\(\\frac{SC_{E}}{IJ-1}\\) \\(\\sigma^{2} + \\frac{n}{IJ - 1}\\sum\\sum\\left( \\mu_{ij} - \\mu_{\\bullet \\bullet} \\right)^{2}\\) A \\(SC_{A} = Jn\\sum_{i}^{}\\overline{Y}_{i \\bullet \\bullet}^{2} - N\\overline{Y}_{\\bullet \\bullet \\bullet}^{2}\\) \\(I-1\\) \\(\\frac{SC_{A}}{I-1}\\) \\(\\sigma^{2} + \\frac{\\text{Jn}}{I - 1}\\sum\\left( \\mu_{i \\bullet} - \\mu_{\\bullet \\bullet} \\right)^{2}\\) B \\(SC_{B} = In\\sum_{j}^{}\\overline{Y}_{\\bullet j \\bullet}^{2} - N\\overline{Y}_{\\bullet \\bullet \\bullet}^{2}\\) \\(J-1\\) \\(\\frac{SC_{B}}{J-1}\\) \\(\\sigma^{2} + \\frac{\\text{In}}{J - 1}\\sum\\left( \\mu_{\\bullet j} - \\mu_{\\bullet \\bullet} \\right)^{2}\\) AB (Interacción) \\(SC_{E}-SC_{A} - SC_{B}\\) \\((I-1)(J-1)\\) \\(\\frac{SC_{AB}}{\\left( I1 \\right)\\left( J - 1 \\right)}\\) \\(\\sigma^{2} + \\frac{n}{\\left( I - 1 \\right)\\left( J - 1 \\right)}\\sum\\sum\\left( \\mu_{ij} - \\mu_{i \\bullet} - \\mu_{\\bullet j} + \\mu_{\\bullet \\bullet} \\right)^{2}\\) Error \\(SC_{T} - SC_{E}\\) \\(N-IJ\\) \\(\\frac{SC_{D}}{N - IJ}\\) \\(\\sigma^{2}\\) Total \\(\\sum_{i}^{}{\\sum_{j}^{}{\\sum_{k}^{}Y_{ijk}^{2}}} - N\\overline{Y}_{\\bullet \\bullet \\bullet}^{2}\\) \\(N-1\\) 7.6 Prueba de F 7.6.0.1 Prueba para la interacción \\[ \\begin{aligned} H_{0}&amp;:\\mu_{ij} - \\mu_{i \\bullet} - \\mu_{\\bullet j} + \\mu_{\\bullet \\bullet} = 0\\ \\forall\\ i,j\\\\ H_{a}&amp;:\\mu_{ij} - \\mu_{i \\bullet} - \\mu_{\\bullet j} + \\mu_{\\bullet \\bullet} \\neq 0\\ para\\ algun\\ i,j \\end{aligned} \\] o \\[ \\begin{aligned} H_{0}&amp;: \\text{todos los } \\left( \\alpha\\beta \\right)_{ij} = 0\\\\ H_{a}&amp;:\\text{no todos los } \\left( \\alpha\\beta \\right)_{ij} = 0 \\end{aligned} \\] La prueba estadística apropiada es: \\[ F^{*} = \\frac{CM_{AB}}{CM_{D}} \\] Recordamos que bajo \\(H_{0}\\) \\(F^{*}\\) se distribuye según una \\(F_{1 - \\alpha;\\left( I-1 \\right)\\left( J-1 \\right),\\left( N-IJ \\right)}.\\) Entonces: Sí \\(F^{*} \\leq F_{1 - \\alpha;\\left( I-1 \\right)\\left( J-1 \\right),\\left( N-IJ \\right)}\\), no se rechaza \\(H_{0}\\) Sí \\(F^{*} &gt; F_{1 - \\alpha;\\left( I-1 \\right)\\left( J-1 \\right),\\left( N-IJ \\right)}\\), se rechaza \\(H_{0}\\) 7.6.0.2 Prueba para los efectos principales Estas pruebas se realizan cuando no existe interacción. Para el factor \\(A\\): \\[ \\begin{aligned} H_{0}&amp;:\\mu_{1 \\bullet} = \\mu_{2 \\bullet} = \\ldots = \\mu_{I \\bullet}\\\\ H_{a}&amp;:No\\ todos\\ los\\ \\mu_{i \\bullet}\\ \\text{son iguales} \\end{aligned} \\] o \\[ \\begin{aligned} H_{0}:\\alpha_{1} = \\alpha_{2} = \\ldots = \\alpha_{I} = 0\\\\ H_{a}: \\text{No todos los } \\alpha_{i}\\ \\text{iguales a cero} \\end{aligned} \\] Se usa el estadístico \\[ F^{*} = \\frac{CM_{A}}{CM_{D}} \\] Dado que \\(F^{*}\\), bajo \\(H_{0}\\), se distribuye según una \\(F_{1 - \\alpha;\\left( I-1 \\right)\\left( J-1 \\right),\\left( N-IJ \\right)}.\\) Entonces: Sí \\(F^{*} \\leq F_{1 - \\alpha;\\left( I-1 \\right),\\left( N-IJ \\right)}\\), no se rechaza \\(H_{0}\\) Sí \\(F^{*} &gt; F_{1 - \\alpha;\\left( I-1 \\right),\\left( N-IJ \\right)}\\), se rechaza \\(H_{0}\\) Para el factor \\(B\\): \\[ \\begin{aligned} H_{0}&amp;:\\mu_{\\bullet 1} = \\mu_{\\bullet 2} = \\ldots = \\mu_{\\bullet J}\\\\ H_{a}&amp;: \\text{No todos los } \\mu_{\\bullet j}\\ \\text{son iguales} \\end{aligned} \\] o \\[ \\begin{aligned} H_{0}&amp;:\\beta_{1} = \\beta_{2} = \\ldots = \\beta_{J} = 0\\\\ H_{a}&amp;:\\text{No todos los } \\beta_{j}\\ \\text{iguales a cero} \\end{aligned} \\] El estadístico es \\[ F^{*} = \\frac{CM_{B}}{CM_{D}} \\] y la regla de decisión es: Sí \\(F^{*} \\leq F_{1 - \\alpha;\\left( J-1 \\right),\\left( N- IJ\\right)}\\), no se rechaza \\(H_{0}\\) Sí \\(F^{*} &gt; F_{1 - \\alpha;\\left( J-1 \\right),\\left( N- IJ\\right)}\\), se rechaza \\(H_{0}\\) Ejemplo: El asma bronquial es una enfermedad alérgica cuya virulencia depende de la estación. Se desea comparar tres fármacos antihistamínicos A, B y C, en las cuatro estaciones del año. Se toma una muestra de 48 personas con asma crónico de intensidad análoga, que se divide en 12 grupos, uno para cada fármaco y estación, a razón de 4 enfermos por grupo. Los resultados se evaluaron en una escala objetiva que iba de 100, y fueron los siguientes: Estación Fármaco A Fármaco B Fármaco C Primavera 23, 28, 32, 18 56, 58, 53, 55 42, 41, 36, 37 Verano 32, 41, 43, 48 64, 58, 67, 72 51, 53, 55, 60 Otoño 18, 16, 21, 10 48, 50, 47, 47 28, 31, 23, 33 Invierno 30, 40, 33, 47 60, 61, 63, 59 56, 60, 61, 55 Determinar si existen diferencias entre los fármacos A, B y C y entre las estaciones. ¿Es significativa la interacción? Tabla 7.7: Resumen de los datos de tres fármacos contra el asma en las cuatro estaciones del año. Estación Fármaco n Suma Media Varianza Invierno A 4 150 37.500 57.666667 Invierno B 4 243 60.750 2.916667 Invierno C 4 232 58.000 8.666667 Otoño A 4 65 16.250 21.583333 Otoño B 4 192 48.000 2.000000 Otoño C 4 115 28.750 18.916667 Primavera A 4 101 25.250 36.916667 Primavera B 4 222 55.500 4.333333 Primavera C 4 156 39.000 8.666667 Verano A 4 164 41.000 44.666667 Verano B 4 261 65.250 34.250000 Verano C 4 219 54.750 14.916667 Total A 16 480 30.000 135.866667 Total B 16 918 57.375 52.650000 Total C 16 722 45.125 160.650000 (ref:grafico-asma) Efecto de tres fármacos contra el asma en las cuatro estaciones del año. a – gráfico de cajas y barras para las 4 estaciones, n = 12. b – gráfico de cajas y barras para los 3 fármacos, n = 16. c – gráfico de interacción fármaco x estación, n = 4. Figura 7.3: (ref:grafico-asma) (#tab:anova-asma) Tabla de ANOVA. 7.7 Contrastes 7.7.1 Entre Filas \\[ \\hat{f} = \\sum c_{i}{\\overline{Y}_{i \\bullet \\bullet}} \\] Bonferroni y Scheffé \\[ \\varepsilon = \\frac{\\left| \\hat{f} \\right|}{\\sqrt{CM_{D}\\left. \\ \\frac{\\sum c_{i}^{2}}{\\text{Jn}} \\right.\\ }\\ } \\] 7.7.1.1 Planeados: 7.7.1.1.1 Bonferroni \\[ VC = t_{1 - \\frac{\\alpha}{2m};GL_{D}} \\] 7.7.1.2 No Planeados: 7.7.1.2.1 Scheffé \\[ VC = S = \\sqrt{\\left( I - 1 \\right)F_{\\left( I - 1 \\right);GL_{D};1 - \\alpha}} \\] 7.7.1.2.2 Tukey \\[ \\frac{\\overline{Y}_{i \\bullet \\max } - \\overline{Y}_{i \\bullet \\min }}{S_{\\overline{Y}}}\\sim q_{I;N - IJ} \\] \\[ S_{\\overline{y}} = \\sqrt{\\frac{CM_{D}}{\\text{Jn}}} \\] 7.7.1.2.3 Ortogonales \\[ SC = \\frac{{\\hat{f}}^{2}}{\\frac{\\sum c_{i}^{2}}{\\text{Jn}}}\\ \\] 7.7.2 Entre columnas \\[ \\hat{f} = \\sum c_{j}{\\overline{Y}_{\\bullet j \\bullet}} \\] Bonferroni y Scheffé \\[ \\varepsilon = \\frac{\\left| \\hat{f} \\right|}{\\sqrt{CM_{D}\\left. \\ \\frac{\\sum c_{j}^{2}}{\\text{In}} \\right.\\ }\\ } \\] 7.7.2.1 Planeados: 7.7.2.1.1 Bonferroni \\[ VC = t_{1 - \\frac{\\alpha}{2m};GL_{D}} \\] 7.7.2.2 No Planeados: 7.7.2.2.1 Scheffé \\[ VC = S = \\sqrt{\\left( J - 1 \\right)F_{\\left( J - 1 \\right);GL_{D};1 - \\alpha}} \\] 7.7.2.2.2 Tukey \\[ \\frac{\\overline{Y}_{\\bullet j\\ \\max } - \\overline{Y}_{\\bullet j\\ \\min }}{S_{\\overline{Y}}}\\sim q_{j;N - IJ} \\] \\[ S_{\\overline{Y} = \\sqrt{\\frac{CM_{D}}{In}}} \\] 7.7.2.2.3 Ortogonales \\[ SC = \\frac{{\\hat{f}}^{2}}{\\frac{\\sum c_{j}^{2}}{\\text{In}}}\\ \\] 7.7.3 Interacción \\[ \\hat{f} = \\sum c_{ij}{\\overline{Y}_{ij \\bullet}} \\] Bonferroni y Scheffé \\[ \\varepsilon = \\frac{\\left| \\hat{f} \\right|}{\\sqrt{CM_{D}\\left. \\ \\frac{\\sum c_{ij}^{2}}{n} \\right.\\ }\\ } \\] 7.7.3.1 Planeados: 7.7.3.1.1 Bonferroni \\[ VC = t_{1 - \\frac{\\alpha}{2m};GL_{D}} \\] 7.7.3.2 No Planeados: 7.7.3.2.1 Scheffé \\[ VC = S = \\sqrt{\\left( IJ - 1 \\right)F_{\\left( IJ - 1 \\right);N - IJ;1 - \\alpha}} \\] 7.7.3.2.2 Tukey \\[ \\frac{\\overline{Y}_{ij \\max} - \\overline{Y}_{ij\\min}}{S_{\\overline{Y}}} \\sim q_{ij;N - IJ} \\] \\[ S_{\\overline{Y}} = \\sqrt{\\frac{CM_{D}}{n}} \\] 7.7.3.2.3 Ortogonales \\[ SC = \\frac{{\\hat{f}}^{2}}{\\frac{\\sum c_{ij}^{2}}{n}}\\ \\] \\[ \\begin{matrix} f = \\sum c_{ij}\\mu_{ij} &amp; \\hat{f} = \\sum c_{ij}\\overline{Y}_{ij} \\\\ \\end{matrix} \\] Ejemplo (continuación del anterior) Expansión de los contrastes ortogonales Analysis of Variance Model Df Sum Sq Mean Sq F value Pr(&gt;F) Estación 3 4132 1377 64.69 1.425e-14 Estación: Invierno vs Verano 1 15.04 15.04 0.7065 0.4062 Estación: Otoño vs Verano 1 3828 3828 179.8 1.442e-15 Estación: Primavera vs Verano 1 289 289 13.57 0.0007492 Farmaco 2 6017 3009 141.3 9.013e-18 Farmaco: A vs B 1 1830 1830 85.95 4.507e-11 Farmaco: B vs C 1 4187 4187 196.7 3.698e-16 Estación:Farmaco 6 338.8 56.47 2.652 0.03106 Estación:Farmaco: Invierno vs Verano.A vs B 1 45.56 45.56 2.14 0.1522 Estación:Farmaco: Otoño vs Verano.A vs B 1 28.52 28.52 1.34 0.2547 Estación:Farmaco: Primavera vs Verano.A vs B 1 5.042 5.042 0.2368 0.6295 Estación:Farmaco: Invierno vs Verano.B vs C 1 25.52 25.52 1.199 0.2809 Estación:Farmaco: Otoño vs Verano.B vs C 1 189.1 189.1 8.88 0.005141 Estación:Farmaco: Primavera vs Verano.B vs C 1 45.12 45.12 2.119 0.1541 Residuals 36 766.5 21.29 NA NA Contrastes planeados: Invierno vs Verano Df Sum Sq Mean Sq F value Pr(&gt;F) Estación 3 4132 1377 64.69 1.425e-14 Estación: Invierno vs Verano 1 15.04 15.04 0.7065 0.4062 Farmaco 2 6017 3009 141.3 9.013e-18 Estación:Farmaco 6 338.8 56.47 2.652 0.03106 Residuals 36 766.5 21.29 NA NA B vs A-C Df Sum Sq Mean Sq F value Pr(&gt;F) Estación 3 4132 1377 64.69 1.425e-14 Farmaco 2 6017 3009 141.3 9.013e-18 Farmaco: B vs A-C 1 4187 4187 196.7 3.698e-16 Estación:Farmaco 6 338.8 56.47 2.652 0.03106 Residuals 36 766.5 21.29 NA NA C Otoño vs C Otros Df Sum Sq Mean Sq F value Pr(&gt;F) FxE 11 10488 953.5 44.78 1.178e-17 FxE: C Otoño vs C Otros 1 1430 1430 67.17 9.502e-10 Residuals 36 766.5 21.29 NA NA Contrastes no planeados: LSD vs Tukey contrast estimate SE df t.ratio p.value.lsd p.value.tukey Invierno,A - Otoño,A 21.25 3.263 36 6.513 1.442e-07 8.598e-06 Invierno,A - Primavera,A 12.25 3.263 36 3.754 0.0006132 0.02591 Invierno,A - Verano,A -3.5 3.263 36 -1.073 0.2905 0.9941 Invierno,A - Invierno,B -23.25 3.263 36 -7.126 2.247e-08 1.368e-06 Invierno,A - Otoño,B -10.5 3.263 36 -3.218 0.002731 0.09413 Invierno,A - Primavera,B -18 3.263 36 -5.517 3.076e-06 0.0001732 Invierno,A - Verano,B -27.75 3.263 36 -8.505 3.896e-10 2.439e-08 Invierno,A - Invierno,C -20.5 3.263 36 -6.283 2.914e-07 1.72e-05 Invierno,A - Otoño,C 8.75 3.263 36 2.682 0.01099 0.2755 Invierno,A - Primavera,C -1.5 3.263 36 -0.4597 0.6485 1 Invierno,A - Verano,C -17.25 3.263 36 -5.287 6.237e-06 0.0003443 Otoño,A - Primavera,A -9 3.263 36 -2.758 0.009071 0.2402 Otoño,A - Verano,A -24.75 3.263 36 -7.586 5.687e-09 3.503e-07 Otoño,A - Invierno,B -44.5 3.263 36 -13.64 8.623e-16 0 Otoño,A - Otoño,B -31.75 3.263 36 -9.731 1.281e-11 8.121e-10 Otoño,A - Primavera,B -39.25 3.263 36 -12.03 3.581e-14 2.068e-12 Otoño,A - Verano,B -49 3.263 36 -15.02 4.457e-17 0 Otoño,A - Invierno,C -41.75 3.263 36 -12.8 5.846e-15 1.286e-13 Otoño,A - Otoño,C -12.5 3.263 36 -3.831 0.0004921 0.02126 Otoño,A - Primavera,C -22.75 3.263 36 -6.973 3.567e-08 2.162e-06 Otoño,A - Verano,C -38.5 3.263 36 -11.8 6.254e-14 3.79e-12 Primavera,A - Verano,A -15.75 3.263 36 -4.827 2.545e-05 0.001336 Primavera,A - Invierno,B -35.5 3.263 36 -10.88 6.221e-13 3.958e-11 Primavera,A - Otoño,B -22.75 3.263 36 -6.973 3.567e-08 2.162e-06 Primavera,A - Primavera,B -30.25 3.263 36 -9.271 4.507e-11 2.846e-09 Primavera,A - Verano,B -40 3.263 36 -12.26 2.063e-14 1.089e-12 Primavera,A - Invierno,C -32.75 3.263 36 -10.04 5.622e-12 3.572e-10 Primavera,A - Otoño,C -3.5 3.263 36 -1.073 0.2905 0.9941 Primavera,A - Primavera,C -13.75 3.263 36 -4.214 0.0001606 0.007619 Primavera,A - Verano,C -29.5 3.263 36 -9.041 8.542e-11 5.382e-09 Verano,A - Invierno,B -19.75 3.263 36 -6.053 5.903e-07 3.443e-05 Verano,A - Otoño,B -7 3.263 36 -2.145 0.03874 0.596 Verano,A - Primavera,B -14.5 3.263 36 -4.444 8.097e-05 0.004013 Verano,A - Verano,B -24.25 3.263 36 -7.432 8.97e-09 5.505e-07 Verano,A - Invierno,C -17 3.263 36 -5.21 7.891e-06 0.0004325 Verano,A - Otoño,C 12.25 3.263 36 3.754 0.0006132 0.02591 Verano,A - Primavera,C 2 3.263 36 0.613 0.5437 1 Verano,A - Verano,C -13.75 3.263 36 -4.214 0.0001606 0.007619 Invierno,B - Otoño,B 12.75 3.263 36 3.908 0.0003943 0.0174 Invierno,B - Primavera,B 5.25 3.263 36 1.609 0.1163 0.8945 Invierno,B - Verano,B -4.5 3.263 36 -1.379 0.1763 0.9603 Invierno,B - Invierno,C 2.75 3.263 36 0.8428 0.4049 0.9993 Invierno,B - Otoño,C 32 3.263 36 9.808 1.041e-11 6.606e-10 Invierno,B - Primavera,C 21.75 3.263 36 6.666 9.038e-08 5.421e-06 Invierno,B - Verano,C 6 3.263 36 1.839 0.07419 0.7864 Otoño,B - Primavera,B -7.5 3.263 36 -2.299 0.02744 0.4955 Otoño,B - Verano,B -17.25 3.263 36 -5.287 6.237e-06 0.0003443 Otoño,B - Invierno,C -10 3.263 36 -3.065 0.004112 0.1312 Otoño,B - Otoño,C 19.25 3.263 36 5.9 9.457e-07 5.468e-05 Otoño,B - Primavera,C 9 3.263 36 2.758 0.009071 0.2402 Otoño,B - Verano,C -6.75 3.263 36 -2.069 0.04581 0.6463 Primavera,B - Verano,B -9.75 3.263 36 -2.988 0.00503 0.1539 Primavera,B - Invierno,C -2.5 3.263 36 -0.7662 0.4485 0.9997 Primavera,B - Otoño,C 26.75 3.263 36 8.198 9.42e-10 5.869e-08 Primavera,B - Primavera,C 16.5 3.263 36 5.057 1.262e-05 0.0006808 Primavera,B - Verano,C 0.75 3.263 36 0.2299 0.8195 1 Verano,B - Invierno,C 7.25 3.263 36 2.222 0.03266 0.5455 Verano,B - Otoño,C 36.5 3.263 36 11.19 2.858e-13 1.81e-11 Verano,B - Primavera,C 26.25 3.263 36 8.045 1.471e-09 9.141e-08 Verano,B - Verano,C 10.5 3.263 36 3.218 0.002731 0.09413 Invierno,C - Otoño,C 29.25 3.263 36 8.965 1.059e-10 6.664e-09 Invierno,C - Primavera,C 19 3.263 36 5.823 1.197e-06 6.889e-05 Invierno,C - Verano,C 3.25 3.263 36 0.9961 0.3259 0.9968 Otoño,C - Primavera,C -10.25 3.263 36 -3.141 0.003355 0.1114 Otoño,C - Verano,C -26 3.263 36 -7.969 1.84e-09 1.142e-07 Primavera,C - Verano,C -15.75 3.263 36 -4.827 2.545e-05 0.001336 Tabla comparaciones múltiples LSD (triángulo superior), Tukey (triángulo inferior) y medias (diagonal) Invierno.A Otoño.A Primavera.A Verano.A Invierno.B Otoño.B Primavera.B Verano.B Invierno.C Otoño.C Primavera.C Verano.C Invierno.A 37.5 1.442e-07 0.0006132 0.2905 2.247e-08 0.002731 3.076e-06 3.896e-10 2.914e-07 0.01099 0.6485 6.237e-06 Otoño.A 8.598e-06 16.25 0.009071 5.687e-09 8.623e-16 1.281e-11 3.581e-14 4.457e-17 5.846e-15 0.0004921 3.567e-08 6.254e-14 Primavera.A 0.02591 8.121e-10 25.25 2.545e-05 6.221e-13 3.567e-08 4.507e-11 2.063e-14 5.622e-12 0.2905 0.0001606 8.542e-11 Verano.A 0.2402 2.162e-06 2.439e-08 41 5.903e-07 0.03874 8.097e-05 8.97e-09 7.891e-06 0.0006132 0.5437 0.0001606 Invierno.B 0.9941 0.596 0 3.572e-10 60.75 0.0003943 0.1163 0.1763 0.4049 1.041e-11 9.038e-08 0.07419 Otoño.B 3.503e-07 0.0174 1.089e-12 0.0004325 0.9941 48 0.02744 6.237e-06 0.004112 9.457e-07 0.009071 0.04581 Primavera.B 0.001336 0.0001732 5.505e-07 0.9993 0.02591 1 55.5 0.00503 0.4485 9.42e-10 1.262e-05 0.8195 Verano.B 1.368e-06 2.068e-12 0.9603 0.1312 6.606e-10 2.162e-06 0.0006808 65.25 0.03266 2.858e-13 1.471e-09 0.002731 Invierno.C 0 2.846e-09 0.0003443 0.9997 5.468e-05 0.007619 9.141e-08 3.79e-12 58 1.059e-10 1.197e-06 0.3259 Otoño.C 3.958e-11 0.004013 0.1539 0.5455 5.869e-08 1 6.889e-05 5.382e-09 0.6463 28.75 0.003355 1.84e-09 Primavera.C 3.443e-05 0.8945 1.72e-05 0.2755 1.81e-11 5.421e-06 0.1114 0.007619 1 0.9968 39 2.545e-05 Verano.C 0.09413 0.4955 1.286e-13 0.02126 6.664e-09 0.2402 0.0003443 0.7864 0.09413 1.142e-07 0.001336 54.75 7.8 Potencia de la prueba F La potencia de la prueba F para la interacción, los efectos del factor principal A y los efectos del factor principal B puede ser evaluada de manera similar al caso del análisis de un solo factor \\(A\\) través de los gráficos de Pearson-Hartley. El parámetro de no centralidad Φ y los grados de libertad, para cada uno de estos casos son los siguientes: 7.8.1 Interacción \\[ \\Phi = \\frac{1}{\\sigma}\\sqrt{\\frac{n\\sum\\sum\\left(\\alpha \\beta \\right)_{ij}^{2}}{\\left( I - 1 \\right)\\left( J - 1 \\right) + 1}} = \\frac{1}{\\sigma}\\sqrt{\\frac{n\\sum\\sum\\left( \\mu_{ij} - \\mu_{i \\bullet} - \\mu_{\\bullet j} + \\mu_{\\bullet \\bullet} \\right)^{2}}{\\left( I - 1 \\right)\\left( J - 1 \\right) + 1}} \\] \\[ \\begin{matrix} \\nu_{1} = \\left( I - 1 \\right)\\left( J - 1 \\right) &amp; \\nu_{2} = N - IJ \\\\ \\end{matrix} \\] 7.8.2 Prueba para el factor principal A: \\[ \\Phi = \\frac{1}{\\sigma}\\sqrt{\\frac{nJ\\sum\\left( \\alpha \\right)_{i}^{2}}{I}} = \\frac{1}{\\sigma}\\sqrt{\\frac{nJ\\sum\\left( \\mu_{i \\bullet} - \\mu_{\\bullet \\bullet} \\right)^{2}}{I}} \\] \\[ \\begin{matrix} \\nu_{1} = I - 1 &amp; \\nu_{2} = N - IJ \\\\ \\end{matrix} \\] 7.8.3 Prueba para el factor principal B: \\[ \\Phi = \\frac{1}{\\sigma}\\sqrt{\\frac{nI\\sum\\left( \\beta \\right)_{j}^{2}}{J}} = \\frac{1}{\\sigma}\\sqrt{\\frac{nI\\sum\\left( \\mu_{\\bullet j} - \\mu_{\\bullet \\bullet} \\right)^{2}}{J}} \\] \\[ \\begin{matrix} \\nu_{1} = J - 1 &amp; \\nu_{2} = N - IJ \\\\ \\end{matrix} \\] Ejemplo: Cálculo de para la interacción: \\[ \\Phi = 1.50778432 \\] \\[ \\nu_{1} = 6 \\] \\[ \\nu_{2} = 36 \\] \\[ P = 0.8 \\] 7.9 CASO DE UNA OBSERVACIÓN POR TRATAMIENTO 7.9.1 Modelo sin interacción El modelo de ANOVA con niveles del factor fijos y sin interacción, para el caso en que n=1, es: \\[ Y_{ij} = \\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j} + \\varepsilon_{ij} \\] Dado que el valor esperado del \\(CM_{AB}\\) es σ2, en la prueba estadística \\(F^{*}\\) para ver la significación de los efectos principales se utiliza ahora el \\(CM_{AB}\\) en el denominador, en lugar del \\(CM_{D}\\): Efectos del factor principal A: \\(F^{*} = \\frac{CM_{A}}{CM_{AB}}\\) Efectos del factor principal B: \\(F^{*} = \\frac{CM_{B}}{CM_{AB}}\\) De manera similar para realizar contrastes, se reemplaza el \\(CM_{D}\\) por el \\(CM_{AB}\\) y se modifican los grados de libertad. Tabla de ANOVA para el Modelo con Niveles Fijos del Factor sin Interacción, n = 1 Fte. de Variación SC GL CM E(CM) factor \\(A\\) \\(J\\sum\\left( \\overline{Y}_{i \\bullet} - \\overline{Y}_{\\bullet \\bullet} \\right)^{2}\\) \\(I - 1\\) \\(\\frac{SC_{A}}{I - 1}\\) \\(\\sigma^{2} + \\frac{J}{I - 1}\\sum\\left( \\mu_{i \\bullet} - \\mu_{\\bullet \\bullet} \\right)^{2}\\) factor \\(B\\) \\(I\\sum\\left( \\overline{Y}_{\\bullet j} - \\overline{Y}_{\\bullet \\bullet} \\right)^{2}\\) \\(J - 1\\) \\(\\frac{SC_{B}}{J - 1}\\) \\(\\sigma^{2} + \\frac{I}{J - 1}\\sum\\left( \\mu_{\\bullet j} - \\mu_{\\bullet \\bullet} \\right)^{2}\\) Error \\(\\sum\\sum{\\left( Y_{ij} - \\overline{Y}_{i\\bullet} - \\overline{Y}_{\\bullet j} + \\overline{Y}_{\\bullet\\bullet} \\right)}^{2}\\) \\((I - 1)(J - 1)\\) \\(\\frac{SC_{{AB}}}{\\left( I - 1 \\right)\\left( J - 1 \\right)}\\) \\(\\sigma^{2}\\) Total \\(\\sum\\sum\\left( Y_{ij} - \\overline{Y}_{\\bullet \\bullet} \\right)^{2}\\) \\(N - 1\\) 7.9.2 Prueba de Tukey (Aditividad) Supongamos que se asume que: \\[ \\left( \\alpha\\beta \\right)_{ij} = D\\alpha_{i}\\beta_{j} \\] donde \\(D\\) es alguna constante. Usando la expresión del modelo de ANOVA con interacciones para el caso de n = 1, se tiene: \\[ Y_{ij} = \\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j} + D\\alpha_{i}\\beta_{j} + \\varepsilon_{ij} \\] Asumiendo que los otros parámetros son conocidos, el estimador de mínimos cuadrados de D es: \\[ \\hat{D} = \\frac{\\sum_{i}^{}{\\sum_{j}^{}{\\alpha_{i}\\beta_{j}Y_{ij}}}}{\\sum_{i}^{}\\alpha_{i}^{2}\\ \\sum_{j}^{}\\beta_{j}^{2}} \\] El estimador de \\(\\alpha_{i}\\) es \\(\\overline{Y}_{i \\bullet} - \\overline{Y}_{\\bullet \\bullet}\\), y de \\(\\beta_{j}\\) es \\(\\overline{Y}_{\\bullet j} - \\overline{Y}_{\\bullet\\bullet}\\) . Reemplazando los parámetros en \\(\\hat{D}\\) por sus estimadores, se obtiene: \\[ \\hat{D} = \\frac{\\sum_{i}^{}{\\sum_{j}^{}{\\left( {\\overline{Y}_{i \\bullet}} - {\\overline{Y}_{\\bullet \\bullet}} \\right)\\left( {\\overline{Y}_{\\bullet j}} - {\\overline{Y}_{\\bullet \\bullet}} \\right)Y_{ij}}}}{\\sum_{i}{\\left ({\\overline{Y}_{i \\bullet}} - \\overline{Y}_{\\bullet \\bullet} \\right )}^2\\sum_{j}{\\left ({\\overline{Y}_{j \\bullet}} - \\overline{Y}_{\\bullet \\bullet} \\right )}^2} \\] Sustituyendo las estimaciones en \\(\\sum\\sum D^{2}\\alpha_{i}^{2}\\beta_{j}^{2}\\), se obtiene la suma de cuadrados de la interacción: \\[ \\begin{aligned} SC_{AB}^{*} &amp; = \\sum_{i}^{}{\\sum_{j}^{}{{{\\hat{D}}^{2}\\left( \\overline{Y}_{i \\bullet} - \\overline{Y}_{\\bullet \\bullet} \\right)}^{2}\\left( \\overline{Y}_{\\bullet j} - \\overline{Y}_{\\bullet \\bullet} \\right)^{2}}} \\\\ &amp; = \\frac{{\\left\\lbrack\\sum_i\\sum_j\\left(\\overline{Y}_{i\\bullet} -\\overline{Y}_{\\bullet\\bullet} \\right ) \\left ( \\overline{Y}_{\\bullet j} -\\overline{Y}_{\\bullet\\bullet} \\right )Y_{ij}\\right\\rbrack }^{2}}{\\sum_i\\left ( \\overline{Y}_{i\\bullet} -\\overline{Y}_{\\bullet\\bullet}\\right )^2\\sum_j\\left ( \\overline{Y}_{\\bullet j} -\\overline{Y}_{\\bullet\\bullet}\\right )^2} \\end{aligned} \\] La descomposición de la SCT para este caso especial de interacción es: \\[ SC_{T} = SC_{A} + SC_{B} + SC_{AB}^{*} + SC_{R}^{*} \\] donde \\(SC_{R}^{*}\\) es la suma de cuadrados residual: \\[ SC_{R}^{*} = SC_{T} - SC_{A} - SC_{B} - SC_{AB}^{*} \\] si \\(D = 0\\), la prueba estadística: \\[ F^{*} = \\frac{\\text{SC}_{AB}^{*}}{1} \\div \\frac{\\text{SC}_{R}^{*}}{IJ - I - J}\\sim F_{1;IJ - I - J} \\] Las hipótesis a poner a prueba son, entonces: \\(H_{0}:D = 0\\) (no hay interacción) \\(H_{a}:D \\neq 0\\) (hay interacción) La regla de decisión es: Si \\(F^{*} \\leq F_{1 - \\alpha;1;IJ - I - J}\\), no se rechaza \\(H_{0}\\). Si \\(F^{*} &gt; F_{1 - \\alpha;1;IJ - I - J}\\), se rechaza \\(H_{0}\\). Ejemplo: Se estudió la razón de la superficie a peso seco, para tres condiciones de sombra y tres especies de cítricos. Los resultados se presentan en la tabla: Naranja Toronja Mandarina Sol 112 90 123 Media sombra 86 73 89 Sombra 80 62 81 Se desea saber si hay diferencias entre las especies y entre las condiciones de iluminación. Prueba de Tukey Se calculan las medias de los niveles de cada factor Naranja Toronja Mandarina \\(\\overline{Y}_{i \\bullet}\\) Sol 112 90 123 108.333333 Media sombra 86 73 89 82.6666667 Sombra 80 62 81 74.3333333 \\(\\overline{Y}_{\\bullet j}\\) 92.6666667 75 97.6666667 \\(\\overline{Y}_{\\bullet \\bullet} = 88.4444444\\) Se calcula la \\(SC_{AB}^{*}\\) Naranja Toronja Mandarina \\(\\overline{Y}_{i \\bullet} - \\overline{Y}_{\\bullet \\bullet}\\) Sol 112 90 123 19.8888889 Media sombra 86 73 89 -5.77777778 Sombra 80 62 81 -14.1111111 \\(\\overline{Y}_{\\bullet j} - \\overline{Y}_{\\bullet \\bullet}\\) 4.22222222 -13.4444444 9.22222222 \\(\\left( \\overline{Y}_{i \\bullet} - \\overline{Y}_{\\bullet \\bullet} \\right)\\left( \\overline{Y}_{\\bullet j} - \\overline{Y}_{\\bullet \\bullet} \\right)Y_{ij}\\) 9405.23457 -24065.5556 22560.6296 -2097.97531 5670.5679 -4742.2716 -4766.41975 11762.3951 -10541 \\[ \\begin{aligned} \\sum_{i}^{}{\\left( \\overline{Y}_{i \\bullet} - \\overline{Y}_{\\bullet \\bullet} \\right)^{2}} &amp;= 628.074074\\\\ \\sum_{j}^{}{\\left( \\overline{Y}_{\\bullet j} - \\overline{Y}_{\\bullet \\bullet} \\right)^{2}} &amp;=283.62963\\\\ SC_{AB}^{*} &amp;= 56.96674 \\end{aligned} \\] Cálculo de \\(SC_R^{*}\\) \\[ \\begin{aligned} SC_{R}^{*}&amp; = SC_{T}-SC_{A}-SC_{B}-SC_{AB}^{*} \\\\ &amp; = 2822.2222-1884.22217-850.888916-56.96674 \\\\ &amp; = 30.1443745 \\\\ \\end{aligned} \\] Cálculo de \\(F^{*}\\), para poner a prueba la hipótesis de no interacción \\[ F^{*} = \\frac{SC_{AB}^{*}}{1} \\div \\frac{SC_{R}^{*}}{IJ - I - J} = 56.96674 \\div \\frac{30.1443745}{3 \\times 3 - 3 - 3} = 5.66939016 \\] El valor obtenido se compara con \\(F_{0.05;1,3} = 10.1279625\\); con lo cual no se rechaza la hipótesis nula. El valor de \\(p\\) es \\(0.09752552\\). En un gráfico de perfiles se observa: (ref:citricos) Graficos de perfiles de la razón de área foliar a peso de hoja para tres especies de citrícos bajo tres condiciones de luz. Figura 7.4: (ref:citricos) Analysis of Variance Model Df Sum Sq Mean Sq F value Pr(&gt;F) Especie 2 850.9 425.4 19.54 0.008625 Sombra 2 1884 942.1 43.26 0.001953 Residuals 4 87.11 21.78 NA NA Análisis de varianza de dos factores con una sola muestra por grupo RESUMEN Cuenta Suma Promedio Varianza Sol 3 325 108.333333 282.333333 media sombra 3 248 82.6666667 72.3333333 sombra 3 223 74.3333333 114.333333 Naranja 3 278 92.6666667 289.333333 Toronja 3 225 75 199 Mandarina 3 293 97.6666667 497.333333 ANÁLISIS DE VARIANZA Origen de las variaciones Suma de cuadrados Grados de libertad Promedio de los cuadrados F Probabilidad Valor crítico para F Filas 1884.222222 2 942.111111 43.2602041 0.00195266 6.944276265 Columnas 850.8888889 2 425.444444 19.5357143 0.00862465 6.944276265 Error 87.11111111 4 21.7777778 Total 2822.222222 8 7.10 MODELO II Y MODELO III PARA ESTUDIOS DE DOS FACTORES 7.10.1 Modelo aleatorio (Modelo II) El modelo aleatorio para estudios de dos factores con igual tamaño muestral, n, es: \\[ Y_{ijk} = \\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j} + \\left( \\alpha\\beta \\right)_{ij} + \\varepsilon_{ijk} \\] donde: \\(\\mu_{\\bullet \\bullet}\\) es una constante \\(\\alpha_{i}\\), \\(\\beta_{j}\\) , \\(\\left( \\alpha\\beta \\right)_{ij}\\) son variables aleatorias independientes con distribución normal con media cero y varianzas \\(\\sigma_{\\alpha}^{2},\\ \\sigma_{\\beta}^{2},\\ \\sigma_{\\alpha\\beta}^{2}\\) respectivamente. \\(\\varepsilon_{ijk}\\) son independientes \\(N(0,\\sigma^{2})\\) \\(\\alpha_{i}\\), \\(\\beta_{j}\\) , \\(\\left( \\alpha\\beta \\right)_{ij},\\ \\varepsilon_{ijk}\\) son independientes de a pares \\[ i = 1,\\ \\ldots,\\ I;\\ j = 1,\\ \\ldots,\\ J;\\ k = 1,\\ \\ldots,\\ n \\] Para este modelo de ANOVA el valor esperado de los \\(Y_{ijk}\\) es: \\[ E\\left( Y_{ijk} \\right) = \\mu_{\\bullet \\bullet} \\] y la varianza de los \\(Y_{ijk}\\), indicada como \\(\\sigma_{Y}^{2}\\), es: \\[ \\text{Var}\\left( Y_{ijk} \\right) = \\sigma_{Y}^{2} = \\sigma_{\\alpha}^{2} + \\sigma_{\\beta}^{2} + \\sigma_{\\alpha\\beta}^{2} + \\sigma^{2} \\] 7.10.2 Modelo Mixto (Modelo III) Para este caso, con igual tamaño muestral, el modelo de ANOVA es: \\[ Y_{ijk} = \\mu_{\\bullet \\bullet} + \\alpha_{i} + \\beta_{j} + \\left( \\alpha\\beta \\right)_{ij} + \\varepsilon_{ijk} \\] donde: \\(\\mu_{\\bullet \\bullet}\\) es una constante \\(\\alpha_{i}\\) son constantes sujetas a la restricción \\(\\sum\\alpha_{i} = 0\\) \\(\\beta_{j}\\) son independientes \\(N(0,\\sigma_{\\beta}^{2})\\) \\(\\left( \\alpha\\beta \\right)_{ij}\\) son \\(N\\left( 0,\\frac{I - 1}{I}\\sigma_{\\alpha\\beta}^{2} \\right)\\), sujetas a las restricciones: a) \\(\\sum\\left( \\alpha\\beta \\right)_{ij} = 0\\ \\forall\\ j\\); b) \\(\\text{Cov}\\left\\lbrack \\left( \\alpha\\beta \\right)_{ij};\\left( \\alpha\\beta \\right)_{i^{&#39;}j} \\right\\rbrack = - \\frac{1}{I}\\sigma_{\\alpha\\beta}^{2}\\ \\forall\\ \\ i \\neq i^{&#39;}\\) \\(\\varepsilon_{ijk}\\) son independientes y se distribuyen \\(N\\left( 0,\\sigma^{2} \\right)\\) \\(\\beta_{j}\\), \\(\\left( \\alpha\\beta \\right)_{ij}\\) y \\(\\varepsilon_{ijk}\\) son independientes de a pares \\[ i = 1,\\ \\ldots,\\ I;\\ j = 1,\\ \\ldots,\\ J;\\ k = 1,\\ \\ldots,\\ n \\] Para el modelo mixto, el valor esperado de \\(Y_{ijk}\\) es: \\[ E(Y_{ijk}) = \\mu_{\\bullet \\bullet} + \\alpha_{i} \\] y la varianza de \\(Y_{ijk}\\) es: \\[ \\text{Var}\\left( Y_{ijk} \\right) = \\sigma_{Y}^{2} = \\sigma_{\\beta}^{2} + \\frac{I - 1}{I}\\sigma_{\\alpha\\beta}^{2} + \\sigma^{2} \\] 7.10.3 Pruebas estadísticas Tabla 7.9: Pruebas estadísticas para los Modelos Aleatorios y Mixto Prueba para presencia de efectos de … Modelo Fijo (A y B fijos) Modelo Aleatorio (A y B aleatorio) Modelo Mixto (A fijo y B aleatorio) factor \\(A\\) \\(CM_{A}/CM_{D}\\) \\(CM_{A}/CM_{AB}\\) \\(CM_{A}/CM_{AB}\\) factor \\(B\\) \\(CM_{B}/CM_{D}\\) \\(CM_{B}/CM_{AB}\\) \\(CM_{B}/CM_{D}\\) Interacción \\(AB\\) \\(CM_{AB}/CM_{D}\\) \\(CM_{AB}/CM_{D}\\) \\(CM_{AB}/CM_{D}\\) 7.10.4 Estimación de los componentes de la varianza Con un modelo aleatorio, por ejemplo \\(\\sigma_{\\alpha}^{2}\\) puede ser estimado por: \\[ E\\left( CM_{A} \\right) - E\\left( CM_{AB} \\right) = \\sigma^{2} + nJ\\sigma_{\\alpha}^{2} + n\\sigma_{\\alpha\\beta}^{2} - \\left( \\sigma^{2} + n\\sigma_{\\alpha\\beta}^{2} \\right) = nJ\\sigma_{\\alpha}^{2} \\] De esta forma un estimador insesgado de \\(\\sigma_{\\alpha}^{2}\\) es: \\[ S_{\\alpha}^{2} = \\frac{CM_{A} - CM_{AB}}{\\text{nJ}} \\] En un modelo mixto con el factor \\(A\\) fijo y el factor \\(B\\) aleatorio, se obtendrá: \\[ S_{I}^{2} = \\frac{CM_{B} - CM_{D}}{\\text{nI}} \\] Tabla 7.10: Esperanza de los cuadrados medios en estudios de dos factores CM GL Niveles del Factor Fijos (A y B fijos) Niveles del factor Aleatorios (A y B aleatorios) Niveles del Factor Mixtos (A fijo, B aleatorio) \\(CM_{A}\\) \\(I - 1\\) \\(\\sigma^{2} + nJ\\frac{\\sum_{}^{}\\alpha_{i}^{2}}{I - 1}\\) \\(\\sigma^{2} + nJ\\sigma_{\\alpha}^{2} + n\\sigma_{\\alpha\\beta}^{2}\\) \\({\\sigma^{2} + nJ\\frac{\\sum_{}^{}\\alpha_{i}^{2}}{I - 1} + n\\sigma}_{\\alpha\\beta}^{2}\\) \\(CM_{B}\\) \\(J - 1\\) \\(\\sigma^{2} + nI\\frac{\\sum_{}^{}\\beta_{j}^{2}}{J - 1}\\) \\(\\sigma^{2} + nI\\sigma_{\\beta}^{2} + n\\sigma_{\\alpha\\beta}^{2}\\) \\(\\sigma^{2} + nI\\sigma_{\\beta}^{2}\\) \\(CM_{AB}\\) \\((I - 1)(J - 1)\\) \\(\\sigma^{2} + nJ\\frac{\\sum_{}^{}\\left( \\alpha\\beta \\right)_{ij}^{2}}{\\left( I - 1 \\right)\\left( J - 1 \\right)}\\) \\(\\sigma^{2} + n\\sigma_{\\alpha\\beta}^{2}\\) \\(\\sigma^{2} + n\\sigma_{\\alpha\\beta}^{2}\\) \\(CM_{D}\\) \\(N - IJ\\) \\(\\sigma^{2}\\) \\(\\sigma^{2}\\) \\(\\sigma^{2}\\) Ejemplos: 1) Se quiere estudiar el efecto de diferentes longitudes de onda (tratamiento) sobre la fertilidad en D. Melanogaster. Para ello se eligieron al azar 25 cepas y se sometieron a cuatro longitudes de onda diferentes elegidas también al azar. Para combinación longitud de onda-cepa se seleccionaron 12 hembras al azar y se registró la cantidad total de huevos puestos al cabo del cuarto día de postura, con los siguientes resultados: GL CM E(CM) Entre cepas A 24 3243.00 Entre tratamientos B 3 466.59 Interacción AB 72 459.00 Dentro (Error) 1100 231.00 Completar el cuadro con la columna de las E(CM) Hacer las pruebas de hipótesis correspondientes. Calcular los estimadores de los componentes de la varianza. y c) CM Varianzas E(CM) % de la Varianza 3243.00 58.0000 3243 = 231 + 12 * 4 * 58 + 12 * 19 18.82962211 466.59 0.0253 466.59 = 231 + 12 * 25 * 0.0253 * 12 * 19 0.00821361 459.00 19.0000 459 = 231 + 12 * 19 6.16832448 231.00 231.0000 231 74. 99383979 Fte. de Variación GL CM F* p Entre cepas A 24 3243.00 7.06535948 4.8134E -11 Entre tratamientos B 3 466.59 1.01653595 0.39049455 Interacción AB 72 459.00 1.98701299 3.9506E-06 Dentro (Error) 1100 231.00 Para comparar la calidad periodística de tres periódicos A, B y C de ámbito nacional, se eligieron al azar 15 ciudades grandes del país. De cada ciudad se tomó una muestra de 10 lectores de A, 10 lectores de B y 10 lectores de C. A cada lector se le formularon una serie de preguntas cuyo resultado era una puntuación que indicaba la calidad del periódico. Tratados los datos mediante un análisis de la varianza, se obtuvo la siguiente tabla: Fuente de Variación GL SC Entre periódicos 2 14682 Entre ciudades 14 32712 Interacción 28 52570 Error (Dentro) 405 480308 Indicar que tipo de diseño se utilizó para realizar el análisis. Estudiar si existe diferencia entre periódicos, entre ciudades y la significación de la interacción. Es un diseño mixto de dos factores con interacción. El factor periódico es fijo, mientras que el factor ciudad es de efectos aleatorios (las ciudades han sido elegidas al azar). Fuente de Variación GL SC CM F* p Entre periódicos 2 14682 7341.000000 3.909987 0.03180 Entre ciudades 14 32712 2336.571429 0.068106 0.99999 Interacción 28 52570 1877.500000 1.583125 0.03119 Error (Dentro) 405 480308 1185.945679 Un estimador de la varianza del factor ciudades es: \\[ S_{\\beta}^{2} = \\frac{\\left. \\ CM_{B} - CM_{D} \\right.\\ }{nI} = \\frac{2336.571429 - 1185.945679}{10 \\times 3} = 38.35419165 \\] "],
["problemas-anova-dos-factores.html", "Capítulo 8 Problemas ANOVA Dos Factores 8.1 Formulas con más de una variable independiente 8.2 Problemas", " Capítulo 8 Problemas ANOVA Dos Factores 8.1 Formulas con más de una variable independiente Para indicar que la variable \\(Y\\) depende de la variable \\(X\\) usabamos el tilde para construir una fórmula en R, e.g. Y ~ X. Cuando tenemos más de una variable no podemos construir un modelo por cada variable (¿Por qué?), entonces hay que construir una fórmula que muestre que hay más variables. Para eso se usa el símbolo +. Por ejemplo, además de la variable \\(X\\) tenemos también la variable \\(Z\\): Y ~ X + Z Este modelo equivale a decir que la variable \\(Y\\) depende de \\(X\\) y \\(Z\\) asumiendo que no hay interacción entre estas últimas. La interacción se indica con dos puntos :. Por ejemplo: Y ~ X + Z + X:Z Un atajo, para escribir menos es usar la múltiplicación. Que en las formulas en verdad no es una multiplicación sino que se expande para dar como resultado los efectos principales más la interacción entre ellos. Por ejemplo, Y ~ X * Z, es equivalente al ejemplo anterior. Esto es ventajoso para cuando hay más de dos variables: Y ~ X * Z * W Y ~ X + Z + W + X:Z + X:W + Z:W + X:Z:W 8.1.1 Test de aditividad de Tukey Para probar que \\(H_0: D=0\\) se debe realizar el test de aditividad de Tukey. En R está implementado como una función del paquete additivityTests. Especificamente en la función tukey.test. Tiene dos argumentos, data, los datos y el nivel de alpha. Hay que tener especial cuidado en los datos, ya que debe ser una matriz. Una matriz como un data frame en el sentido que tiene filas y columnas. Pero a diferencia de este donde las columnas pueden tener distintos tipos de datos, en la matriz todos los datos tienen que ser del mismo tipo. Por ejemplo: matrix(1:10, nrow = 2) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 Esto implica que los datos que queramos comprobar deben estar en formato ancho y sin las columnas que identifican los tratamientos. Por la tanto si los datos están en formato largo: 1- deben ser puestos en formato ancho, 2- eliminar las columnas que no continen la variable de respuesta, 3 - covertir a matriz, 4- realizar la prueba de aditividad de Tukey. Vean el ejercicio 4 para un ejemplo de como se aplica. 8.2 Problemas Antes de comenzar bajen el archivo donde realizarán su informe reproducible. En la consola copien este código: download.file(&quot;http://bit.ly/informe-anova-dos-factores&quot;, &quot;informe-anova-dos-factores.Rmd&quot;) Pueden abrirlo desde la pestaña de archivos, a la derecha. Cambien el nombre por el suyo en el encabezado y mientras leen este capítulo respondan las preguntas. En todos los casos recuerden hacer gráficos, comprobar supuestos e indicar que tipo de diseño es. 1.- Los Dres. V Vampirus y José Dracul deciden llevar a cabo una experiencia para determinar el tiempo de sangrado de ciertos animales (Horno sapiens), cuando se les produce una mordedura. Aplican un método que supuestamente lo alarga y lo comparan con un control (método tradicional). Como les surgen dudas respecto a la hora en que se realiza el sangrado experimentan a la medianoche y a la madrugada. Los datos corresponden al tiempo de sangrado, en minutos. Tabla 8.1: Minutos de sangrado en Horno sapiens bajo el método tradicional y un nuevo método a la medianoche y a la madrugada. medianoche madrugada medianoche madrugada método tradicional método tradicional método nuevo método nuevo 8.53 17.53 39.14 32 20.53 21.07 26.2 23.8 12.53 20.8 31.33 28.87 14 17.33 45.8 25.06 10.8 20.07 40.2 29.33 ¿En qué formato están los datos? ¿Largo, ancho o intermedio? Comprobar las suposiciones del ANOVA Realizar el ANOVA Si Ud. fuera uno de los mencionados galenos, ¿Cuándo y cómo realizaría el sangrado? 2.- La siguiente tabla muestra la ganancia en peso de ratas macho sometidas a seis dietas diferentes: Tabla 8.2: Datos de ganancia de peso (en g.) en ratas macho sometidas a seis dietas diferentes Fuente Alta.proteina Alta.proteina Baja.proteina Baja.proteina Carne de vaca 107 79 90 51 Carne de vaca 102 100 76 72 Carne de vaca 118 87 90 90 Carne de vaca 104 117 64 95 Carne de vaca 81 111 86 78 Cereal 98 88 107 74 Cereal 74 82 95 74 Cereal 56 77 97 67 Cereal 95 92 98 58 Cereal 111 86 80 89 Carne de cerdo 94 102 49 97 Carne de cerdo 79 108 82 106 Carne de cerdo 96 91 73 70 Carne de cerdo 98 120 86 61 Carne de cerdo 102 105 81 82 ¿Hay diferencia entre los valores de proteína? ¿Hay diferencia entre las fuentes animal y vegetal? La diferencia entre las fuentes animal y vegetal ¿varia con el nivel de proteína? ¿Hay diferencia entre la carne vacuna y la de cerdo? La diferencia entre la carne vacuna y la de cerdo ¿varía con el nivel de proteína? Pista: Piensen en los contrastes que son necesarios hacer. 3.- Se desean comparar tres genotipos distintos de Drosophila melanogaster observando si existen diferencias de viabilidad sembrando 100 y 800 huevos. De este modo para cada uno de los tratamientos se dispusieron 6 preparados y al cabo de un tiempo suficiente de ser sembrados los huevos se obtuvo el porcentaje de huevos que hablan eclosionado. Los resultados fueron: Tabla 8.3: Viabilidad en porcentaje de huevos sembrados para tres genotipos (PP, NN, PN) sembrando distintas cantidades de huevos Huevos.Sembrados PP PP PP PN PN PN NN NN NN 100 93 94 93 95.5 83.5 92 92 91 90 100 90 93 86 92.5 82 82.5 95 84 78 800 83.3 80.1 79.6 84 84.4 77 85.3 89.4 85.4 800 87.6 81.9 49.4 67 69.1 88.4 87.4 52 77 ¿Son diferentes los tres genotipos en cuanto a viabilidad? ¿Es distinta la viabilidad sembrando 100 u 800 huevos? ¿Existe interacción entre genotipo y número de huevos? ¿Es necesario transformar los datos? ¿Por qué? Si su respuesta es afirmativa vuelva a realizar los análisis, comprobar supuestos y revise sus conclusiones. 4.- Se sembró Lactobacíllus fermentus en una mezcla de agar y un medio de cultivo sin tiamina se llenaron 18 tubos de ensayo con una cantidad fija de la mezcla. A grupos de tres, elegidos al azar, se les agregó un sobrenadante que contenía una dosis fija de tres preparaciones de tiamina. Después de una incubación por 18 horas se midió el crecimiento de L. fermentus en términos del largo de la columna de nebulosidad que se desarrolla por debajo del sobrenadante: Tabla 8.4: Nebulosidad (1/Transmitancia) de cultivos de Lactobacíllus fermentus bajo varias concentraciones de tiamina. Concentracion_de_tiamina A B C 0.0125 5.5 6.4 5.2 0.05 6.9 7.3 6.9 0.2 10.1 10 10.2 0.8 12.3 12.5 12.7 3.2 18.3 18.5 15.9 12.8 17.3 19.5 21.6 ¿Qué tipo de diseño es? A un nivel del 1% y del 5% docimar la hipótesis de no significación entre las preparaciones y entre las dosis. Interpretar los resultados. Hacer un estudio de perfiles. 5.- La siguiente tabla ilustra el engorde semanal de cerdos clasificados por sexo, con tres tipos de alimentación A, B y C. Tabla 8.5: Engorde semanal en kilos en cerdos de ambos sexos, con tres tipos de alimentación A, B y C   A B C Macho 9.52 10 9.75 Hembra 9.94 8.51 9.11 Hacer un estudio de perfiles Comprobar los supuestos del modelo Estudiar si existe variabilidad entre sexos y entre tipos de alimentación 6.- Se eligen al azar seis laboratorios para analizar la concentración de humedad en cuatro tipos de levadura: de panadería (I), liviana de cerveza (II), pesada de cerveza (III) y Torula (IV). A cada uno de los laboratorios se les entregan recipientes con uno de los cuatro tipos de levadura, pero sin indicar cual. La concentración de humedad se analiza por secado de horno al vacío. Los datos que se dan a continuación corresponden al contenido de humedad (en términos de % de humedad - 6.00) de 4 alícuotas de 2 gramos cada una. Tabla 8.6: Humedad (en porcentaje) de cuatro tipos de levadura (de panadería (I), liviana de cerveza (II), pesada de cerveza (III) y Torula (IV)) analizada por seis laboratorios diferentes. Laboratorio A B C D E F de panadería 0.81 0.69 0.74 0.6 0.41 0.74 de panadería 0.73 0.82 0.77 0.57 0.52 0.77 de panadería 0.19 0.78 0.7 0.65 0.59 0.79 de panadería 0.82 0.67 0.67 0.57 0.59 0.81 pesada de cerveza 0.86 0.98 0.8 0.63 0.55 0.56 pesada de cerveza 0.87 0.84 0.78 0.66 0.62 0.63 pesada de cerveza 0.92 0.76 0.58 0.58 0.83 0.67 pesada de cerveza 0.99 0.78 0.72 0.49 0.87 0.73 liviana de cerveza 0.65 0.62 0.48 0.5 0.15 0.43 liviana de cerveza 0.68 0.68 0.48 0.34 0.23 0.46 liviana de cerveza 0.65 0.56 0.33 0.47 0.56 0.52 liviana de cerveza 0.73 0.44 0.34 0.43 0.53 0.51 Torula 2.25 2.15 2.16 2.16 1.88 2.06 Torula 2.23 2.13 2.21 1.9 1.88 1.12 Torula 2.2 1.87 2.03 1.86 2.04 2.1 Torula 2.16 1.87 2.1 1.84 2.06 2.15 7.- Eligiendo 4 tardes al azar del verano, se midió la temperatura de un lago a diferentes profundidades, con los siguientes resultados Tabla 8.7: Temperatura de un lago (°C) en cuatro tardes elegidas al azar durante el verano en distintas profundidades (m). profundidad 1 2 3 4 0 23.8 24 24.6 24.8 1 22.6 22.4 22.9 23.2 2 22.2 22.1 22.1 22.2 3 21.2 21.8 21 21.2 4 18.4 19.3 19 18.8 5 13.5 14.4 14.2 13.8 ¿Que tipo de diseño se utilizó? Examinar si hay diferencias entre profundidades y entre fechas No se puede hacer un test formal de normalidad para cada grupo debido al bajo número de replicas. "],
["prueba-de-wilcoxon-mann-whitney-para-dos-pruebas-independientes.html", "Capítulo 9 Prueba de Wilcoxon-Mann-Whitney para dos pruebas independientes 9.1 Datos 9.2 Supuestos 9.3 Procedimiento básico 9.4 Estadísticos 9.5 Hipótesis 9.6 Ejemplo 2 9.7 Prueba de Wilcoxon de rangos con signo para muestras apareadas", " Capítulo 9 Prueba de Wilcoxon-Mann-Whitney para dos pruebas independientes Esta prueba es el análogo más directo de la prueba de t para dos muestras independientes. Existen dos variantes de esta prueba que utilizan distintos estadísticos (W y U), cada uno de los cuales posee su propia tabla de valores críticos. Sin embargo, ambas variantes dan idénticos resultados debido a que son formas diferentes de utilizar y evaluar de igual manera la misma información. Cuando se emplea el estadístico U esta prueba suele ser denominada también como Prueba U de Mann-Whitney. 9.1 Datos Para utilizar esta prueba es necesario contar con dos muestras aleatorias de las dos poblaciones a comparar, tal que \\(\\left ({X_1,X_2,…,X_{N1}}\\right )\\) es la muestra aleatoria de tamaño \\(N_1\\) de la población 1 y \\(\\left ({Y_1,Y_2,…,Y_{N2}} \\right )\\) es la muestra aleatoria de tamaño \\(N_2\\) de la población 2. La función de distribución en probabilidades de la población 1 es \\(F(x)\\) y la función de distribución en probabilidades de la población 2 es \\(G(x)\\). 9.2 Supuestos Los supuestos de esta prueba son: 1. Ambas muestras son muestras aleatorias de las respectivas poblaciones. 2. Los datos son independientes tanto dentro de cada muestra como entre muestras. 3. Las mediciones han sido tomadas utilizando al menos una escala ordinal. 4. Si existen diferencias en las funciones de distribución de ambas poblaciones, estas diferencias están asociadas a la localización de las distribuciones. Esto significa que \\(F(x)=G(x+c)\\), donde \\(c\\) es una constante. Este supuesto sólo es necesario cuando la hipótesis a poner a prueba está asociada a las \\(E(X)\\) y \\(E(Y)\\). 9.3 Procedimiento básico Los datos combinados de ambas muestras deben ser ordenados de menor a mayor. A estos datos ordenados se les deben asignar rangos desde 1 a \\(N1+N2\\). En caso de existir empates, debe asignarse el rango promedio de los rangos correspondientes. Luego de la asignación de los rangos, deben obtenerse las sumas de los rangos de cada muestra como: \\[ \\begin{matrix} R_1=\\sum_{i=1}^{N_1}R(X_i)\\\\ R_2=\\sum_{i=1}^{N_2}R(Y_i) \\end{matrix} \\] donde \\(R_1\\) es la suma de los rangos asignados a la muestra de la población 1, \\(R_2\\) es la suma de los rangos asignados a la muestra de la población 2, \\(R(X_i)\\) es el rango asignado al i-ésimo dato de la muestra de la población 1 y \\(R(Y_i)\\) es el rango asignado al i-ésimo dato de la muestra de la población 2. Igualmente, los valores \\(R_1\\) y \\(R_2\\) se encuentran relacionados, de tal manera que: \\(R_1+R_2=(N_1+N_2)(N_1+N_2+1)/2\\) Esto implica que conocidos \\(N_1\\), \\(N_2\\) y uno de los \\(R_i\\) es posible hallar el otro. 9.3.1 Ejemplo Supongamos que obtenemos los siguientes valores para la población 1 y 2: Tabla 9.1: Datos de dos poblaciones de ejemplo Pob1 Pob2 1 2 2 2 2 4 0 4 4 5 6 3 2 5 1 6 2 6 4 4 A continuación debemos reunir esos datos en una solo conjunto, agregando una identificación1 Tabla 9.2: Datos de ambas poblaciones juntas Pob Valor Pob1 1 Pob1 2 Pob1 2 Pob1 0 Pob1 4 Pob1 6 Pob1 2 Pob1 1 Pob1 2 Pob1 4 Pob2 2 Pob2 2 Pob2 4 Pob2 4 Pob2 5 Pob2 3 Pob2 5 Pob2 6 Pob2 6 Pob2 4 A continuación se ordenan y se la asigna un número de 1 hasta \\(N_1+N_2\\) según el orden (columna Rango)2. Los empates deben tratarse de forma especial ya que no hay forma de decidir que número va primero. Por lo tanto se promedian los valores de sus rangos (columna Rango_Empates ) datos_long &lt;- datos_long %&gt;% mutate(Rango = rank(Valor, ties.method = &quot;r&quot;)) %&gt;% arrange(Rango) %&gt;% mutate(Rango_Empates = rank(Valor)) kable(datos_long) Pob Valor Rango Rango_Empates Pob1 0 1 1.0 Pob1 1 2 2.5 Pob1 1 3 2.5 Pob2 2 4 6.5 Pob2 2 5 6.5 Pob1 2 6 6.5 Pob1 2 7 6.5 Pob1 2 8 6.5 Pob1 2 9 6.5 Pob2 3 10 10.0 Pob2 4 11 13.0 Pob1 4 12 13.0 Pob2 4 13 13.0 Pob2 4 14 13.0 Pob1 4 15 13.0 Pob2 5 16 16.5 Pob2 5 17 16.5 Pob2 6 18 19.0 Pob2 6 19 19.0 Pob1 6 20 19.0 9.4 Estadísticos 9.4.1 Variante Wilcoxon (W) 9.4.1.1 Sin empates Si no existen empates o si estos son pocos, el estadístico a calcular es: \\[W=R_1\\] Los valores críticos para este estadístico (\\(w_p\\)) pueden obtenerse de la Tabla A7. En esta tabla se presentan los valores críticos (cuantiles) para \\(1-\\alpha\\) en el rango 0.001-0.5. Los valores para el rango 0.5-0.999 pueden obtenerse como: \\[w_{1-p}=N_1 (N_1+N_2+1)-w_p\\] La Tabla A7 permite obtener los valores críticos para muestras donde \\(N_1\\le20\\) y \\(N_2\\le20\\). En el caso de que \\(N_1\\) y/o \\(N_2\\) fueran mayores que 20, los valores críticos pueden obtenerse mediante una aproximación normal de la forma: \\[w_p\\cong \\frac{N_1 (N_1+N_2+1)}{2}+z_p \\sqrt{\\frac{N_1 N_2 (N_1+N_2+1)}{12}}\\] 9.4.1.2 Con empates Si existen numerosos empates, el estadístico a calcular es: \\[ W_1=\\frac{W-\\frac{N_1(N_1+N_2+1)}{2}}{\\sqrt{\\frac{N_1 N_2}{(N_1+N_2)(N_1+N_2-1)}\\sum_{i=1}^{N_1+N_2}{R_i^2}-\\frac{N_1 N_2 (N_1+N_2+1)^2}{4(N_1+N_2-1)}}} \\] donde \\(R_i^2\\) son los \\(R(X_i)\\) y \\(R(Y_i)\\) elevados al cuadrado. Este estadístico se distribuye según una distribución normal estándar. 9.4.2 Variante Mann-Whitney (U) Se calculan dos estadísticos, el \\(U\\) y el \\(U’\\). Ambos estadísticos se obtienen de igual forma como: \\[ \\begin{matrix} U = N_1 N_2+\\frac{N_1 (N_1+1)}{2}-R_1 \\\\ U&#39;=N_2 N_1+\\frac{N_2 (N_2+1)}{2}-R_2 \\end{matrix} \\] Además, cualquiera de ellos puede obtenerse a partir del otro teniendo en cuenta que la relación entre ellos es: \\[U=N_1 N_2-U&#39;\\] Cómo puede verse a partir de las ecuaciones que definen a \\(U\\) y \\(U’\\), puede considerarse que \\(U\\) es el estadístico asociado a la población 1, mientras que \\(U’\\) es el estadístico asociado a la población 2. Igualmente, dado que la asignación de población 1 y población 2 es puramente arbitraria, los estadísticos \\(U\\) y \\(U’\\) pueden corresponder indistintamente a cualquiera de las dos muestras a comparar. Por una razón exclusivamente operativa y relacionada con la tabla de valores críticos que se utilizará, se considera como población 1 a la que posea el mayor tamaño muestral y como población 2 a la que posea el menor tamaño muestral (\\(N_1 &lt; N_2\\)). Los valores críticos para los estadísticos U o U’ pueden obtenerse utilizando la Tabla \\(U\\) para muestras donde \\(N_1\\le 20\\) y \\(N_2\\le 20\\). Para tamaños muestrales mayores, pueden utilizarse aproximaciones normales. Dependiendo de la existencia o no de empates los estadísticos a calcular son: 9.4.2.1 Sin empates \\[Z=\\frac{U-\\frac{(N_1 N_2)}{2}}{\\sqrt{\\frac{N_1 N_2 (N_1+N_2+1)}{12}}}\\] 9.4.2.2 Con empates Previamente al cálculo del estadístico y considerando la existencia de E grupos de datos empatados debe obtenerse el valor \\(\\sum e\\) tal que: \\[\\sum e=\\sum_{i=1}^E(\\varepsilon_i^3-\\varepsilon_i ) \\] donde \\(\\varepsilon_i\\) es el número de datos empatados en el grupo i-ésimo de datos empatados. Una vez obtenido el \\(\\sum e\\), el estadístico corregido por empates (\\(z_c\\)) puede obtenerse como: \\[Z_c=\\frac{U-\\frac{(N_1 N_2)}{2}} {\\sqrt{\\left [\\frac{N_1N_2}{(N_1+N_2)^2-(N_1+N_2)}\\right ]\\left [\\frac{(N_1 + N_2)^3 -(N_1+N_2)-\\sum e)}{12}\\right ]}}\\] Ambos estadísticos (\\(z\\) y \\(z_c\\)) se distribuyen de acuerdo a una distribución normal estándar. 9.5 Hipótesis En términos generales, la prueba de Wilcoxon-Mann-Whitney se utiliza para poner a prueba hipótesis sobre las distribuciones \\(F(x)\\) y \\(G(x)\\). Como la prueba es sensible a diferencias en las tendencias centrales puede emplearse para poner a prueba hipótesis sobre \\(E(X)\\) y \\(E(Y)\\). 9.5.1 Prueba a dos colas \\(H_0: F(x)=G(x)\\) para todo \\(x\\) ó \\(E(X)=E(Y)\\) \\(H_a: F(x)≠G(x)\\) para algún \\(x\\) ó \\(E(X)≠E(Y)\\) 9.5.1.1 Variante Wilcoxon* Utilizando el estadístico \\(W\\) los criterios de decisión son: Si \\(W\\le w_{\\alpha/2}\\) ó \\(W\\ge w_{1-\\alpha/2}\\) Entonces Rechazo \\(H_0\\) Si \\(w_{\\alpha/2}&lt;W&lt;w_{1-\\alpha/2}\\) Entonces No rechazo \\(H_0\\) Para evitar calcular el \\(w_{1-\\alpha/2}\\), puede calcularse un estadístico \\(W’\\) como: \\[W&#39;=N_1 (N_1+N_2+1)-W\\] quedando definidos los criterios de decisión como: Si \\(W\\le w_{\\alpha/2}\\) ó \\(W’\\le w_{\\alpha/2}\\) Entonces Rechazo \\(H_0\\) Si \\(W&gt;w_{\\alpha/2}\\) y \\(W’&gt;w_{\\alpha/2}\\) Entonces No rechazo \\(H_0\\) Utilizando el estadístico \\(W1\\) los criterios de decisión son: Si \\(2*[1-P(Z≤ W_1 )]\\le \\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(2*[1-P(Z≤ W_1 )]&gt;\\alpha\\) Entonces No rechazo \\(H_0\\) 9.5.1.2 Variante Mann-Whitney Definiendo como estadístico \\(U\\) al mayor entre \\(U\\) y \\(U’\\), los criterios de decisión son: Si \\(U≥U_{N1,N2,\\alpha/2}\\) Entonces Rechazo \\(H_0\\) Si \\(U&lt;U_{N1,N2,\\alpha/2}\\) Entonces No rechazo \\(H_0\\) Donde \\(U_{N1,N2,\\alpha/2}\\) es el valor crítico obtenido de la Tabla U. Utilizando la aproximación normal, ya sea sin empates (\\(z\\)) o con corrección por empates (\\(z_c\\)), los criterios de decisión son: Si \\(2*[1-P(Z≤ z )]≤\\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(2*[1-P(Z≤ z )]&gt;\\alpha\\) Entonces No rechazo \\(H_0\\) 9.5.2 Prueba de una cola a la izquierda \\(H_0\\): \\(F(x)=G(x)\\) para todo \\(x\\) ó \\(E(X)=E(Y)\\) \\(H_a\\): \\(F(x)&gt;G(x)\\) para algún \\(x\\) ó \\(E(X)&lt;E(Y)\\) Para visualizar la relación entre las hipótesis planteadas en términos de las funciones de distribución y las hipótesis planteadas en términos de las esperanzas puede utilizarse la 9.1. En esta figura muestra las funciones de densidad y de distribución para dos variables normales que difieren solamente en el valor de su media. Figura 9.1: Funciones de densidad [\\(f(x)\\) y \\(g(x)\\)] y funciones de distribución [\\(F(x)\\) y \\(G(x)\\)] para dos variables aleatorias que se distribuyen normalmente con \\(\\sigma^2=3\\) y que sólo difieren en el valor de \\(\\mu=10\\) y \\(\\mu=15\\). 9.5.2.1 Variante Wilcoxon Utilizando el estadístico \\(W\\) los criterios de decisión son: Si \\(W≤w_\\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(W&gt;w_\\alpha\\) Entonces No rechazo \\(H_0\\) Utilizando el estadístico \\(W_1\\) los criterios de decisión son: Si \\(P(Z≤W_1 )≤ \\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(P(Z&gt;W_1 )&gt; \\alpha\\) Entonces No rechazo \\(H_0\\) 9.5.2.2 Variante Mann-Whitney Definiendo como estadístico \\(U\\) al \\(U\\), los criterios de decisión son: Si \\(U≥U_{N1,N2,\\alpha}\\) Entonces Rechazo \\(H_0\\) Si \\(U&lt;U_{N1,N2,\\alpha}\\) Entonces No rechazo \\(H_0\\) Donde \\(U_{N1,N2,\\alpha}\\) es el valor crítico obtenido de la Tabla U. Utilizando la aproximación normal, ya sea sin empates (\\(z\\)) o con corrección por empates (\\(z_c\\)), los criterios de decisión son: Si \\(1-P(Z\\le z)\\le\\alpha\\) ó \\(1-P(Z≤z_c )\\le\\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(1-P(Z≤z)&gt;\\alpha\\) ó \\(1-P(Z≤z_c )&gt;\\alpha\\) Entonces No rechazo \\(H_0\\) 9.5.3 Prueba de una cola a la derecha \\(H_0\\): \\(F(x)=G(x)\\) para todo x ó \\(E(X)=E(Y)\\) \\(H_a\\): \\(F(x)&lt;G(x)\\) para algún x ó \\(E(X)&gt;E(Y)\\) 9.5.3.1 Variante Wilcoxon Utilizando el estadístico \\(W\\) los criterios de decisión son: Si \\(W≥w_{1-\\alpha}\\) Entonces Rechazo \\(H_0\\) Si \\(W&lt;w_{1-\\alpha}\\) Entonces No rechazo \\(H_0\\) También puede utilizarse el estadístico \\(W’\\), quedando entonces los criterios de decisión como: Si \\(W&#39;≤w_\\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(W&#39;&gt;w_\\alpha\\) Entonces No rechazo \\(H_0\\) Utilizando el estadístico \\(W_1\\) los criterios de decisión son: Si \\(P(Z≤W_1 )≤ \\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(P(Z≤W_1 )&gt; \\alpha\\) Entonces No rechazo \\(H_0\\) 9.5.3.2 Variante Mann-Whitney Definiendo como estadístico U al U, los criterios de decisión son: Si \\(U≥U_{N1,N2,\\alpha}\\) Entonces Rechazo \\(H_0\\) Si \\(U&lt;U_{N1,N2,\\alpha}\\) Entonces No rechazo \\(H_0\\) Donde \\(U_{N1,N2,\\alpha}\\) es el valor crítico obtenido de la Tabla U. Utilizando la aproximación normal, ya sea sin empates (\\(z\\)) o con corrección por empates (\\(z_c\\)), los criterios de decisión son: Si \\(1-P(Z≤z)≤\\alpha\\) ó \\(1-P(Z≤z_c )≤\\alpha\\) Entonces Rechazo \\(H_0\\) Si \\(1-P(Z≤z)&gt;\\alpha\\) ó \\(1-P(Z≤z_c )&gt;\\alpha\\) Entonces No rechazo \\(H_0\\) 9.6 Ejemplo 2 Un ecólogo desea comparar las tallas de raneya (pez demersal bentónico) consumidas por machos y hembras del lobo marino. Los largos totales de las raneyas fueron estimados a partir de los otolitos hallados en los estómagos. Debido a que las hembras son predadores más costeros que los machos y las áreas costeras son áreas de cría para diversas especies de peces, el biólogo especula que las raneyas consumidas por las hembras deberían ser más pequeñas que las consumidas por los machos. Los datos obtenidos fueron: Tabla 9.3: Largo total (cm) de las raneyas consumidas por machos y hembras del lobo marino Machos Hembras 15 9 10 8 14 5 17 5 16 5 11 9 15 9 20 10 13 13 13 15 5 12 6 Teniendo en cuenta que el tamaño muestral de las hembras es mayor que el de los machos, se considera como población 1 (X) a las hembras y como población 2 (Y) a los machos. En función de esto, las hipótesis para este enunciado son: \\(H_0\\): \\(F(x)=G(x)\\) \\(H_a\\): \\(F(x)&gt;G(x)\\) ó \\(E(X)&lt;E(Y)\\) Los cálculos realizados son: Sexo LT R_i e Hembras 5 2.5 60 Hembras 5 2.5 0 Hembras 5 2.5 0 Hembras 5 2.5 0 Hembras 6 5.0 0 Hembras 8 6.0 0 Hembras 9 8.0 24 Hembras 9 8.0 0 Hembras 9 8.0 0 Machos 10 10.5 6 Hembras 10 10.5 0 Machos 11 12.0 0 Hembras 12 13.0 0 Machos 13 15.0 24 Hembras 13 15.0 0 Hembras 13 15.0 0 Machos 14 17.0 0 Machos 15 19.0 24 Machos 15 19.0 0 Hembras 15 19.0 0 Machos 16 21.0 0 Machos 17 22.0 0 Machos 20 23.0 0 Variante Wilcoxon W 117,5 W’ 218,5 1 cola w14;9;0,05 142 2 colas w14;9;0,025 137 w14;9;0,975 199 W1 -3,19943151 1 cola Valor p 0,00068856 2 colas Valor p 0,00137712 Variante Mann-Whitney 2 colas U 113,5 \\(U_{14;9;0,025}\\) 95 1 cola U 113,5 \\(U_{14;9;0,05}\\) 90 \\(z\\) 3,18120098 1 cola Valor p 0,00073339 2 colas Valor p 0,00146679 \\(z_c\\) 3,19943151 1 cola Valor p 0,00068856 2 colas Valor p 0,00137712 De acuerdo a los resultados obtenidos, es posible rechazar la hipótesis nula. Considerando la hipótesis a dos colas, es posible rechazar la hipótesis nula. Esta prueba puede realizarse con R. Se encuentra en la función wilcox.test() en el paquete stats. Hay que tener en cuenta que la definición del estadístico \\(W\\) en R es diferente. En este caso es la suma de rangos de la primer población menos el mínimo. Por lo tanto, el estadístico es el mismo que el \\(U\\) de Mann-Whitney. Los resultados para el ejemplo son: wilcox.test(datos_ejemplo2$Machos, datos_ejemplo2$Hembras, alternative = &quot;greater&quot;) ## Warning in wilcox.test.default(datos_ejemplo2$Machos, ## datos_ejemplo2$Hembras, : cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: datos_ejemplo2$Machos and datos_ejemplo2$Hembras ## W = 113.5, p-value = 0.0007681 ## alternative hypothesis: true location shift is greater than 0 Nótese que la prueba da el valor de la variante Mann-Whitney. Las mismas consideraciones que se hicieron para la prueba de rangos de Wilcoxon en la sección anterior deben hacerse aquí. Es decir, especificar la cola a usar y rechazar cuando p≤α 9.7 Prueba de Wilcoxon de rangos con signo para muestras apareadas Esta prueba implica comparar una serie de pares ordenados \\((x_{i},\\ y_{i})\\) obtenidos de una serie de variables aleatorias bivariadas \\((X_{i},\\ Y_{i})\\). Básicamente la prueba se basa en la asignación de rangos a las diferencias obtenidas \\(D_{i}\\) para cada uno de los pares ordenados de la muestra. Por lo tanto, dada una muestra aleatoria de \\(N\\) pares ordenados, se calculan las diferencias como: \\[ D_{i} = x_{i} - y_{i}\\text{ para }i = 1, 2, 3,\\ldots, N \\] Una vez obtenidas las \\(D_{i}\\), se descartan todas las diferencias donde \\(D_{i} = 0\\). De esta forma el tamaño muestral real para la prueba (\\(n\\)) se reduce de tal forma que \\(n \\leq N\\). Posteriormente se ordenan las \\(D_{i}\\) remanentes de menor a mayor teniendo en cuenta el valor absoluto de las diferencias \\((\\left| D_{i} \\right|)\\). Luego, se asignan los rangos a las \\(|D_{i}|\\) desde \\(1\\) hasta \\(n\\). En aquellos casos donde las \\(|D_{i}|\\) son iguales (empates), se les asigna a todas las diferencias empatadas el promedio de los rangos correspondientes. Finalmente, se les asigna a los rangos el signo correspondiente a las \\(D_{i}\\) obteniéndose de esta forma los rangos con signo \\(R_{i}\\). 9.7.1 Supuestos La prueba de Wilcoxon tiene los siguientes supuestos: La distribución de los \\(D_{i}\\) es simétrica. Las \\(D_{i}\\) son mutuamente independientes. Todas las \\(D_{i}\\) tiene igual media. Las \\(D_{i}\\) están medidas utilizando como mínimo una escala de intervalo. 9.7.2 Estadísticos Para esta prueba se calculan dos estadísticos, el \\(T^{+}\\) y el \\(T^{-}\\). Ambos estadísticos se calculan como: \\[ \\begin{matrix} T^{+} = \\left| \\sum_{}^{}R_{i} \\right| &amp; \\text{si }R_{i} &gt; 0 \\\\ T^{-} = \\left| \\sum_{}^{}R_{i} \\right| &amp; \\text{si }R_{i} &lt; 0 \\\\ \\end{matrix} \\] Los valores críticos para estos estadísticos wp se obtienen de una tabla construida ad hoc para esta prueba (Tabla A12). En esta tabla se presentan los valores críticos (cuantiles) para \\(1 - \\alpha\\) en el rango 0.005-0.5. Los valores para el rango 0.5-0.995 pueden obtenerse como: \\[ w_{p} = \\frac{n\\left( n + 1 \\right)}{2} - w_{1 - p} \\] presentándose también en 12 el primer término de esta diferencia para facilitar el cálculo de los valores críticos. El n es el tamaño muestral efectivo de la prueba. Para \\(n &gt; \\ 50\\) o para aquellos casos con numerosos empates, puede obtenerse un estadístico que se distribuye en forma aproximadamente normal y que se calcula como: \\[ Z = \\frac{\\sum_{i = 1}^{n}R_{i}}{\\sqrt{\\sum_{i = 1}^{n}R_{i}^{2}}} \\] 9.7.3 Hipótesis 9.7.3.1 Prueba a dos colas La prueba a dos colas implica poner a prueba las siguientes hipótesis: \\(H_{0}:\\ E(D) = 0\\) ó \\(E(X) = E(Y)\\) \\(H_{a}:\\ E\\left( D \\right) \\neq 0\\) ó \\(E\\left( X \\right) \\neq E(Y)\\) Para poner a prueba esta hipótesis se utiliza el menor de los estadísticos T+ y T-, comparando el estadístico con los valores críticos \\(w_{\\alpha/2}\\) y \\(w_{1 - \\alpha/2}\\). Si \\(T^{\\pm} &gt; \\ w_{1 - \\frac{\\alpha}{2}}\\) ó \\(T^{\\pm} &lt;w_{\\frac{\\alpha}{2}}\\) Entonces Rechazo \\(H_{0}\\) Si \\(w_{\\frac{\\alpha}{2}}\\ &lt; T^{\\frac{+}{-}} &lt; \\ w_{1 - \\frac{\\alpha}{2}}\\) Entonces No rechazo \\(H_{0}\\) Empleando la aproximación normal, si Si \\(2*\\left\\lbrack 1 - P\\left( z \\leq \\left| Z \\right| \\right) \\right\\rbrack \\leq \\alpha\\) Entonces Rechazo \\(H_{0}\\) Si \\(2*\\left\\lbrack 1 - P\\left( z \\leq \\left| Z \\right| \\right) \\right\\rbrack &gt; \\alpha\\) Entonces No rechazo \\(H_{0}\\) 9.7.3.2 Prueba de una cola a la izquierda La prueba de una cola a la izquierda implica poner a prueba las siguientes hipótesis: \\(H_{0}:\\ E\\left( D \\right) \\geq 0\\) ó \\(E\\left( X \\right) &gt; E(Y)\\) \\(H_{a}:\\ E(D) &lt; 0\\) ó \\(E(X) &lt; E(Y)\\) Para poner a prueba esta hipótesis se utiliza el estadístico T+, comparando el estadístico con el valor crítico \\(w_{\\alpha}\\) Si \\(T^{+} &lt; w_{\\alpha}\\) Entonces Rechazo \\(H_{0}\\) Si \\(T^{+} \\geq \\text{\\ w}\\alpha\\) Entonces No rechazo \\(H_{0}\\) Empleando la aproximación normal, si Si \\(1 - P\\left( z \\leq \\left| Z \\right| \\right) \\leq \\alpha\\) Entonces Rechazo \\(H_{0}\\) Si \\(1 - P\\left( z \\leq \\left| Z \\right| \\right) &gt; \\alpha\\) Entonces No rechazo \\(H_{0}\\) 9.7.3.3 Prueba de una cola a la derecha La prueba de una cola a la derecha implica poner a prueba las siguientes hipótesis: \\(H_{0}:\\ E\\left( D \\right) \\leq 0\\) ó \\(E\\left( X \\right) \\leq E(Y)\\) \\(H_{a}:\\ E(D) &gt; 0\\) ó \\(E(X) &gt; E(Y)\\) Para poner a prueba esta hipótesis se utiliza el estadístico T+, comparando el estadístico con el valor crítico w1-α Si \\(T^{+} &gt; \\ w_{1 - \\alpha}\\) Entonces Rechazo \\(H_{0}\\) Si \\(T^{+}\\leq w_{1 - \\alpha}\\)Entonces No rechazo \\(H_{0}\\) Empleando la aproximación normal, si Si \\(1 - P\\left( z \\leq \\left| Z \\right| \\right) \\leq \\alpha\\) Entonces Rechazo \\(H_{0}\\) Si \\(1 - P\\left( z \\leq \\left| Z \\right| \\right) &gt; \\alpha\\) Entonces No rechazo \\(H_{0}\\) Ejemplo 1: Una especie de ave pone dos huevos por temporada reproductiva. Un etólogo desea evaluar si el primer pichón que eclosiona presenta un comportamiento menos agresivo que el segundo. Para ello, tomó 12 nidos al azar y marcó a los pichones para poder identificar cuál de ellos fue el primero en eclosionar. Asimismo, registró el número de “peleas entre hermanos” iniciadas por cada pichón. Este registro fue iniciado a partir del momento en que ambos pichones estaban en condiciones de iniciar una pelea y finalizó cuando alguno de los pichones abandonó el nido. Los datos obtenidos fueron: Tabla 9.4: Número de peleas iniciadas Pichón 1 X Pichón 2 Y 96 91 65 77 80 71 72 87 76 77 72 72 81 88 88 86 90 91 64 68 77 71 65 70 \\(H_{0}:\\ E\\left( X \\right) \\geq E(Y)\\) ó \\(E\\left( D \\right) &gt; 0\\) \\(H_{a}:\\ E\\left( X \\right) &lt; E(Y)\\) ó \\(E\\left( D \\right) &lt; 0\\) Cálculo de los estadísticos: Orden X Y \\(D_{i}\\) \\(\\|D_{i}\\|\\) Rango \\(R_{i}\\) 1 72 72 0 0 - - 2 76 77 -1 1 1.5 -1.5 3 90 91 -1 1 1.5 -1.5 4 88 86 2 2 3 3 5 64 68 -4 4 4 -4 6 96 91 5 5 5.5 5.5 7 65 70 -5 5 5.5 -5.5 8 77 71 6 6 7 7 9 81 88 -7 7 8 -8 10 80 71 9 9 9 9 11 65 77 -12 12 10 -10 12 72 87 -15 15 11 -11 T+ 24.5 Valores 1 cola w_{0.05} 14 T- 41.5 críticos 2 colas w_{0.025} 11 n 11 w_{0.975} 55 Z -0.756 \\(P(z &lt; = \\|Z\\|)\\) 0.775 \\(1 - P(z \\leq \\|Z\\|) = p\\) (1 cola) 0.225 \\(2*\\lbrack 1 - P(z \\leq \\|Z\\|)\\rbrack = p\\) (2 colas) 0.449 La conclusión de esta prueba es no rechazar la \\(H_{0}\\), ya que el \\(T^{+} &gt; w_{0.05}\\). Utilizando la aproximación normal, la conclusión es similar (p=0.225). La prueba de Wilcoxon a dos colas puede realizarse con R. Esta prueba se encuentra en la función wilcox.test() en el paquete stats. Los resultados para el ejemplo son: ## ## Wilcoxon signed rank test with continuity correction ## ## data: X and Y ## V = 24.5, p-value = 0.2382 ## alternative hypothesis: true location shift is less than 0 Nótese que en esta tabla de resultados se presenta como estadístico T al menor entre \\(T^{+}\\) y \\(T^{-}\\), y se nombra como V. Por otro lado, debemos indicar que cola queremos usar en el parámetro alternative para que nos calcule la probabilidad. Si queremos usar dos colas, la opción por defecto “two.sided” es la que queremos. Entonces la probabilidad es calculada como \\(2*\\lbrack P(w \\leq W)\\rbrack\\), donde w es el valor del estadístico y W es la distribución de Wilcoxon. En cambio, si queremos la cola derecha, la opción a usar es “less”; si queremos la cola izquierda la opción es “greater”. En el primer caso se calcula la \\(P(w \\leq W)\\); mientras que en el segundo caso es calculada su complemento: \\(\\operatorname{1-P}\\left( w \\leq W \\right)\\). En todos los casos debemos comparar \\(p\\) con nuestro \\(\\alpha\\) y la regla de decisión será: Si \\(p \\leq \\alpha\\) Rechazo \\(H_{0}\\). Esto se puede hacer sin muchas vueltas con la función gather() del paquete tidyr.↩ Ver la función rank()↩ "],
["ejercicios-de-dos-muestras-no-parametrico.html", "Capítulo 10 Ejercicios de dos muestras no paramétrico 10.1 Reproduciendo el algoritmo manualmente 10.2 Funciones no paramétricas en R 10.3 Fórmulas 10.4 Muestras apareadas 10.5 Problemas", " Capítulo 10 Ejercicios de dos muestras no paramétrico 10.1 Reproduciendo el algoritmo manualmente Si bien el algoritmo está implementado en R, resulta importante entender como funciona. Una buena forma es realizar los cálculos manualmente. No vamos a hacer todo manualmente, vamos a aprovechar algunas funciones para hacer los pasos más tediosos. Usando los datos del ejemplo de los lobos marinos: library(tidyverse) lobos_marinos &lt;- tibble(Machos = c(15, 10, 14, 17, 16, 11, 15, 20, 13, rep(NA, 5)), Hembras = c(9, 8, 5, 5, 5, 9, 9, 10, 13, 13, 15, 5, 12,6)) lobos_marinos ## # A tibble: 14 x 2 ## Machos Hembras ## &lt;dbl&gt; &lt;dbl&gt; ## 1 15 9 ## 2 10 8 ## 3 14 5 ## 4 17 5 ## 5 16 5 ## 6 11 9 ## 7 15 9 ## 8 20 10 ## 9 13 13 ## 10 NA 13 ## 11 NA 15 ## 12 NA 5 ## 13 NA 12 ## 14 NA 6 Recordemos que primero cargué el paquete tidyverse que nos va facilitar muchas funciones para trabajar con los datos. Luego creé el objeto lobos_marinos que contiene los datos de la longitudes de raneyas consumidas por machos y hembras. Notarás que usé una función nueva, rep(). Lo que hace es repetir la secuencia que quieras el número de veces indicados. En este caso NA (not available, no disponible) cinco veces. Esto lo hice porque los data frame son estructuras rectangulares de datos y todas las columnas deben tener el mismo número de observaciones. Luego, se puede ver el resultado escribiendo el nombre del objeto. Ahora ya tengo los datos, pero para ordenarlos debo tener los largos en una columna, sin perder de vista que datos corresponden a machos y cuales a hembras. lobos_marinos %&gt;% gather(key = Sexo, value = LT) ## # A tibble: 28 x 2 ## Sexo LT ## &lt;chr&gt; &lt;dbl&gt; ## 1 Machos 15 ## 2 Machos 10 ## 3 Machos 14 ## 4 Machos 17 ## 5 Machos 16 ## 6 Machos 11 ## 7 Machos 15 ## 8 Machos 20 ## 9 Machos 13 ## 10 Machos NA ## # ... with 18 more rows La función gather() se encarga de juntar los datos. Por defecto junta todas las columnas y devuelve dos. Una es la key (clave) que es el nombre de la columna a la cual pertence a el valor de la columna value. Aquí le he dado como nombres Sexo a la columna key y LT (Largo Total) a la de value. Todo muy bonito, pero hay un problema. Tenemos los valores NA que no necesitamos. Los podemos quitar con na.omit() que va eliminar todas las filas que contengan NA. lobos_marinos %&gt;% gather(key = Sexo, value = LT) %&gt;% na.omit() ## # A tibble: 23 x 2 ## Sexo LT ## &lt;chr&gt; &lt;dbl&gt; ## 1 Machos 15 ## 2 Machos 10 ## 3 Machos 14 ## 4 Machos 17 ## 5 Machos 16 ## 6 Machos 11 ## 7 Machos 15 ## 8 Machos 20 ## 9 Machos 13 ## 10 Hembras 9 ## # ... with 13 more rows Ya no están las filas con NA. Hay que tener mucho cuidado cuando usamos na.omit porque esto elimina la fila con tan solo un valor de NA y puede que falte un dato pero ¡los otros sirvan! Ya están los datos en el formato adecuado. Ese formato se conoce como largo; una fila corresponde a una observación. Ahora podemos comenzar con el algoritmo que está la sección de teoría 9.3. El primer paso es rankear los datos. Recordemos que si hay empates se debe poner el valor promedio de los rangos de cada uno. Es decir que si tenemos cada dos datos son 10 y suponiendo que a cada uno le asignamos el rango 11 y 12. Entonces el valor que le corresponde es el promedio de 11 y 12, 11.5. Afortunadamente, la función rank() tiene un argumento para indicar como queremos definir los empates. Lo que pide el algoritmo es la media que equivale colocar el argumento ties = &quot;average&quot;. lobos_marinos &lt;- lobos_marinos %&gt;% gather(key = Sexo, value = LT) %&gt;% na.omit() %&gt;% mutate(rango = rank(x = LT, ties = &quot;average&quot;)) lobos_marinos ## # A tibble: 23 x 3 ## Sexo LT rango ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Machos 15 19 ## 2 Machos 10 10.5 ## 3 Machos 14 17 ## 4 Machos 17 22 ## 5 Machos 16 21 ## 6 Machos 11 12 ## 7 Machos 15 19 ## 8 Machos 20 23 ## 9 Machos 13 15 ## 10 Hembras 9 8 ## # ... with 13 more rows Usé la función mutate() que agrega una columna a un data frame. Bien, ya tengo los rangos y he resuelto el problema de los empates en un solo paso gracias a la función rank(). Guardé los resultados con el mismo nombre porque no modifiqué los datos originales, sino que solo agregué variables nuevas. Ahora podemos calcular el estadístico \\(W\\) que igual a la suma de los rangos de la población 1. Recordemos que la definición de población 1 y 2 es totalmente arbitraria. lobos_marinos %&gt;% group_by(Sexo) %&gt;% summarise(W = sum(rango), N = n()) ## # A tibble: 2 x 3 ## Sexo W N ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Hembras 118. 14 ## 2 Machos 158. 9 Aquí hay dos nuevas funciones. La primera, group_by(), agrupa los valores de acuerdo a la/s columna/s especificadas. En este caso Sexo. En segundo lugar, la función summarize resume los datos según las funciones que especifiquemos. Aquí, sumamos los valores de la columna rango según la columna sexo. Estos valores que calculamos sirven si los empates son pocos. ¿Pero realmente son pocos? Podemos comprobarlo con la ayuda de otra función: frecuencias &lt;- table(lobos_marinos$LT) sum(frecuencias[frecuencias &gt; 1])/sum(frecuencias) ## [1] 0.6521739 La función table() calcula las frecuencias de los valores y usé esas frecuencias para ver cuantos de esos se repiten, es decir que la frecuencia es mayor a 1. Y dividí la suma de los frecuencias de valores repetidos por el N total. No tienen que entender todo el código de arriba, pero eso es lo que hice. Entonces, el 65% de los valores están empatados. Es un porcentaje alto de los datos. Lo que justifica calcular el [estadístico mucho mas complejo][w-con-empates]. Es una formula larga que puede dar lugar a errores. Hay varios valores que se usan varias veces y conviene calcularlos antes. lobos_marinos_W_N &lt;- lobos_marinos %&gt;% group_by(Sexo) %&gt;% summarise(W = sum(rango), N = n()) N1 &lt;- lobos_marinos_W_N$N[1] N2 &lt;- lobos_marinos_W_N$N[2] N1N2 &lt;- N1 * N2 N1_N2 &lt;- N1 + N2 W &lt;- lobos_marinos_W_N$W[1] Ri2 &lt;- sum(lobos_marinos$rango^2) (W - N1*(N1_N2+1)/2) / sqrt((N1N2/(N1_N2*(N1_N2-1))*Ri2 - (N1N2*(N1_N2+1)^2)/(4*(N1_N2-1)))) ## [1] -3.199432 wilcox_empates &lt;- function(r1, r2){ W &lt;- sum(r1) r &lt;- c(r1, r2) N1 &lt;- length(r1) N2 &lt;- length(r2) N1N2 &lt;- N1 * N2 N1_N2 &lt;- N1 + N2 Ri2 &lt;- sum(r^2) (W - N1*(N1_N2+1)/2) / sqrt((N1N2/(N1_N2*(N1_N2-1))*Ri2 - (N1N2*(N1_N2+1)^2)/(4*(N1_N2-1)))) } rango_machos &lt;- lobos_marinos %&gt;% filter(Sexo == &quot;Machos&quot;) %&gt;% pull(rango) rango_hembras &lt;- lobos_marinos %&gt;% filter(Sexo == &quot;Hembras&quot;) %&gt;% pull(rango) wilcox_empates(rango_machos, rango_hembras) ## [1] 3.199432 Lo primero es probar que lo que pretendemos convertir en función funcione como esperamos fuera. Luego, es más fácil convertir el código en función. No vamos a entrar en detalles ahora sobre las funciones. Solo tienen que saber que se crean con function, se definen los argumentos, el código a ejecutar y se guarda como un objeto. Ejercicio 3.3 1. Notarán que los resultados son iguales pero de signo opuesto. ¿Por qué? Implementen el algoritmo para calcular la U de Mann-Whitney. 10.2 Funciones no paramétricas en R Los ejercicios a continuación deben realizarse utilizando pruebas no paramétricas. La prueba de Wilcoxon o Mann-Whitney para dos muestras o la prueba de rangos con signos de Wilcoxon para muestras apareadas. En R, la función wilcox.test() realiza esta pruebas para dos muestras o muestras apareadas. Por ejemplo, dados estos datos de constante de permeabilidad de la membrana corioaminiótica humana a término (x) y entre 12 y 26 semanas de gestación (y). La hipótesis alternativa de interés es que hay mayor permabilidad de esta membrana al término del embarazo. x &lt;- c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46) y &lt;- c(1.15, 0.88, 0.90, 0.74, 1.21) perm &lt;- data.frame(muestra = rep(c(&quot;x&quot;, &quot;y&quot;), times = c(length(x), length(y))), permeabilidad = c(x, y)) Los datos son de Hollander &amp; Wolfe (1973), 69f. Aquí, los guardamos como x e y. Luego los reunimos en un data frame para poder gráficar con ggplot. Para esto queremos que quede en una columna los valores, que se puede realizar con función c() que concatena los valores que le indiquemos. Y además, tenemos que agregar una columna que indique si ese valor es de la muestra x o de y. Se podría usar c() y repetir tantas veces como sea necesario x e y manualmente. Pero es aburrido y seguramente nos equivocaremos. Y si para algo se hicieron las máquinas es para que nosotros no tengamos que hacer las tareas aburridas (o que al menos nos lleven menos tiempo). Para que lleve menos tiempo usamos la función rep()que repite el una secuencia la cantidad de veces que indiquemos (el argumento times). Para obtener la cantidad de veces que cada una debe repetirse usamos la función length() que nos devuelve la longitud cada uno de esos objetos. Siempre es recomendable graficar los datos antes de analizarlos. Nos va a revelar mucha información y posibles problemas solo con mirarlos. Hay que graficar los datos tal cual están, ya que si solo graficamos las medidas de resúmen típcas hay información que puede quedar escondida. ggplot(perm, aes(muestra, permeabilidad)) + geom_point() Aunque también podemos combinar este gráfico con un boxplot y obtener un poco más de información. ggplot(perm, aes(muestra, permeabilidad)) + geom_boxplot() + geom_point() Graficamente, parece ser que la permeabilidad es mayor a término que en menores semanas de gestación. Pero, ¿es significativa estadísticamente?. Para responder esta pregunta vamos a usar la prueba de Wilcoxon para dos muestras. La función wilcox.test tiene varios argumentos. Por un lado, debemos especificar los objetos que contienen los datos, x e y en nuestro caso. Y en luego, la hipótesis alternativa, la distribución de x es mayor (greater), menor (less) o distinta (mayor o menor, two.sided) que y. Es importante tener esto en claro. Siempre se trata del primero versus el segundo. wilcox.test(x, y, alternative = &quot;greater&quot;) ## ## Wilcoxon rank sum test ## ## data: x and y ## W = 35, p-value = 0.1272 ## alternative hypothesis: true location shift is greater than 0 Dado que \\(P[X&gt;x]=0.1272\\) no rechazamos la hipótesis nula. 10.3 Fórmulas Otra forma muy común de analizar datos es utilizando fórmulas. Estas no son fórmulas algebraicas como ya vimos en la parte de gráficos con ggplot2. Las formulas en R describen como se relacionan las variables. Por un lado, tenemos las variables dependientes y por otro las independientes y estás separadas por la virguilla (~). Por ejemplo: variable_dependiente ~ variable_independiente. Por ahora solo vamos a trabajar con una sola variable dependiente e independiente, pero es posible describir todo tipo de modelos mediante esta interfaz. El ejemplo anterior puede ser analizado usando la interfaz de fórmula: # Debemos usar el argumento data para que la función sepa donde estan los datos. # Prueben que sucede si no está. wilcox.test(formula = permeabilidad ~ muestra, alternative = &quot;greater&quot;, data = perm) ## ## Wilcoxon rank sum test ## ## data: permeabilidad by muestra ## W = 35, p-value = 0.1272 ## alternative hypothesis: true location shift is greater than 0 Presten atención a que hay que agregar el argumento data. Sin este argumento la función wilcox.test() no sabe donde buscar los datos. Como ver, los resultados son iguales. Pero hay que tener cuidados. El orden de los datos puede no ser el mismo que usaron en la versión sin fórmula. Esto se debe a que los niveles de los factores se asignan por orden alfabético si no los especificamos. Entonces, el orden puede ser distinto, y recordemos que la prueba especifica el primero vs el segundo. Por ejemplo, si vemos los niveles de muestra, no dice que tiene dos niveles, x y luego y. perm$muestra ## [1] x x x x x x x x x x y y y y y ## Levels: x y Veamos que pasa si cambiamos el orden de los niveles: perm &lt;- perm %&gt;% mutate(muestra = factor(x = muestra, levels = c(&quot;y&quot;, &quot;x&quot;) )) perm$muestra ## [1] x x x x x x x x x x y y y y y ## Levels: y x Ahora volvamos a hacer el análisis igual que antes: wilcox.test(formula = permeabilidad ~ muestra, alternative = &quot;greater&quot;, data = perm) ## ## Wilcoxon rank sum test ## ## data: permeabilidad by muestra ## W = 15, p-value = 0.8968 ## alternative hypothesis: true location shift is greater than 0 ¡Noten que los resultados son muy distintos! 10.4 Muestras apareadas En el caso de tener muestras apareadas, la función para realizar la prueba la prueba de Wilcoxon de rangos con signos es la misma que usamos antes wilcox.test(). Debemos indicar que se trata de muestras apareadas con el argumento paired = TRUE. Además, no acepta formulas. Ejemplo de la ayuda de wilcox.test(): ## One-sample test. ## Hollander &amp; Wolfe (1973), 29f. ## Hamilton depression scale factor measurements in 9 patients with ## mixed anxiety and depression, taken at the first (x) and second ## (y) visit after initiation of a therapy (administration of a ## tranquilizer). x &lt;- c(1.83, 0.50, 1.62, 2.48, 1.68, 1.88, 1.55, 3.06, 1.30) y &lt;- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29) wilcox.test(x, y, paired = TRUE, alternative = &quot;greater&quot;) ## ## Wilcoxon signed rank test ## ## data: x and y ## V = 40, p-value = 0.01953 ## alternative hypothesis: true location shift is greater than 0 tibble(x, y) %&gt;% gather(tiempo, hamilton) %&gt;% ggplot(aes(tiempo, hamilton)) + geom_boxplot() Como no tiene opción para usar formula, tampoco funciona el argumento data. Solo funciona si las columnas están directamente disponibles en el espacio de trabajo. Hacerlo implica de alguna forma desorganizar los datos, que ya no estarán juntos sino que estarán separados en el espacio de trabajo. Por ejemplo, si hay columnas antes y después dentro de data frames diferentes, también a hay que poner alguna indicación de donde vino esa columna. Hay una manera mejor de hacer esto. Usando el operador %$%exposition, del paquete magrittr. Funciona de manera similar %&gt;% pipe, pero expone los nombres de las columnas del lado izquierdo a la función de lado derecho. data.frame(z = rnorm(100)) %$% ts.plot(z) 10.5 Problemas Antes de comenzar bajen el archivo donde realizarán su informe reproducible. En la consola copien este código: download.file(&quot;http://bit.ly/informe-dos-muestras-NP&quot;, &quot;informe-dos-muestras-NP.Rmd&quot;) Pueden abrirlo desde la pestaña de archivos, a la derecha. Cambien el nombre por el suyo en el encabezado y mientras leen este capítulo respondan las preguntas. En todos los casos indicar la hipótesis nula y la alternativa. Graficar y realizar la prueba apropiada. Los datos de los problemas ya se encuentran guardados. Hay que cargarlos con: load(&quot;data/dos_muestras_np.RData&quot;) 1.- Un investigador, trabajando con una especie de ratones de campo, desea saber si los ejemplares provenientes del valle son de similar tamaño a los provenientes de la meseta. Para ello realizó capturas de ratones en ambos ambientes, midiendo el peso en gr de los ejemplares capturados. Los datos obtenidos fueron: ratones ## # A tibble: 35 x 3 ## Peso Ambiente row ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 38 Valle 1 ## 2 47 Valle 2 ## 3 50 Valle 3 ## 4 51 Valle 4 ## 5 39 Valle 5 ## 6 39 Valle 6 ## 7 44 Valle 7 ## 8 40 Valle 8 ## 9 46 Valle 9 ## 10 50 Valle 10 ## # ... with 25 more rows ¿Qué conclusión se puede sacar con estos datos? 2.- Una empresa pesquera desea evaluar si existen diferencias entre dos jefes de planta que trabajan en uno de sus buques factoría. De acuerdo a lo expresado por el capitán del buque, el Jefe 1 aprovecha mejor la captura que el Jefe 2. Para estudiar esta cuestión embarcaron a ambos jefes de planta en un mismo viaje de pesca y les asignaron aleatoriamente a cada uno de ellos los lances que debían procesar. En cada lance, un empleado imparcial de control de calidad registraba el porcentaje de descarte producido a partir de la captura. Los resultados obtenidos fueron: jefes ## # A tibble: 35 x 3 ## Descarte Jefe row ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 17 1 1 ## 2 10 1 2 ## 3 15 1 3 ## 4 22 1 4 ## 5 15 1 5 ## 6 9 1 6 ## 7 20 1 7 ## 8 14 1 8 ## 9 21 1 9 ## 10 19 1 10 ## # ... with 25 more rows ¿Tiene razón el capitán? Definición 10.1 Más adelante no podrán usar la formulas para hacer las pruebas apareadas. Para hacer comparaciones de datos apareados cada unidad muestral debe estar en una fila, con dos columnas: una para antes y otra para el después. Sin embargo, esta forma no permite utilizar el argumento data para indicar donde se encuentran los datos. Por eso, hay que hacer que estén disponibles para la función. Una forma es usar el operador de exposición %$%; expone los nombres del objeto que está a la izquierda del mismo: datos %$% wilcox.test(x = x, y = y, paired = TRUE) Además, vemos que debemos agregar el argumento paired = TRUE para indicar que los datos son apareados. 3.- En un estudio clínico, se desea evaluar si una cierta droga disminuye o no la concentración de un virus en sangre. Para ello se utilizaron 17 cobayos infectados, registrándose previamente al inicio de la experiencia la concentración del virus en sangre. Luego de finalizado el tratamiento con la droga, se volvió a estudiar la concentración del virus en los cobayos. Los resultados obtenidos fueron: cobayos ## # A tibble: 17 x 3 ## Ejemplar Antes Despues ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 6 ## 2 2 16 12 ## 3 3 18 14 ## 4 4 9 7 ## 5 5 18 18 ## 6 6 11 11 ## 7 7 3 2 ## 8 8 16 14 ## 9 9 13 10 ## 10 10 9 6 ## 11 11 6 1 ## 12 12 13 12 ## 13 13 19 19 ## 14 14 4 4 ## 15 15 5 7 ## 16 16 17 15 ## 17 17 15 14 ¿Qué conclusiones puede Ud. sacar acerca de la efectividad del tratamiento? 4 - Un ecólogo desea evaluar en una especie de foca si el éxito reproductivo de las hembras está asociado al sexo de sus crías. Para ello utilizó información de una población que ha sido seguida durante varias generaciones, registrando para 15 hembras el número de nietos producidos por sus hijos e hijas. Los datos fueron: focas ## # A tibble: 15 x 3 ## Ejemplar Hembra Macho ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 128 120 ## 2 2 95 111 ## 3 3 104 119 ## 4 4 99 111 ## 5 5 111 120 ## 6 6 93 109 ## 7 7 132 108 ## 8 8 129 130 ## 9 9 127 130 ## 10 10 100 119 ## 11 11 122 105 ## 12 12 124 132 ## 13 13 94 127 ## 14 14 96 126 ## 15 15 127 121 ¿Depende el éxito reproductivo de las hembras del sexo de sus hijos? 5.- Una especie de ave pone dos huevos por temporada reproductiva. Se ha visto que de los dos pichones el primero en eclosionar tiene mayores probabilidades de sobrevivir. Un biólogo desea establecer si esta situación está relacionada con el peso del pichón al momento de la eclosión. Para ello registró el peso de los pichones al momento de la eclosión del huevo, obteniendo los siguientes datos: pichones ## # A tibble: 17 x 3 ## Nido Primer Segundo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 102 110 ## 2 2 86 95 ## 3 3 112 117 ## 4 4 85 119 ## 5 5 91 117 ## 6 6 101 94 ## 7 7 102 102 ## 8 8 111 96 ## 9 9 116 103 ## 10 10 114 120 ## 11 11 83 102 ## 12 12 85 98 ## 13 13 105 118 ## 14 14 95 106 ## 15 15 107 94 ## 16 16 94 108 ## 17 17 99 102 ¿Cuál es la conclusión que debería sacar el biólogo? 6.- Un biólogo está estudiando el efecto del aprendizaje en la habilidad de los osos para capturar peces. Para ello registra el porcentaje de éxitos de captura durante una semana de 17 ositos cuando comienzan a pescar y repite el análisis 6 meses después. Los resultados obtenidos fueron: osos ## # A tibble: 17 x 3 ## Oso Tiempo0 Tiempo6 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 42 75 ## 2 2 47 61 ## 3 3 58 63 ## 4 4 40 74 ## 5 5 56 65 ## 6 6 60 54 ## 7 7 45 58 ## 8 8 72 53 ## 9 9 63 66 ## 10 10 79 77 ## 11 11 43 65 ## 12 12 45 52 ## 13 13 79 64 ## 14 14 45 65 ## 15 15 42 62 ## 16 16 49 54 ## 17 17 53 68 ¿Mejora la capacidad de captura de los ositos con la experiencia? 7.- Los pingüinos de Magallanes hacen sus nidos en cuevas en las laderas o bajo de los arbustos. Un biólogo sostiene que el éxito reproductivo de las hembras que nidifican en las laderas es mayor que el de aquellas que lo hacen bajo los arbustos. Para poner a prueba esta hipótesis utilizó datos del número de pichones vivos que tuvieron durante su vida hembras que nidificaron en laderas y en arbustos. Las hembras pudieron identificarse debido a que fueron anilladas de pichones y no se registraron cambios en el tipo de nido que utilizaron a lo largo de la vida. Los datos obtenidos fueron: Número de pichones producidos por hembras de pingüino de Magallanes a lo largo de su vida, discriminado por el tipo de nido que utilizaron. pinguinos ## # A tibble: 32 x 3 ## Pichones Nido row ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 10 Arbusto 1 ## 2 13 Arbusto 2 ## 3 7 Arbusto 3 ## 4 7 Arbusto 4 ## 5 11 Arbusto 5 ## 6 11 Arbusto 6 ## 7 8 Arbusto 7 ## 8 11 Arbusto 8 ## 9 8 Arbusto 9 ## 10 12 Arbusto 10 ## # ... with 22 more rows ¿Está Ud. de acuerdo con el biólogo? 8.- Estudiando la dieta de un delfín y del lobo marino, un biólogo desea establecer si las tallas consumidas de calamares por estos predadores son similares. Utilizando regresiones alométricas estimó los largos dorsales del manto (LDM) a partir de los picos hallados en los contenidos estomacales. Los datos obtenidos fueron: Tallas de calamares (LDM, cm) consumidos por delfines y lobos marinos. LDM ## # A tibble: 37 x 3 ## LDM Especie row ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 26 Delfín 1 ## 2 21 Delfín 2 ## 3 24.6 Delfín 3 ## 4 20.9 Delfín 4 ## 5 26.4 Delfín 5 ## 6 23.9 Delfín 6 ## 7 25.6 Delfín 7 ## 8 24.2 Delfín 8 ## 9 20.4 Delfín 9 ## 10 23.3 Delfín 10 ## # ... with 27 more rows ¿Qué puede concluir sobre las tallas de los calamares consumidos por los delfines y lobos marinos? 9.- Para determinar si una droga es eficaz para disminuir la concentración de un virus en sangre, se seleccionaron al azar ratones infectados y se les inyectó la droga a evaluar. Otro grupo de ratones infectados fue utilizado como control empleándose un placebo en lugar de droga. Luego del experimento se midió la concentración del virus en sangre utilizando una escala apropiada. Los resultados fueron: Concentración del virus en sangre de los ratones tratados y del grupo control droga ## # A tibble: 24 x 3 ## Virus Tratamiento row ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 34.4 Control 1 ## 2 39.8 Control 2 ## 3 26.6 Control 3 ## 4 33.3 Control 4 ## 5 37.3 Control 5 ## 6 30.3 Control 6 ## 7 16.2 Control 7 ## 8 21.1 Control 8 ## 9 43.4 Control 9 ## 10 26.0 Control 10 ## # ... with 14 more rows ¿Es eficaz la droga para disminuir la concentración del virus en sangre? 10.- Se desea establecer si las poblaciones bonaerense y patagónica de anchoita presentan similares niveles de parasitosis por nematodes en músculo. Para ello se tomaron muestras aleatorias de anchoitas de ambas poblaciones y se determinó para cada ejemplar el número de larvas de nematodes alojadas en el músculo. Los resultados fueron: Número de larvas de nematodes en el músculo de anchoitas discriminadas por poblaciones. nematodes ## # A tibble: 41 x 3 ## Larvas Anchoita row ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 14 Patagónica 1 ## 2 36 Patagónica 2 ## 3 26 Patagónica 3 ## 4 23 Patagónica 4 ## 5 14 Patagónica 5 ## 6 36 Patagónica 6 ## 7 26 Patagónica 7 ## 8 23 Patagónica 8 ## 9 14 Patagónica 9 ## 10 26 Patagónica 10 ## # ... with 31 more rows ¿Existen diferencias entre poblaciones de anchoita con respecto a la parasitosis por nematodes en músculo? 11.- Un ecólogo desea determinar si la eficiencia de captura de dos especies de araña es similar. Para ello realizó un experimento donde seleccionó al azar ejemplares de cada especie, les permitió confeccionar sus telas y luego introdujo una mosca en cada caja. El ecólogo determinó para cada araña el tiempo en segundos que tardó en capturar la mosca. Los resultados fueron: Tiempo de captura de la mosca en segundos. arana ## # A tibble: 36 x 3 ## Tiempo Especie row ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 159 1 1 ## 2 143 1 2 ## 3 90 1 3 ## 4 130 1 4 ## 5 148 1 5 ## 6 150 1 6 ## 7 161 1 7 ## 8 166 1 8 ## 9 164 1 9 ## 10 87 1 10 ## # ... with 26 more rows ¿Existen diferencias en las eficiencias de captura entre las especies de araña? Un productor de fruta fina está convencido que la producción en el Bolsón es más alta que en Esquel. Para ello tomó datos producción de distintas parcelas en ambas localidades. Sabiendo que el azar tenía algo que ver con la estadística, se preocupó de seleccionar al azar las parcelas. Los datos obtenidos fueron: Producción de fruta fina (kg) en cada parcela, discriminada por localidad fruta ## # A tibble: 35 x 3 ## Producción Localidad row ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 56.6 Esquel 1 ## 2 90.7 Esquel 2 ## 3 29.7 Esquel 3 ## 4 30.0 Esquel 4 ## 5 61.4 Esquel 5 ## 6 46.0 Esquel 6 ## 7 61.3 Esquel 7 ## 8 59.0 Esquel 8 ## 9 61.4 Esquel 9 ## 10 52.1 Esquel 10 ## # ... with 25 more rows Una vez tomados los datos, el productor acude a Ud. en busca de ayuda para responder su pregunta. ¿Qué le dirá al productor? "],
["anova-no-parametrico.html", "Capítulo 11 ANOVA No Paramétrico 11.1 Pruebas para varias muestras independientes 11.2 Prueba de Kruskal-Wallis", " Capítulo 11 ANOVA No Paramétrico 11.1 Pruebas para varias muestras independientes Las pruebas no paramétricas para varias muestras independientes son, conceptualmente, una extensión de las pruebas para dos muestras. Este tipo de pruebas tienen como análogo paramétrico al ANOVA de una vía. Esencialmente, este tipo de pruebas comparan k muestras y pretenden determinar si las k muestras son similares entre sí. 11.1.1 Prueba de la mediana 11.1.1.1 Datos Para utilizar esta prueba es necesario contar con \\(k\\) muestras aleatorias de las k poblaciones a comparar. Cada muestra tiene un tamaño \\(n_{i}\\) de tal forma que el tamaño muestral total (\\(N\\)) puede obtenerse como: \\[ N = n_{1} + n_{2} + \\ldots + n_{k} \\] 11.1.1.2 Supuestos Cada una de las k muestras es una muestra aleatoria de la población respectiva. Las k muestras son independientes entre sí. Las mediciones están realizadas en, al menos, una escala ordinal. Si todas las poblaciones a comparar tienen idéntica mediana, la probabilidad p de que una observación cualquiera exceda el valor de la mediana es la misma para todas las poblaciones. Nótese que este supuesto no implica que las funciones de distribución de cada una de las poblaciones deban ser las mismas, ni que estas funciones sean simétricas con respecto a la mediana. 11.1.1.3 Procedimiento Se toman los \\(N\\) datos y se calcula la mediana general (Gran Mediana, \\(M\\)). Se clasifican los datos de cada muestra teniendo en cuenta si son mayores que la gran mediana o menores o iguales que este parámetro (\\(X_{\\text{ij}} &gt; M\\) ó \\(X_{\\text{ij}} \\leq M\\)). Utilizando esta clasificación, se construye una tabla de contingencia de 2xk de la forma: Población Marginal 1 2 \\(\\ldots\\) K \\(X\\text{ij} &gt; M\\) \\(O_{11}\\) \\(O_{12}\\) \\(\\ldots\\) \\(O_{1k}\\) a \\(X\\text{ij} \\leq M\\) \\(O_{21}\\) \\(O_{22}\\) \\(\\ldots\\) \\(O_{2k}\\) b Marginal (\\(n_i\\)) \\(n_{1}\\) \\(n_{2}\\) \\(\\ldots\\) \\(n_{k}\\) N Se evalúa la hipótesis utilizando los estadísticos adecuados para las tablas de contingencia (\\(\\chi^{2}\\) o \\(G\\)) y teniendo en cuenta las restricciones aplicables a este tipo de pruebas. Usualmente, cuando se realiza la prueba de la mediana se tiende a emplear el estadístico \\(\\chi^{2}\\). 11.1.1.4 Hipótesis \\[ \\begin{aligned} H_{0}&amp;: \\text{las }k\\text{ poblaciones tienen idéntica mediana.}\\\\ H_{a}&amp;: \\text{alguna de las } k\\text{ poblaciones presenta una mediana diferente.} \\end{aligned} \\] Los criterios de decisión para esta prueba son los correspondientes a las tablas de contingencia. En este caso, dado que el número de filas es siempre igual a 2, los grados de libertad para el \\(\\chi^{2}\\) son \\(k - 1\\). El estadístico obtenido debe ser comparado con un \\(\\chi_{\\alpha;k - 1}^{2}\\). Los criterios de decisión son: Si \\(\\chi^{2} &gt; \\chi_{\\alpha;k - 1}^{2}\\) entonces Rechazo \\(H_{0}\\) Si \\(\\chi^{2} \\leq \\chi_{\\alpha;k - 1}^{2}\\) entonces No rechazo \\(H_{0}\\) En forma equivalente, se puede emplear el Valor \\(p\\) para tomar la decisión. El Valor \\(p\\) se calcula como: \\[ P\\left( \\chi_{k - 1}^{2} \\geq \\chi^{2} \\right) = p \\] Esta probabilidad puede obtenerse en R utilizando la función pchisq(). De esta forma, el criterio de decisión empleando el Valor p queda definido como: Si \\(p \\leq \\alpha\\) entonces Rechazo \\(H_{0}\\) Si \\(p &gt; \\alpha\\) entonces No rechazo \\(H_{0}\\) 11.1.1.5 Contrastes Si la prueba resultó significativa, entonces es de utilidad determinar cuáles son las poblaciones que tienen medianas diferentes. Se utiliza la misma prueba de la mediana, pero considerando \\(k = 2\\). Así, se puede construir una tabla de 2x2 y comparar a las poblaciones de a pares. Cabe aclarar que para cada uno de estos contrastes de a pares debe calcularse la gran mediana. Aunque este método permite realizar contrastes, algunas consideraciones deben ser realizadas. En primer lugar, al definir una tabla de 2x2, deberán realizarse las correcciones de continuidad pertinentes ya que la tabla tiene un grado de libertad. 11.1.1.6 Comentarios La prueba de la mediana para dos poblaciones independientes, cuando se la compara con la prueba de \\(t\\) y se cumplen los supuestos de esta última prueba, tiene una eficiencia relativa del 95% para valores pequeños de \\(N\\) (≈6-10) y disminuye a medida que \\(N\\) se incrementa hasta alcanzar una eficiencia relativa asintótica del 63%. En el caso de la extensión de la prueba para \\(k\\) poblaciones, cuando se cumplen los supuestos del ANOVA de un factor, la eficiencia relativa asintótica es del 64%. Sin embargo, cuando ambas pruebas se comparan utilizando poblaciones no-normales, por ejemplo, la distribución es exponencial, la eficiencia relativa asintótica de la prueba de la mediana con respecto al ANOVA es del 200%. Ejemplo 1: Prueba de la mediana. Se desea evaluar la eficiencia de 4 tratamientos de combinaciones de fertilizantes en la producción de maíz. Para ello se seleccionaron parcelas de similares características y se le asignó al azar a cada una de ellas un tipo de fertilizante. Luego de la cosecha se determinó la producción de cada parcela en kg de mazorcas. Los resultados fueron: Tabla 11.1: Producción de maíz (kg) en las parcelas tratadas con 4 tratamientos de fertilizantes diferentes. Tratamiento 1 Tratamiento 2 Tratamiento 3 Tratamiento 4 83 91 101 78 91 90 100 82 94 81 91 81 89 83 93 77 89 84 96 79 96 83 95 81 91 88 94 80 92 91 81 90 89 84 \\(H_{0}:\\) La mediana de la producción es la misma con los 4 tratamientos. \\(H_{a}:\\)La mediana de la producción con alguno de los tratamientos es diferente Los tamaños muestrales (\\(n_{i}\\)) de cada tratamiento fueron: \\[ \\text{Tratamiento}\\ 1:\\ 9\\\\ \\text{Tratamiento}\\ 2:\\ 10\\\\ \\text{Tratamiento}\\ 3:\\ 7\\\\ \\text{Tratamiento}\\ 4:\\ 8 \\] La gran mediana (\\(M\\)) para los 34 datos obtenidos fue: \\[ M = 89 \\] La tabla de datos observados y esperados construida a partir de los datos y considerando el valor de \\(M\\) fue: Observado Trat 1 Trat 2 Trat 3 Trat 4 Marginal &gt; Mediana 6 3 7 0 16 &lt;=Mediana 3 7 0 8 18 Marginal 9 10 7 8 34 Esperado Trat 1 Trat 2 Trat 3 Trat 4 &gt; Mediana 4,235 4,706 3,294 3,765 &lt;=Mediana 4,765 5,294 3,706 4,235 Utilizando un estadístico \\(\\chi^{2}\\) de la forma: \\[ \\chi^{2} = \\sum_{i = 1}^{k}{\\sum_{j = 1}^{2}\\frac{\\left( O_{\\text{ij}} - E_{\\text{ij}} \\right)^{2}}{E_{\\text{ij}}}} \\] Se obtiene: \\(\\chi^{2}\\) 17,543 gl 3 Valor p 0,00054637 Por lo tanto, rechazo la \\(H_{0}\\). Existe algún tratamiento cuya mediana es diferente de las demás. Se puede utilizar la función Median.test() en R. La tabla de resultados obtenida es: trt &lt;- c(rep(1, 9), rep(2, 10), rep(3, 7), rep(4, 8)) y &lt;- c(83, 91, 94, 89, 89, 96, 91, 92, 90, 91, 90, 81, 83, 84, 83, 88, 91, 89, 84, 101, 100, 91, 93, 96, 95, 94, 78, 82, 81, 77, 79, 81, 80, 81) Median.test(y, trt, group = FALSE) ## ## The Median Test for y ~ trt ## ## Chi Square = 17.54306 DF = 3 P.Value 0.00054637 ## Median = 89 ## ## Median r Min Max Q25 Q75 ## 1 91.0 9 83 96 89.00 92.00 ## 2 86.0 10 81 91 83.25 89.75 ## 3 95.0 7 91 101 93.50 98.00 ## 4 80.5 8 77 82 78.75 81.00 ## ## Post Hoc Analysis ## ## median chisq pvalue signif. ## 1 and 2 89.0 2.554444 0.1100 ## 1 and 3 92.5 6.349206 0.0117 * ## 1 and 4 83.0 13.432099 0.0002 *** ## 2 and 3 91.0 13.246753 0.0003 *** ## 2 and 4 82.5 14.400000 0.0001 *** ## 3 and 4 82.0 15.000000 0.0001 *** Debido a que se rechazó \\(H_{0}\\), es necesario realizar contrastes para detectar la ubicación de las diferencias. Los contrastes se realizan automáticamente en R. Los resultados obtenidos se muestras arriba bajo el título “Post Hoc Analysis” En función de estos resultados, es posible concluir que todos los tratamientos difieren entre sí, exceptuando a los tratamientos 1 y 2, los cuáles no presentan diferencias significativas. Estas diferencias pueden apreciarse en la siguiente figura: (ref:cajas-medianas) Gráfico de cajas y bigotes para la producción de maíz en cuatro tratamientos. Figura 11.1: (ref:cajas-medianas) La conclusión final de este análisis sería recomendar al tratamiento 3 como el más adecuado para incrementar la producción de maíz. 11.2 Prueba de Kruskal-Wallis La prueba de Kruskal-Wallis es la extensión para k muestras de la prueba de Mann-Whitney. A diferencia de la prueba de la mediana, esta prueba evalúa las funciones de distribución de las poblaciones, aunque es sensible fundamentalmente a diferencias en la tendencia central. 11.2.1 Datos Los datos necesarios para utilizar esta prueba son \\(k\\) muestras aleatorias de las poblaciones a comparar. Cada muestra tiene un tamaño \\(n_{i}\\), obteniéndose el tamaño muestral total (\\(N\\)) como \\(N = \\Sigma n_{i}\\). 11.2.2 Supuestos Todas las muestras son muestras aleatorias de las respectivas poblaciones. Los datos dentro de cada muestra son independientes y las muestras son independientes entre sí. La variable a estudiar está medida, al menos, en una escala ordinal. Las k poblaciones tiene idéntica función de distribución o, de lo contrario, una de ellas tiende a presentar valores mayores que las demás. 11.2.3 Procedimiento Se ordenan y ranquean los N datos de menor a mayor, calculándose cuando es necesario los rangos por empate. De esta forma, cada observación tiene un rango asignado [\\(R(X_{\\text{ij}})\\)], pudiéndose calcular las sumas de los rangos de cada población como: \\[ R_{i} = \\sum_{j = i}^{n_{i}}{R(X_{ij})} \\] Se calcula un término de varianza (\\(S^{2}\\)) como: \\[ S^{2} = \\frac{1}{N - 1}\\left( \\sum_{i = 1}^{k}{\\sum_{j = 1}^{n_{i}}{R\\left( X_{\\text{ij}} \\right)^{2}}} - \\frac{N\\left( N + 1 \\right)^{2}}{4} \\right) \\] Se calcula el estadístico de prueba (\\(H\\)) como: \\[ h = \\frac{1}{S^{2}}\\left( \\sum_{i = 1}^{k}\\frac{R_{i}^{2}}{n_{i}} - N\\frac{\\left( N + 1 \\right)^{2}}{4} \\right) \\] Este estadístico ya incorpora la corrección por empates y se distribuye aproximadamente según una \\(\\chi^{2}\\) con \\(k - 1\\) grados de libertad. 11.2.4 Hipótesis \\(H_{0}\\): Las funciones de distribución de las \\(k\\) poblaciones son idénticas. \\(H_{a}\\): Al menos una de las poblaciones tiende a presentar valores mayores que (al menos) otra de las poblaciones. Esta hipótesis alternativa suele plantearse también como: Las \\(k\\) poblaciones no tienen idéntica media. Esto se debe a que la prueba de Kruskal-Wallis ha sido desarrollada para ser más sensible a diferencias en los parámetros de tendencia central que a otros parámetros de la distribución. Los criterios de decisión para esta prueba son: Si \\(H &gt; \\chi_{\\alpha,\\ k - 1}^{2}\\) entonces Rechazo \\(H_{0}\\) Si \\(H \\leq \\chi_{\\alpha,\\ k - 1}^{2}\\) entonces No rechazo \\(H_{0}\\) En forma equivalente, se puede emplear el Valor \\(p\\) para tomar la decisión. El Valor \\(p\\) se calcula como: \\[ P\\left( \\chi_{k - 1}^{2} \\geq H \\right) = p \\] De esta forma, el criterio de decisión empleando el Valor p queda definido como: Si \\(p \\leq \\alpha\\) entonces Rechazo \\(H_{0}\\) Si \\(p &gt; \\alpha\\) entonces No rechazo \\(H_{0}\\) 11.2.5 Contrastes Si la prueba resultó significativa, pueden utilizarse varios métodos de contrastes de a pares para poder detectar la ubicación de las diferencias. Nosotros utilizaremos dos. Los denominaremos como Contraste t y Contraste z en función de los valores críticos que utilizan. Ambos contrastes utilizan el mismo estadístico, difiriendo entre sí en el valor crítico que emplean. El estadístico se obtiene en ambos casos como: \\[ \\varepsilon_{\\text{ij}} = \\left| \\frac{R_{i}}{n_{i}} - \\frac{R_{j}}{n_{j}} \\right| \\] Los valores críticos pueden obtenerse como: Contraste t \\[ VC_{\\text{ij}} = t_{N - k;\\alpha/2}\\sqrt{S^{2}\\frac{\\left( N - 1 - H \\right)}{N - k}}\\sqrt{\\frac{1}{n_{i}} + \\frac{1}{n_{j}}} \\] Contraste z \\[ VC_{\\text{ij}} = Z_{\\frac{\\alpha}{\\lbrack k\\left( k - 1 \\right)\\rbrack}}\\sqrt{\\frac{N\\left( N + 1 \\right)}{12}}\\sqrt{\\frac{1}{n_{i}} + \\frac{1}{n_{j}}} \\] Para ambos contrastes, la regla de decisión es: Si \\(\\varepsilon_{\\text{ij}} &gt; VC_{\\text{ij}}\\) entonces Rechazo \\(H_{0}\\) Si \\(\\varepsilon_{\\text{ij}} \\leq VC_{\\text{ij}}\\) entonces No rechazo \\(H_{0}\\) Entre ambos métodos, el Contraste t es más eficiente, siendo el Contraste z mucho más conservativo. La ventaja de este último contraste es su mayor simplicidad de cálculo, ya que no requiere de S2 para la obtención del valor crítico. Asimismo, el Contraste z puede ser útil cuando sólo se desean detectar diferencias altamente significativas. 11.2.6 Comentarios En comparación con el ANOVA de un factor, la eficiencia relativa asintótica (ARE) de la prueba de Kruskal-Wallis nunca es menor al 86.4%. Cuando las poblaciones a comparar son normales, de la prueba de Kruskal-Wallis con respecto al ANOVA es del 95%. Si las distribuciones son uniformes, asciende al 100%, mientras que, si la distribución es exponencial, es del 150%. Comparando la prueba de Kruskal-Wallis con la prueba de la mediana, es del 150%, 300% y 75% si las distribuciones son normales, uniformes y exponenciales respectivamente. Ejemplo 1: Prueba de Kruskal-Wallis. Con el objetivo de comparar ambas pruebas, utilizaremos el ejemplo 1 sobre los tratamientos con fertilizantes en cultivos de maíz, para ejemplificar el uso de la prueba de Kruskal-Wallis. Luego de ordenar y ranquear, los cálculos parciales y finales obtenidos fueron: Grupo \\(n_{i}\\) \\(R_{i}\\) \\(R_{i}/n_{i}\\) \\(R_{i}^{2}/n_{i}\\) Trat 1 9 196,50 21,833 4290,250 Trat 2 10 153,00 15,300 2340,900 Trat 3 7 207,00 29,571 6121,286 Trat 4 8 38,50 4,813 185,281 \\(N\\) 34 \\(H\\) 25,6288 \\(k\\) 4 \\(gl\\) 3 \\(\\sum\\left\\lbrack R\\left( X_{\\text{ij}} \\right)^{2} \\right\\rbrack\\) 13664 Valor \\(p\\) 1,14057E-05 \\(S^{2}\\) 98,5303 Esta prueba puede realizarse utilizando la función krukal.test() del paquete stats de R o kruskal() del paquete agricolae. La tabla de resultados obtenida es: ## ## Kruskal-Wallis rank sum test ## ## data: observation by method ## Kruskal-Wallis chi-squared = 25.629, df = 3, p-value = 1.141e-05 En función de los resultados obtenidos se puede rechazar la \\(H_{0}\\) y concluir que existen diferencias en la producción de maíz entre los tratamientos aplicados. Para detectar la ubicación de estas diferencias se emplearon ambos métodos de contrastes. Los resultados obtenidos fueron: Contraste t Se presentan los \\(R_{i}/n_{i}\\); los\\(\\ n_{i}\\) y la identificación del tratamiento, tanto en los encabezamientos de filas como de columnas. En el vértice superior izquierdo de la tabla se indican el valor de \\(t\\) y sus grados de libertad, así como el resultado obtenido de la primera raíz en la ecuación del \\(VC_{\\text{ij}}\\) del contraste (Factor). El \\(t_{\\alpha/2\\ }\\) y la primera raíz son los únicos factores en la ecuación del \\(VC_{\\text{ij}}\\)comunes a todas las comparaciones. En la tabla de resultados se presentan por debajo de la diagonal principal los valores de \\(\\varepsilon_{\\text{ij}}\\) y por encima de ésta los \\(VC_{\\text{ij}}\\), indicándose con itálicas los \\(\\varepsilon_{\\text{ij}}\\) que resultaron significativos. \\(t_{\\alpha/2\\ }\\) 2,042 21,833 15,300 29,571 4,813 \\(\\text{gl}\\) 30 9 10 7 8 Factor 4,920 Trat 1 Trat 2 Trat 3 Trat 4 21,833 9 Trat 1 4,617 5,064 4,883 15,300 10 Trat 2 6,533 4,952 4,766 29,571 7 Trat 3 7,738 14,271 5,201 4,813 8 Trat 4 17,021 10,488 24,759 Contraste z Se presentan los \\(R_{i}/n_{i}\\); los\\(\\ n_{i}\\) y la identificación del tratamiento, tanto en los encabezamientos de filas como de columnas. En el vértice superior izquierdo de la tabla se indican el valor del \\(Z\\) y de \\(1 - \\ \\alpha\\ /\\ \\lbrack k(k - 1)\\rbrack\\), así como el resultado obtenido de la primera raíz en la ecuación del \\(VC_{\\text{ij}}\\) del contraste (Factor). El \\(Z_{\\frac{\\alpha}{\\lbrack k\\left( k - 1 \\right)\\rbrack}}\\) y la primera raíz son los únicos factores en la ecuación del \\(VC_{ij}\\) comunes a todas las comparaciones. En la tabla de resultados se presentan por debajo de la diagonal principal los valores de \\(\\varepsilon_{\\text{ij}}\\) y por encima de ésta los \\(VC_{\\text{ij}}\\), indicándose con itálicas los \\(\\varepsilon_{\\text{ij}}\\) que resultaron significativos. \\(Z_{\\frac{\\alpha}{\\lbrack k\\left( k - 1 \\right)\\rbrack}}\\) 2,6383 21,833 15,300 29,571 4,813 \\(1-\\alpha/[k(k-1)]\\) 0,9958 9 10 7 8 Factor 9,9582 Trat 1 Trat 2 Trat 3 Trat 4 21,833 9 Trat 1 11,749 12,947 12,462 15,300 10 Trat 2 6,533 14,043 13,597 29,571 7 Trat 3 7,738 14,271 13,136 4,813 8 Trat 4 17,021 10,488 24,759 Como puede observarse, el Contraste t indica que todos los tratamientos difieren entre sí. Por su parte, el Contraste z no detecta diferencias significativas en las comparaciones 1 vs 2, 1 vs 3 y 2 vs 4. El Contraste z sólo detecta las diferencias más evidentes. Si se comparan los resultados con la prueba de la mediana, vemos que ambas pruebas detectan claramente las diferencias. Sin embargo, en términos de comparaciones, el Contraste t detectó todas las diferencias como significativas, en un punto intermedio quedó la prueba de la mediana donde la comparación 1 vs 2 no se detectó como significativa y en el otro extremo aparece el Contraste z que detecta como significativas sólo tres de las seis comparaciones realizadas. Con R: Contraste \\(t\\) (posthoc.kruskal.conover.test()) ## Warning in posthoc.kruskal.conover.test.default(x = observation, g = ## method, : Ties are present. Quantiles were corrected for ties. ## ## Pairwise comparisons using Conover&#39;s-test for multiple ## comparisons of independent samples ## ## data: observation and method ## ## 1 2 3 ## 2 0.0071 - - ## 3 0.0040 1.9e-06 - ## 4 6.4e-08 9.7e-05 8.8e-11 ## ## P value adjustment method: none Contraste z (posthoc.kruskal.dunn.test()) ## Warning in posthoc.kruskal.dunn.test.default(x = observation, g = method, : ## Ties are present. z-quantiles were corrected for ties. ## ## Pairwise comparisons using Dunn&#39;s-test for multiple ## comparisons of independent samples ## ## data: observation and method ## ## 1 2 3 ## 2 0.15200 - - ## 3 0.12189 0.00353 - ## 4 0.00042 0.02592 1.4e-06 ## ## P value adjustment method: none "],
["problemas-de-anova-no-parametrico.html", "Capítulo 12 Problemas de ANOVA No Paramétrico 12.1 Problemas", " Capítulo 12 Problemas de ANOVA No Paramétrico Las dos pruebas de ANOVA No Paramétrico están implentandas en R bajo las funciones Median.test() del paquete agricolae y la funcion kruskal.test() en el paquete stats de R o la función krukal() de agricolae. La función Median.test() tiene varios argumentos, el primero y = corresponde a la variable de respuesta, el segundo trt = es la columna que contiene la identificación del tratamiento. Esos dos son los únicos argumentos obligatorios. Otro argumento interesante es groups que puede ser verdadero o falso y agrupa los tratamientos según si hay diferencias significativas entre sí. La función krukal() tiene los mismos argumentos principales. Ambos requieren acceso directo a las columnas, de igual forma que la función wilcox.test() para muestras apareadas por eso es necesario usar el operador de exposición %$%. Por otro lado, la función kruskal.test() funciona de forma similar a wilcox.test(). Tiene una interfaz que es igual a la anterior, con el primer argumento siendo la variable de respuesta y el segundo la variable de tratamientos. Esta forma necesita ser expuestas la variables del objeto directamente con el operador de exposición. Además, tiene la forma de ingresar como fórmula y especificar el objeto donde están lso datos. 12.1 Problemas download.file(&quot;http://bit.ly/informe-anova-no-parametrico&quot;, &quot;informe-anova-no-parametrico.Rmd&quot;) Un ecólogo ha estudiado la estructura social de los lobos marinos durante su permanencia en los apostaderos y ha sugerido que los juveniles de 1-2 años permanecen asociados a grupos de hembras, presumiblemente donde se encuentran sus madres. Para intentar establecer si la información disponible permite respaldar la existencia de esta asociación durante los viajes de alimentación, el ecólogo utiliza información previamente colectada mediante registradores satelitales sobre la distancia media recorrida por viaje de alimentación. Esta información se encuentra discriminada por sexo y categoría de edad. En este caso sólo utilizó la información de los machos y hembras adultos y de juveniles de 1-2 años de ambos sexos. Estos datos fueron: Tabla 12.1: Distancia media (km) recorrida por viaje de alimentación de lobos marinos machos y hembras, juveniles y adultos. ¿Qué concluyó el ecólogo a partir de estos datos? Hembras.Juveniles Hembras.Adultas Macho.Juveniles Macho.Adultos 124 114 109 123 85 121 83 177 71 95 90 179 83 116 85 119 105 97 91 157 78 73 134 151 57 95 133 164 90 128 75 176 112 103 90 145 82 134 120 179 109 74 85 154 64 74 158 68 131 119 112 85 143 125 115 178 89 68 185 57 85 141 99 172 85 115 69 126 122 126 113 155 En un estudio de ecología trófica se han detectado diferencias en la dieta entre machos y hembras del lobo marino. Estas diferencias parecerían deberse a que las hembras se alimentarían en aguas más someras y cercanas a la costa que los machos. Sabiendo que los cormoranes son predadores exclusivamente costeros, que los tiburones son predadores típicos de la plataforma intermedia, y que las áreas costeras y someras se caracterizan por ser zonas de cría para muchas especies de peces, un biólogo comparó las tallas de las raneyas (pez demersal- bentónico) consumidas por cormoranes, tiburones y machos y hembras del lobo marino. La intención de esta comparación fue evaluar si la hipótesis planteada sobre las diferentes áreas de forrajeo de machos y hembras del lobo marino se ve sustentada o no. Tabla 12.2: Largo total (cm) de raneyas consumidas por machos y hembras del lobo marino, por tiburones y por cormoranes. Lobo.Hem Lobo.Mac Tiburon Cormoran 12 26 13 12 14 23 17 14 13 21 25 8 20 26 22 8 15 17 19 16 16 17 13 9 15 17 22 10 15 15 26 16 13 15 21 10 13 21 26 9 18 27 14 9 13 15 9 20 16 12 18 14 15 16 17 10 20 19 ¿Cuál es su conclusión? La dirección nacional de pesca desea determinar si los tres tipos de redes que se utilizan para la pesca de langostino son igualmente eficaces. Para ello calcula (para cada barco) el promedio anual de sus capturas (kg de langostino/ lance) y clasifica a los barcos en función del tipo de red empleada. Los resultados obtenidos fueron: Tabla 12.3: Promedio anual de capturas de langostino por barco según el tipo de red empleada (kg/lance) Red.1 Red.2 Red.3 195 227 213 280 254 231 262 216 180 279 248 198 206 243 172 263 245 239 208 222 218 229 222 161 236 228 179 235 216 195 277 245 223 255 231 183 262 205 193 231 248 208 231 213 224 206 ¿Se podría recomendar alguna red en particular? En una primera etapa de un proyecto sobre el cultivo de pulpos, se desea determinar si es posible aumentar la tasa de crecimiento de los pulpos mediante un incremento de la temperatura del agua. Para ello se seleccionaron al azar juveniles de pulpo y se los asignó aleatoriamente a tres peceras de cultivo. En la primera pecera (Temp 0) se mantuvo a los pulpitos en condiciones de temperatura similares a las del medio natural, en la segunda pecera (Temp 1) se utilizó una temperatura constante de 20ºC y en la tercera pecera (Temp 2) se mantuvo una temperatura constante de 25ºC. Los pulpitos fueron alimentados ad libitum durante los 60 días que duró la experiencia. Debido a que no se dispuso de peceras individuales, los datos de los ejemplares de una misma pecera no pueden considerarse verdaderas réplicas, sino más bien pseudo-réplicas. Estos datos fueron asumidos como réplicas verdaderas teniendo en cuenta las dimensiones de las peceras (re-grandotas) en relación con los pulpitos (re- chiquitos) y a la nula observación de interacciones entre los ejemplares. Para cada pulpito se determinó el porcentaje de incremento en peso durante la experiencia como: \\[\\%Peso=\\frac{Peso_{inicial}-Peso_{final}}{Peso_{inicial}}100 \\] Los resultados obtenidos fueron: %Peso para los pulpos utilizados en cada experiencia Tabla 12.4: Porcentaje de aumento de peso de pulpos juveniles bajo tres temperaturas diferentes. Temp.0 = Medio Natural, Temp.1 = 20°C, Temp.2 = 25°C. Temp.0 Temp.1 Temp.2 78 107 124 84 115 121 117 96 140 82 110 141 112 106 134 115 92 125 107 114 118 102 85 133 104 90 128 103 108 141 77 94 126 125 108 143 123 134 76 118 140 ¿Es factible aumentar la tasa de crecimiento de los pulpos aumentando la temperatura del medio? "],
["disenos-experimentales.html", "Capítulo 13 DISEÑOS EXPERIMENTALES 13.1 Bloques al azar 13.2 Análisis de la varianza y pruebas", " Capítulo 13 DISEÑOS EXPERIMENTALES 13.1 Bloques al azar 13.1.1 Diseño de experimentos El diseño de un experimento se refiere a la estructura del experimento, en particular a: El grupo de tratamientos incluidos en el estudio. El grupo de unidades experimentales incluidas en el estudio. Las reglas y procedimientos por los cuales los tratamientos son asignados a las unidades experimentales (o viceversa). Las medidas son hechas sobre las unidades experimentales después que los tratamientos han sido aplicados. Los diseños estadísticos hacen referencia a las reglas y procedimientos a través de los cuales los tratamientos son asignados a las unidades experimentales. El uso de reglas y procedimientos impropios en la asignación de los tratamientos a las unidades experimentales puede traer serias dificultades. El proceso de medición es otro problema importante en los diseños experimentales. Idealmente, el proceso de medición debería producir medidas insesgadas y precisas. Medidas sesgadas pueden causar serias dificultades en el análisis del estudio. Una fuente importante de sesgo se debe a la falta de reconocimiento de diferencias en el proceso de evaluación. 13.1.2 Elementos de los Diseños de Bloques al Azar Un diseño de bloques al azar es un diseño aleatorio restringido en el cual las unidades experimentales son distribuidas en grupos homogéneos, llamados bloques, y los tratamientos son asignados al azar dentro de los bloques. Es un diseño especialmente utilizado en experimentación agrícola, en el que se desean comparar I tratamientos (por ejemplo, fertilizantes), asignando los tratamientos en J bloques (por ejemplo J fincas), de modo que se reparten los I tratamientos aleatoriamente en cada bloque (los fertilizantes se aplican aleatoriamente en I parcelas de una misma finca). Para una correcta aplicación de este diseño debe haber una máxima homogeneidad dentro de cada bloque, para que el efecto bloque sea el mismo para todos los tratamientos dentro de cada bloque. En un experimento agrícola, cada bloque está constituido por un número de parcelas que forman una superficie de relativa homogeneidad respecto al resto del campo. Cuando se conoce de gradiente de variabilidad del suelo, los bloques deben orientarse perpendicularmente al gradiente y las unidades experimentales deben tener su mayor dimensión en la misma dirección y sentido que dicho gradiente. En experimentos con animales, cada bloque estará constituido por un número de animales de aproximadamente igual peso, edad, raza, etc. Debe haber diferencias entre bloques. (ref:bloque) Esquema de diseño en bloques al azar. Cada bloque se dividió en cuatro y cada uno de los tratamientos dentro de los bloques se asignó al azar. Figura 13.1: (ref:bloque) 13.1.3 Criterios para definir los bloques Es necesaria una definición precisa de la unidad experimental, que dependerá del problema en particular. Una vez definida existen dos tipos de criterios para definir los bloques: Características asociadas con la unidad experimental: edad, inteligencia, educación, áreas geográficas, tamaño de la población, etc. Características asociadas con las características experimentales: observadores, instrumento de medición, tiempo de procesado, etc. 13.1.4 Ventajas y desventajas Ventajas Puede proveer resultados más precisos que un diseño completamente al azar de tamaño comparable. Se pueden estimar los datos de algunas unidades experimentales si se pierden. El análisis estadístico es relativamente simple. Desventajas Las observaciones perdidas dentro de cada bloque requieren cálculos complejos. Los grados de libertad del error experimental no son tan grandes como en un diseño completamente aleatorizado. Se hacen más suposiciones que en un modelo completamente al azar. 13.1.5 Modelo Se puede pensar a un diseño en bloques al azar como un estudio de dos factores (los bloques y los tratamientos son los factores), con una observación por celda. Si se puede asumir que no hay interacción entre los dos factores, se puede realizar un análisis de los efectos de los factores cuando hay una sola observación por celda y los factores tienen efectos fijos. Así, el modelo para un diseño en bloques al azar, cuando tanto los bloques como los tratamientos tienen efectos fijos y hay n bloques y r tratamientos, es: \\[ Y_{ij} = \\mu_{\\bullet \\bullet} + \\rho_{i} + \\tau_{j} + \\varepsilon_{ij} \\] donde: \\(\\mu_{\\bullet \\bullet}\\). es una constante \\(\\rho_{i}\\)son constantes para el efecto del bloque, sujetas a la restricción \\(\\sum\\rho_{i} = 0\\). \\(\\tau_{j}\\) son constantes para los efectos de los tratamientos, sujetas a la restricción \\(\\sum\\tau_{i} = 0\\) \\(\\varepsilon_{ij}\\) son v. a. independientes \\(N(0, \\sigma^{2})\\) \\[ i = 1,2,...,n;j = 1,2,...,r \\] Las \\(Y_{ij}\\) observaciones son independientes y se distribuyen normalmente, con media: \\[ E\\left( Y_{ij} \\right) = \\mu_{\\bullet \\bullet} + \\rho_{i} + \\tau_{j} \\] y varianza constante: \\[ {Var}\\left( Y_{ij} \\right) = \\sigma^{2} \\] 13.2 Análisis de la varianza y pruebas Los estimadores de mínimos cuadrados son: Parámetro Estimador \\(\\mu_{\\bullet \\bullet}\\) \\({\\hat{\\mu}}_{\\bullet \\bullet} = \\overline{Y_{\\bullet \\bullet}}\\) \\(\\rho_{i}\\) \\({\\hat{\\rho}}_{i} = {\\overline{Y}}_{i \\bullet} - {\\overline{Y}}_{\\bullet \\bullet}\\) \\(\\tau_{j}\\) \\({\\hat{\\tau}}_{j} = {\\overline{Y}}_{\\bullet j} - {\\overline{Y}}_{\\bullet \\bullet}\\) Los valores ajustados serán entonces: \\[ {\\hat{Y}}_{ij} = \\overline{Y}_{\\bullet \\bullet} + \\left( {\\overline{Y}}_{i \\bullet} - {\\overline{Y}}_{\\bullet \\bullet} \\right) + \\left( {\\overline{Y}}_{\\bullet j} - {\\overline{Y}}_{\\bullet \\bullet} \\right) = {\\overline{Y}}_{i \\bullet} + {\\overline{Y}}_{\\bullet j} - {\\overline{Y}}_{\\bullet \\bullet} \\] y los residuos son: \\[ \\varepsilon_{ij} = {\\overline{Y}}_{ij} - {\\hat{Y}}_{ij} = {\\overline{Y}}_{ij} - {\\overline{Y}}_{i \\bullet} + {\\overline{Y}}_{\\bullet j} - {\\overline{Y}}_{\\bullet \\bullet} \\] 13.2.1 Análisis de la varianza 13.2.1.1 Hipótesis Efectos de los tratamientos fijos Efectos de los tratamientos aleatorios \\(H_{0}\\): todos \\(\\tau_{j} = 0\\) \\(H_{0}:\\sigma_{\\tau}^{2} = 0\\) \\(H_{a}\\): no todos los \\(\\tau_{j} = 0\\) \\(H_{a}:\\sigma_{\\tau}^{2} &gt; 0\\) Se usa la misma prueba estadística, tanto si los efectos de los tratamientos son al azar como si son fijos. \\[ F^{*} = \\frac{CM_{TR}}{CM_{\\text{BL.TR}}} \\] la regla de decisión será entonces: Sí \\(F^{*} \\leq F_{\\alpha;\\left( r-1 \\right),\\left( n-1 \\right)\\left( r-1 \\right)}\\) no se rechaza \\(H_{0}\\) Sí \\(F^{*} &gt; F_{\\alpha;\\left( r-1 \\right),\\left( n-1 \\right)\\left( r-1 \\right)}\\) se rechaza \\(H_{0}\\) 13.2.2 Prueba de Tukey de Aditividad La suma de cuadrados especial de interacción está dada, en este caso, por: \\[ SC_{BL \\bullet TR}^{*} = \\frac{\\left\\lbrack \\sum_{i}^{}{\\sum_{j}^{}{\\left( \\overline{Y_{i \\bullet}} - \\overline{Y_{\\bullet \\bullet}} \\right)\\left( \\overline{Y_{\\bullet j}} - \\overline{Y_{\\bullet \\bullet}} \\right)Y_{ij}}} \\right\\rbrack^{2}}{{\\sum_{i}^{}\\left( \\overline{Y_{i \\bullet}} - \\overline{Y_{\\bullet \\bullet}} \\right)}^{2}\\sum_{j}^{}\\left( \\overline{Y_{\\bullet j}} - \\overline{Y_{\\bullet \\bullet}} \\right)^{2}} \\] La suma de cuadrados remanente se obtiene como: \\[ SC_{Rem}^{*} = SC_{T}-SC_{BL}-SC_{TR}-SC_{BL \\bullet TR}^{*} \\] La prueba estadística es. \\[ F^{*} = \\frac{SC_{BL\\bullet TR}^{*}}{1} \\div \\frac{SC_{Rem}^{*}}{rn - r - n} \\] Tabla 13.1: Tabla de anova para un diseño de bloques al azar Fte. de variación SC GL CM E(CM) Tratamientos fijos E(CM) Tratamientos aleatorios Bloques \\(r{\\sum_{i}^{}\\left( \\overline{Y_{i \\bullet}}- \\overline{Y_{\\bullet \\bullet}} \\right)}^{2}\\) \\(n - 1\\) \\(\\frac{SC_{BL}}{n - 1}\\) \\(\\sigma^{2}+\\frac{r\\sum_{i}^{}\\rho_{i}}{n - 1}\\) \\(\\sigma^{2}+\\frac{r\\sum_{i}^{}\\rho_{i}}{n - 1}\\) Tratamientos \\(n\\sum_{j}^{}\\left( \\overline{Y_{\\bullet j}}- \\overline{Y_{\\bullet \\bullet}} \\right)^{2}\\) \\(r - 1\\) \\(\\frac{SC_{TR}}{r - 1}\\) \\(\\sigma^{2}+\\frac{n\\sum_{j}^{}\\tau_{j}}{r - 1}\\) \\(\\sigma^{2}+ \\backslash n\\sigma_{\\tau}^{2}\\) Error \\(SC_T - SC_{BL} - SC_{TR}\\) \\((n 1)(r 1)\\) \\(\\frac{SC_{BL \\bullet TR}^{*}}{\\left( n - 1 \\right)\\left( r - 1 \\right)}\\) \\(\\sigma^{2}\\) \\(\\sigma^{2}\\) Total \\(nr - 1\\) Ejemplo: Para estudiar las diferencias entre tres fertilizantes sobre la producción de papas, se dispuso de 5 fincas, cada una de las cuales se dividió en tres parcelas del mismo tamaño y tipo. Los fertilizantes fueron asignados al azar en las parcelas de cada finca. El rendimiento en toneladas fue: Finca Fertilizante 1 Fertilizante 2 Fertilizante 3 1 1 5 8 2 2 8 14 3 7 9 16 4 6 13 18 5 12 14 17 Se desea saber si hay diferencias entre los fertilizantes. Se trata de un diseño de bloques al azar, cada finca es un bloque. Prueba de aditividad \\[ SC_{BL \\bullet TR}^{*} = 0.262665101 \\] \\[ SC_{Rem}^{*} = 23.60400157 \\] \\[ CM_{Rem}^{*} = 3.372000224 \\] \\[ F = 0.077895932 \\] \\[ p = 0.78823515 \\] Por lo tanto, no se rechaza la hipótesis nula, no hay interacción. ANOVA RESUMEN Cuenta Suma Promedio Varianza 1 3 14 4.66666667 12.3333333 2 3 24 8 36 3 3 32 10.6666667 22.3333333 4 3 37 12.3333333 36.3333333 5 3 43 14.3333333 6.33333333 Fertilizante 1 5 28 5.6 19.3 Fertilizante 2 5 49 9.8 13.7 Fertilizante 3 5 73 14.6 15.8 Origen de las variaciones Suma de cuadrados Grados de libertad Promedio de los cuadrados F Probabilidad Valor crítico para F Filas 171.333333 4 42.8333333 14.3575419 0.00100812 3.83785448 Columnas 202.8 2 101.4 33.9888268 0.00012292 4.45896831 Error 23.8666667 8 2.98333333 Total 398 14 En R ## ## Tukey test on 5% alpha-level: ## ## Test statistic: 0.0779 ## Critival value: 5.591 ## The additivity hypothesis cannot be rejected. Tabla 13.2: Producción de papas por fertilizante fertilizante mean var sd Fertilizante 1 5.6 19.3 4.393 Fertilizante 2 9.8 13.7 3.701 Fertilizante 3 14.6 15.8 3.975 Tabla 13.3: Producción de papas por finca Finca mean var sd 1 4.667 12.33 3.512 2 8 36 6 3 10.67 22.33 4.726 4 12.33 36.33 6.028 5 14.33 6.333 2.517 Tabla 13.4: Prueba de homogeneidad de varianzas de Bartlett. Test statistic df P value 0.1074 2 0.9477 Tabla 13.5: Prueba de homogeneidad de varianzas de Levene.   Df F value Pr(&gt;F) group 2 0.06604 0.9364 12 NA NA Tabla 13.6: ANOVA para la producción de papa con tres fertilizantes con bloques al alzar.   Df Sum Sq Mean Sq F value Pr(&gt;F) fertilizante 2 202.8 101.4 33.99 0.0001229 Finca 4 171.3 42.83 14.36 0.001008 Residuals 8 23.87 2.983 NA NA Tabla 13.6: Medias marginales, error estándar, grados de libertad, intervalo de confianza inferior y superior del 95%, y agrupamiento según la prueba de Tukey. fertilizante lsmean SE df lower.CL upper.CL .group Fertilizante 1 5.6 0.7724 8 3.819 7.381 1 Fertilizante 2 9.8 0.7724 8 8.019 11.58 2 Fertilizante 3 14.6 0.7724 8 12.82 16.38 3 (ref:produccion) Producción de papas (tn/ha) bajo tres fertilizantes en 5 fincas. Figura 13.2: (ref:produccion) "],
["problemas-anova-diseno-experimental.html", "Capítulo 14 Problemas ANOVA Diseño Experimental 14.1 Ejemplo 14.2 Problemas", " Capítulo 14 Problemas ANOVA Diseño Experimental Antes de comenzar bajen el archivo donde realizarán su informe reproducible. En la consola copien este código: download.file(&quot;http://bit.ly/informe-bloques&quot;, &quot;informe-bloques.Rmd&quot;) Pueden abrirlo desde la pestaña de archivos, a la derecha. Cambien el nombre por el suyo en el encabezado y mientras leen este capítulo respondan las preguntas. 14.1 Ejemplo Para estudiar las diferencias entre tres fertilizantes sobre la producción de papas, se dispuso de 5 fincas, cada una de las cuales se dividió en tres parcelas del mismo tamaño y tipo. Los fertilizantes fueron asignados al azar en las parcelas de cada finca. El rendimiento en toneladas fue: fert %&gt;% pander(caption = &quot;(#tab:papas) Rendimiento en toneladas de papa por ha en 5 fincas (bloques) bajo tres fertilizantes (tratamientos).&quot;) Tabla 14.1: Rendimiento en toneladas de papa por ha en 5 fincas (bloques) bajo tres fertilizantes (tratamientos). Finca Fertilizante.1 Fertilizante.2 Fertilizante.3 1 1 5 8 2 2 8 14 3 7 9 16 4 6 13 18 5 12 14 17 El gráfico de perfiles de los datos Figura 14.1: Gráfico de Perfiles para la producción de papas bajo tres fertilizantes, en cinco fincas. Lo primero que hay comprobar es que se da el supuesto de supuesto de aditividad. Es decir que no hay interacción entre los bloques y los tratamientos. Es decir, que el bloque que produce mayores o menores rendimientos lo hace para todos los fertilizantes y viceversa. Esto se hace con el test de aditividad de Tukey, la función tukey.test() en R. Recuerden del práctico de [ANOVA doble][aditividad-tukey] que la función necesita una matriz con solo los datos. fert %&gt;% select(-Finca) %&gt;% as.matrix() %&gt;% tukey.test() ## ## Tukey test on 5% alpha-level: ## ## Test statistic: 0.0779 ## Critival value: 5.591 ## The additivity hypothesis cannot be rejected. Medias por Fertilizante: fert_long %&gt;% group_by(Fertilizante) %&gt;% summarise(media = mean(Produccion), var = var(Produccion), sd = sd(Produccion)) ## # A tibble: 3 x 4 ## Fertilizante media var sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Fertilizante.3 14.6 15.8 3.97 ## 2 Fertilizante.2 9.8 13.7 3.70 ## 3 Fertilizante.1 5.6 19.3 4.39 Medias por Finca: fert_long %&gt;% group_by(Finca) %&gt;% summarise(media = mean(Produccion), var = var(Produccion), sd = sd(Produccion)) ## # A tibble: 5 x 4 ## Finca media var sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4.67 12.3 3.51 ## 2 2 8 36 6 ## 3 3 10.7 22.3 4.73 ## 4 4 12.3 36.3 6.03 ## 5 5 14.3 6.33 2.52 Prueba de supuestos: bartlett.test(Produccion ~ Fertilizante, data = fert_long) ## ## Bartlett test of homogeneity of variances ## ## data: Produccion by Fertilizante ## Bartlett&#39;s K-squared = 0.10744, df = 2, p-value = 0.9477 leveneTest(Produccion ~ Fertilizante, data = fert_long) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.066 0.9364 ## 12 Para realizar el ANOVA se usa la formula de misma manera que cuando tenemos un diseño de dos factores sin réplicas. Es decir, del lado derecho de la fórmula, se escribe el nombre de la variable unido con un signo +con la variable que corresponde a los bloques. El orden no importa. Siguiendo el ejemplo de fertilizante: fert_aov &lt;- aov(Produccion ~ Fertilizante + Finca, data = fert_long) summary(fert_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Fertilizante 2 202.80 101.40 33.99 0.000123 *** ## Finca 4 171.33 42.83 14.36 0.001008 ** ## Residuals 8 23.87 2.98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Para obtener las comparaciones múltiples entre los fertilizantes se puede usar la función emmeans() (estimated marginal means). fert_lsm &lt;- emmeans(fert_aov, pairwise ~ Fertilizante) summary(fert_lsm) ## $emmeans ## Fertilizante emmean SE df lower.CL upper.CL ## Fertilizante.3 14.6 0.772442 8 12.818746 16.381254 ## Fertilizante.2 9.8 0.772442 8 8.018746 11.581254 ## Fertilizante.1 5.6 0.772442 8 3.818746 7.381254 ## ## Results are averaged over the levels of: Finca ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Fertilizante.3 - Fertilizante.2 4.8 1.092398 8 4.394 0.0058 ## Fertilizante.3 - Fertilizante.1 9.0 1.092398 8 8.239 0.0001 ## Fertilizante.2 - Fertilizante.1 4.2 1.092398 8 3.845 0.0121 ## ## Results are averaged over the levels of: Finca ## P value adjustment: tukey method for comparing a family of 3 estimates Para obtener los grupos la función cld() (compact letter display) permite construirlos o usar cld dentro de la formula: emmeans(fert_aov, cld ~ Fertilizante) ## Fertilizante emmean SE df lower.CL upper.CL .group ## Fertilizante.1 5.6 0.772442 8 3.818746 7.381254 1 ## Fertilizante.2 9.8 0.772442 8 8.018746 11.581254 2 ## Fertilizante.3 14.6 0.772442 8 12.818746 16.381254 3 ## ## Results are averaged over the levels of: Finca ## Confidence level used: 0.95 ## P value adjustment: tukey method for comparing a family of 3 estimates ## significance level used: alpha = 0.05 14.2 Problemas Se estudia mediante un diseño de bloques al azar el efecto de una hormona de crecimiento sobre ratas jóvenes. Se prueba una dosis débil (tratamiento A), una dosis fuerte (tratamiento B) y un tercer tratamiento, que sería el testigo, consistente en la aplicación de un placebo. Se toman 6 camadas de ratas al azar, también al azar se seleccionan tres animales de cada una. Se asignan los tratamientos al azar dentro de cada camada y al cabo de 15 días se mide el aumento en peso, en decigramos, con los siguientes resultados: Tabla 14.2: Aumento de peso (dg) en ratas de 6 camadas bajo el efecto de la hormona de crecimiento en bajo dos concetraciones (A: baja, B: alta) y un control. Tratamiento 1 2 3 4 5 6 Testigo 3.98 2.75 1.9 3.89 2.82 3.31 A 6.61 3.02 2.09 5.62 3.16 6.31 B 8.71 4.68 2.82 5.01 3.16 6.61 Estudiar la aditividad del modelo (validez del modelo). Poner a prueba la efectividad de la hormona. Realizar contrastes múltiples, si corresponde. ¿Qué sucede si omitimos el factor bloque? ¿A donde fue a parar toda la variabilidad debida a los bloques? Un investigador estudia los efectos de tres dietas experimentales con variado contenido de grasas sobre el nivel total de lípidos en el plasma. El nivel total de lípidos en el plasma es utilizado como un indicador de posibles enfermedades coronarias. Quince hombres de peso corporal (unos Adonis) ideal fueron agrupados en 5 bloques de acuerdo a la edad. Dentro de cada bloque las dietas fueron asignadas al azar a los tres sujetos. La tabla presenta los datos correspondientes a la reducción en el nivel de lípidos (g/l) después de que los individuos fueron sometidos a la dieta durante un período de tiempo fijo: Tabla 14.3: Reducción de lípidos (g/l) bajo tres dieta con distinto contenido de grasas. Bloque Extremadamente.bajo Bajo Moderadamente.bajo 15 – 24 0.73 0.67 0.15 25 – 34 0.86 0.75 0.21 35 – 44 0.94 0.81 0.26 45 – 54 1.4 1.32 0.75 55 – 64 1.62 1.41 0.78 ¿Por qué la edad fue usada como una variable de bloque? Realice la prueba de Tukey, ¿cuáles son sus conclusiones? (trabaje con \\(\\alpha\\) = 0.05 y \\(\\alpha\\) = 0.01) Asumiendo que un diseño en bloques al azar es el adecuado c) Obtenga la tabla de ANOVA Estime \\(D_1 = \\mu_1- \\mu_2\\) y \\(D_2 = \\mu_2 - \\mu_3\\) , usando un intervalo de confianza de Bonferroni. ¿Es significativo alguno de los contrastes? En este experimento no fue usada una dieta estándar como control. ¿Que tipo de justificación puede haber tenido el investigador para no incluir un tratamiento testigo con fines comparativos? "],
["pruebas-para-varias-muestras-pareadas.html", "Capítulo 15 Pruebas para varias muestras pareadas 15.1 Prueba de Friedman para varias muestras relacionadas", " Capítulo 15 Pruebas para varias muestras pareadas Existen diversos tipos de pruebas no paramétricas para comparar k muestras relacionadas. Se considera un bloque a una unidad experimental homogénea que puede ser particionada en k elementos o una serie de k unidades experimentales que se encuentran vinculadas entre sí de tal forma que la respuesta de cada una de ellas a un tratamiento dado no es independiente de las demás. Un bloque puede estar constituido por diferentes animales de una misma camada (base genética común y diferente de cualquier otra camada), un mismo individuo sometido a k tratamientos diferentes, una parcela de terreno dividida en k sub-parcelas (idéntico tipo de suelo para las k-sub-parcelas), etc. De esta manera, el diseño experimental subyacente a las pruebas no paramétricas para k muestras relacionadas es el diseño de bloques al azar. Este diseño consiste en comparar una serie de c tratamientos. Para ello, se toman r bloques que son particionados en c partes y a cada una de ellas se le asigna al azar cada uno de los c tratamientos. Si los diferentes bloques utilizados fueran homogéneos entre sí, el diseño experimental puede reducirse a un diseño análogo al del ANOVA de un factor de la estadística paramétrica. La tabla básica de datos para estas pruebas no paramétricas es de la forma: Tratamientos Bloques 1 2 \\(\\ldots\\) \\(c\\) Total de fila 1 \\(X_{11}\\) \\(X_{12}\\) \\(\\ldots\\) \\(X_{1c}\\) \\(R_{1}\\) 2 \\(X_{21}\\) \\(X_{22}\\) \\(\\ldots\\) \\(X_{2c}\\) \\(R_{2}\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\ddots\\) \\(\\vdots\\) \\(\\vdots\\) \\(r\\) \\(X_{r1}\\) \\(X_{r2}\\) \\(\\ldots\\) \\(X_{rc}\\) \\(R_{r}\\) Total de columna \\(C_{1}\\) \\(C_{2}\\) \\(\\ldots\\) \\(C_{c}\\) \\(N\\) 15.1 Prueba de Friedman para varias muestras relacionadas La prueba de Friedman es la versión no paramétrica del ANOVA de dos vías con una única observación por celda cuando el diseño es el de bloques al azar. En este caso, se asignan rangos dentro de cada bloque [\\(R(X_{ij})\\)] de tal manera que la menor observación tiene el rango 1 y así sucesivamente hasta llegar a la mayor observación que se corresponde con el rango c. En caso de empates, se promedian los rangos correspondientes. Este procedimiento se realiza en cada bloque, obteniéndose una tabla de rangos de la forma: Tratamientos Bloques 1 2 \\(\\ldots\\) c 1 \\(R(X_{11})\\) \\(R(X_{12})\\) \\(\\ldots\\) \\(R(X_{1c})\\) 2 \\(R(X_{21}\\)) \\(R(X_{22})\\) \\(\\ldots\\) \\(R(X_{2c})\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\vdots\\) \\(\\ddots\\) \\(\\vdots\\) r \\(R(X_{r1})\\) \\(R(X_{r2})\\) \\(\\ldots\\) \\(R(X_{rc})\\) Suma de rangos (\\(R_{j}\\)) \\(R_{1}\\) \\(R_{2}\\) \\(\\ldots\\) \\(R_{c}\\) 15.1.1 Estadístico La prueba de Friedman presenta dos estadísticos diferentes (\\(\\chi_{F}^{2}\\) y \\(F_{F}\\)). Estos estadísticos, calculados con la corrección por empates, se obtienen como: \\[ {A_{1} = \\sum_{i = 1}^{r}{\\sum_{j = 1}^{c}\\left\\lbrack R\\left( X_{ij} \\right) \\right\\rbrack^{2}} } \\\\{C_{1} = \\frac{\\text{rc}\\left( c + 1 \\right)^{2}}{4}}\\\\{\\chi_{F}^{2} = \\frac{\\left( c - 1 \\right)\\sum_{j = 1}^{c}\\left( R_{j} - \\frac{r\\left( c + 1 \\right)}{2} \\right)^{2}}{A_{1} - C_{1}}}\\\\{F_{F} = \\frac{\\left( r - 1 \\right)\\chi_{F}^{2}}{r\\left( c - 1 \\right) - \\chi_{F}^{2}}} \\] El \\(\\chi_{F}^{2}\\) se distribuye en forma aproximada según una \\(\\chi^{2}\\) con \\(c - 1\\) grados de libertad. Sin embargo, aún con muestras relativamente grandes, esta aproximación no es buena. Por esta razón es preferible utilizar el estadístico \\(F\\) el cuál se distribuye aproximadamente según una distribución de Fisher con \\(\\nu_1=(c-1)\\) y \\(\\nu_2=(r-1)(c-1)\\) grados de libertad. 15.1.2 Supuestos Los r bloques son mutuamente independientes. Esto significa que los resultados obtenidos en un bloque no influencian los resultados de los otros bloques. Dentro de cada bloque se asignan los rangos a los resultados de los c tratamientos de acuerdo a algún criterio de interés. 15.1.3 Hipótesis \\(H_{0}\\): los c tratamientos tiene idéntico efecto. \\(H_{a}\\): al menos uno de los c tratamientos presenta un efecto diferente de los demás. Los criterios de decisión para la prueba son: Si \\(\\chi_{F}^{2} \\geq \\chi_{c - 1;1 - \\alpha}^{2}\\) ó \\(F_{F} \\geq F_{\\nu_{1} = \\left( c - 1 \\right);\\nu_{2} = \\left( r - 1 \\right)\\left( c - 1 \\right);1 - \\alpha\\ }\\) entonces Rechazo \\(H_{0}\\). Si \\(\\chi_{F}^{2} &lt; \\chi_{c - 1;1 - \\alpha}^{2}\\) ó \\(F_{F} &lt; F_{\\nu_{1} = \\left( c - 1 \\right);\\nu_{2} = \\left( r - 1 \\right)\\left( c - 1 \\right);1 - \\alpha\\ }\\) entonces No Rechazo \\(H_{0}\\). 15.1.4 Comparaciones múltiples Si la prueba de Friedman resultó significativa, es pertinente realizar contrastes para determinar cuáles son los tratamientos que difieren entre sí. Para ello se realizan comparaciones de a pares entre los c tratamientos. Se considera que dos tratamientos son significativamente diferentes si se verifica la siguiente desigualdad: \\[ \\left| R_{j} - R_{i} \\right| \\geq t_{\\nu = \\left( r - 1 \\right)\\left( c - 1 \\right);1 - \\alpha\\backslash 2}\\sqrt{\\frac{2\\left( rA_{1} - \\sum_{j = 1}^{c}R_{j}^{2} \\right)}{\\left( r - 1 \\right)\\left( c - 1 \\right)}} \\] Otro método de contraste que es más sencillo de calcular, pero más conservativo que el anterior, es el que se define con la siguiente desigualdad: \\[ \\left| R_{j} - R_{i} \\right| \\geq \\frac{q_{\\infty;c;\\alpha}}{\\sqrt{2}}\\sqrt{\\frac{c(c + 1)}{6\\ n}} \\] Para ambos tipos de contrastes, α es el nivel de significación de la prueba y debe ser el mismo que se empleó para la prueba de Friedman. Ejemplo. En un estudio sobre diferentes métodos posibles para detener el proceso de desertificación, se intenta evaluar si cuatro variedades de pasto son igualmente efectivas para fijar el suelo. Para ello se seleccionaron al azar distintas parcelas con diferentes niveles de desertificación y cada una de ellas fue dividida en cuatro sectores. Cada sector fue plantado con una de las variedades de pasto, realizándose esta asignación aleatoriamente. Luego de un tiempo razonable se evaluó con una escala apropiada el porcentaje de suelo cubierto por cada variedad de pasto, considerándose que cuanto mayor sea este índice mejor es la performance de la variedad de pasto para fijar el suelo. Los resultados obtenidos fueron: Parcela Pasto 1 Pasto 2 Pasto 3 Pasto 4 1 14 10.5 7 3.5 2 14 7 10.5 3.5 3 10.5 5.25 5.25 14 4 10.5 3.5 7 14 5 14 7 3.5 10.5 6 7 7 7 14 7 3.5 10.5 7 14 8 7 14 3.5 10.5 9 12.25 3.5 7 12.25 10 14 3.5 10.5 7 11 14 7 10.5 3.5 12 12.25 3.5 7 12.25 Empleando la prueba de Friedman, los cálculos realizados pueden esquematizarse como: Rangos PASTO1 PASTO2 PASTO3 PASTO4 1 4 3 2 1 2 4 2 3 1 3 3 1.5 1.5 4 4 3 1 2 4 5 4 2 1 3 6 2 2 2 4 7 1 3 2 4 8 2 4 1 3 9 3.5 1 2 3.5 10 4 1 3 2 11 4 2 3 1 12 3.5 1 2 3.5 r 12 c 4 \\(A_1\\) 356.500 \\(C_1\\) 300.000 \\(R_j\\) 38.000 23.500 24.500 34.000 \\((R_j-r(c+1)/2)^2\\) 64.000 42.250 30.250 16.000 \\(\\chi^2_F\\) 8.097 gl 3 Valor p 0.044 \\(F_F\\) 3.192 \\(\\nu_1\\) 3 \\(\\nu_2\\) 33 Valor p 0.036 En R: Esta prueba puede realizarse en R. Existen dos funciones: friedman.test() en el paquete stats, que viene por defecto con la instalación de R. friedman() en el paquete agricolae, hay que instalar el paquete pero ofrece, además del test, las comparaciones entre tratamientos. posthoc.friedman.conover.test() en el paquete PMCMR realiza la comparación de a pares usando la distribución t. posthoc.friedman.nemeyi.test() en el paquete PMCMR realiza la comparación de a pares usando la distribución q, de . En ambos casos los resultados de la prueba son iguales. Abajo está el resultado de friedman() pasto &lt;- gather(pasto, key = &quot;Pasto&quot;, value = &quot;performance&quot;, -Parcela) pasto %$% friedman(Parcela, Pasto, performance, group = FALSE, console = TRUE) ## ## Study: performance ~ Parcela + Pasto ## ## Pasto, Sum of the ranks ## ## performance r ## Pasto.1 38.0 12 ## Pasto.2 23.5 12 ## Pasto.3 24.5 12 ## Pasto.4 34.0 12 ## ## Friedman&#39;s Test ## =============== ## Adjusted for ties ## Critical Value: 8.097345 ## P.Value Chisq: 0.04404214 ## F Value: 3.192198 ## P.Value F: 0.03621547 ## ## Post Hoc Analysis ## ## Comparison between treatments ## Sum of the ranks ## ## difference pvalue signif. LCL UCL ## Pasto.1 - Pasto.2 14.5 0.0149 * 3.02 25.98 ## Pasto.1 - Pasto.3 13.5 0.0226 * 2.02 24.98 ## Pasto.1 - Pasto.4 4.0 0.4834 -7.48 15.48 ## Pasto.2 - Pasto.3 -1.0 0.8604 -12.48 10.48 ## Pasto.2 - Pasto.4 -10.5 0.0717 . -21.98 0.98 ## Pasto.3 - Pasto.4 -9.5 0.1017 -20.98 1.98 Y aquí el resultado de friedman.test() : friedman.test(performance ~ Pasto | Parcela, data = pasto) ## ## Friedman rank sum test ## ## data: performance and Pasto and Parcela ## Friedman chi-squared = 8.0973, df = 3, p-value = 0.04404 La función devuelve los dos tipos de estadísticos con su probabilidad asociada. En ambos casos la probabilidad resulta menor a \\(\\alpha\\) por lo que se concluye que existen diferencias significativas entre las especies de pasto. Entonces, resulta de interés saber que pasto es el mejor. La función también nos da el resultado del primer tipo de contraste, es decir del contraste t: library(PMCMR) posthoc.friedman.nemenyi.test(performance ~ Pasto | Parcela, data = pasto) ## ## Pairwise comparisons using Nemenyi multiple comparison test ## with q approximation for unreplicated blocked data ## ## data: performance and Pasto and Parcela ## ## Pasto.1 Pasto.2 Pasto.3 ## Pasto.2 0.10 - - ## Pasto.3 0.14 1.00 - ## Pasto.4 0.92 0.34 0.44 ## ## P value adjustment method: none No nos da el segundo tipo de contraste, pero a partir de los datos que devuelve la función resulta sencillo calcularlo. Utilizando ambos tipos de contrastes los resultados son: 38 23.5 24.5 34 Contraste t VC 11.481687 PASTO1 PASTO2 PASTO3 PASTO4 t 2.035 38 PASTO1 S S ns gl 33 23.5 PASTO2 14.5 ns ns ES 5.643 24.5 PASTO3 13.5 1 ns VC 11.482 34 PASTO4 4 10.5 9.5 38 23.5 24.5 34 Contraste q VC 16.685835 PASTO1 PASTO2 PASTO3 PASTO4 Z 2.638 38 PASTO1 ns ns ns 1-α 0.996 23.5 PASTO2 14.5 ns ns ES 6.325 24.5 PASTO3 13.5 1 ns VC 16.686 34 PASTO4 4 10.5 9.5 El método 1 de contrastes permite determinar que el Pasto 1 presenta una mayor eficiencia para fijar el suelo que los Pastos 2 y 3, mientras que el resto de las comparaciones resultan no significativas. En base a estos resultados, el Pasto 1 sería la variedad a elegir para fijar los suelos en zonas desertificadas. Se puede resaltar que el método 2 de contrastes fue incapaz de detectar estas diferencias. "],
["practico-de-pruebas-no-parametricas-para-muestras-relacionadas.html", "Capítulo 16 Práctico de Pruebas no paramétricas para muestras relacionadas 16.1 Problemas 16.2 Problema 4", " Capítulo 16 Práctico de Pruebas no paramétricas para muestras relacionadas Es posible realizar el equivalente no paramétrico de un anova simple con bloques. Hay al menos dos formas de hacerlo en R. Una es con la función friedman.test() del paquete stats. Tiene una interfaz de vectores y otra de formula. La de vectores tiene tres argumentos, y la variable de respuesta, groups la variable tratamiento, y blocks los bloques. La interfaz de formula usa la ya conocida forma y ~ groups pero para indicar los bloques usa una barra recta de esta manera y ~ groups | blocks. pasto ## Parcela Pasto.1 Pasto.2 Pasto.3 Pasto.4 ## 1 1 14.00 10.50 7.00 3.50 ## 2 2 14.00 7.00 10.50 3.50 ## 3 3 10.50 5.25 5.25 14.00 ## 4 4 10.50 3.50 7.00 14.00 ## 5 5 14.00 7.00 3.50 10.50 ## 6 6 7.00 7.00 7.00 14.00 ## 7 7 3.50 10.50 7.00 14.00 ## 8 8 7.00 14.00 3.50 10.50 ## 9 9 12.25 3.50 7.00 12.25 ## 10 10 14.00 3.50 10.50 7.00 ## 11 11 14.00 7.00 10.50 3.50 ## 12 12 12.25 3.50 7.00 12.25 pasto &lt;- gather(pasto, key = &quot;Pasto&quot;, value = &quot;performance&quot;, -Parcela) friedman.test(performance ~ Pasto | Parcela, data = pasto) ## ## Friedman rank sum test ## ## data: performance and Pasto and Parcela ## Friedman chi-squared = 8.0973, df = 3, p-value = 0.04404 out &lt;- pasto %$% friedman(Parcela, Pasto, performance, group = FALSE, console = TRUE) ## ## Study: performance ~ Parcela + Pasto ## ## Pasto, Sum of the ranks ## ## performance r ## Pasto.1 38.0 12 ## Pasto.2 23.5 12 ## Pasto.3 24.5 12 ## Pasto.4 34.0 12 ## ## Friedman&#39;s Test ## =============== ## Adjusted for ties ## Critical Value: 8.097345 ## P.Value Chisq: 0.04404214 ## F Value: 3.192198 ## P.Value F: 0.03621547 ## ## Post Hoc Analysis ## ## Comparison between treatments ## Sum of the ranks ## ## difference pvalue signif. LCL UCL ## Pasto.1 - Pasto.2 14.5 0.0149 * 3.02 25.98 ## Pasto.1 - Pasto.3 13.5 0.0226 * 2.02 24.98 ## Pasto.1 - Pasto.4 4.0 0.4834 -7.48 15.48 ## Pasto.2 - Pasto.3 -1.0 0.8604 -12.48 10.48 ## Pasto.2 - Pasto.4 -10.5 0.0717 . -21.98 0.98 ## Pasto.3 - Pasto.4 -9.5 0.1017 -20.98 1.98 La función imprime el resultado en la pantalla. Pero si no lo guardamos no es posible seguir trabajando con él. Por eso, al guardarlo, podemos ver la misma información que vemos arribal y algunos datos extra con la diferencia mínima significativa. out ## $statistics ## Chisq Df p.chisq F DFerror p.F t.value LSD ## 8.097345 3 0.04404214 3.192198 33 0.03621547 2.034515 11.48168 ## ## $parameters ## test name.t ntr alpha ## Friedman Pasto 4 0.05 ## ## $means ## performance rankSum std r Min Max Q25 Q50 Q75 ## Pasto.1 11.083333 38.0 3.526415 12 3.5 14.0 9.6250 12.250 14.000 ## Pasto.2 6.854167 23.5 3.375140 12 3.5 14.0 3.5000 7.000 7.875 ## Pasto.3 7.145833 24.5 2.413170 12 3.5 10.5 6.5625 7.000 7.875 ## Pasto.4 9.916667 34.0 4.372348 12 3.5 14.0 6.1250 11.375 14.000 ## ## $comparison ## difference pvalue signif. LCL UCL ## Pasto.1 - Pasto.2 14.5 0.0149 * 3.02 25.98 ## Pasto.1 - Pasto.3 13.5 0.0226 * 2.02 24.98 ## Pasto.1 - Pasto.4 4.0 0.4834 -7.48 15.48 ## Pasto.2 - Pasto.3 -1.0 0.8604 -12.48 10.48 ## Pasto.2 - Pasto.4 -10.5 0.0717 . -21.98 0.98 ## Pasto.3 - Pasto.4 -9.5 0.1017 -20.98 1.98 ## ## $groups ## NULL ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; Podemos usar los datos que nos da para calcular a mano el valor crítico con la distribución q: alpha &lt;- 0.05 c &lt;- nrow(out$means) Z &lt;- abs(qnorm(alpha/(c*(c-1)))) r &lt;- unique(out$means$r) VC &lt;- Z * sqrt(r*c*(c+1)/6) VC ## [1] 16.6858 Con el paquete library(PMCMR) ## PMCMR is superseded by PMCMRplus and will be no longer maintained. You may wish to install PMCMRplus instead. ## ## Attaching package: &#39;PMCMR&#39; ## The following object is masked from &#39;package:agricolae&#39;: ## ## durbin.test posthoc.friedman.nemenyi.test(performance ~ Pasto | Parcela, data = pasto) ## ## Pairwise comparisons using Nemenyi multiple comparison test ## with q approximation for unreplicated blocked data ## ## data: performance and Pasto and Parcela ## ## Pasto.1 Pasto.2 Pasto.3 ## Pasto.2 0.10 - - ## Pasto.3 0.14 1.00 - ## Pasto.4 0.92 0.34 0.44 ## ## P value adjustment method: none 16.1 Problemas Un ecólogo desea determinar si cuatro índices de importancia relativa empleados para análisis de la dieta evalúan en forma similar la importancia relativa de las presas. Para ello utilizó datos de dieta del lobo marino, calculó todos los índices y los expresó en porcentaje. ¿Son equivalentes estos índices para estudiar la importancia relativa de las presas?. Los resultados obtenidos fueron: Porcentaje de consumo de presas según cuatro índices. Presa Indice.1 Indice.2 Indice.3 Indice.4 Merluza 34.6 35.09 34.43 35.1 Pulpo 20.24 20.12 20.46 19.71 Calamar 22.93 22.24 23.75 21.96 Raneya 5.48 5.16 4.89 4.02 Anchoíta 3.39 3.54 3.19 2.84 Nototenia 2.39 2.43 3.59 2.94 Pampanito 6.68 6.47 5.99 6.47 Abadejo 0.1 0.71 0.9 1.76 Cangrejos 0.7 1.82 0.7 1.27 Langostino 1.4 1.92 1.6 0.78 Salmón 1.5 0.3 0.3 1.27 Chanchita 0.6 0.2 0.2 1.86 En un estudio veterinario se desea determinar la efectividad de distintas drogas para minimizar el nivel de parasitosis intestinales. Para ello se tomaron al azar camadas de ratones y a diferentes individuos de cada camada se les aplicó al azar una de las drogas. Posteriormente, se alimentó a los ratones con comida infestada y luego de un mes fueron sacrificados, realizándose la necropsia para determinar el número de helmintos hallados en el intestino. ¿Existe un efecto de las drogas sobre el nivel de parasitosis de los ratones? Los resultados fueron: parasitosis %&gt;% pander(caption = &quot;Número de helmitos en intestino de ratones tratados con cuatro antiparasitarios distintos.&quot;) Número de helmitos en intestino de ratones tratados con cuatro antiparasitarios distintos. Camada Droga.1 Droga.2 Droga.3 Droga.4 1 20 13 23 26 2 22 12 23 15 3 31 24 36 65 4 18 9 10 62 5 25 23 50 70 6 17 7 25 60 7 29 11 28 68 8 26 21 15 15 9 26 23 51 38 10 29 15 31 13 11 24 15 45 46 12 19 6 24 42 Un ingeniero agrónomo desea determinar si cuatro fertilizantes tienen igual efecto en la producción de soja. Para ello seleccionó al azar 12 parcelas con características de suelo diferentes, aplicando al azar cada uno de los fertilizantes en distintos sectores de iguales dimensiones de cada parcela. La producción de soja en toneladas obtenida en cada parcela con cada tipo de fertilizante fue: soja %&gt;% pander(caption = &quot;Rendimiento en toneladas por parcela en parcelas fertilizadas con cuatro fertilizantes.&quot;) Rendimiento en toneladas por parcela en parcelas fertilizadas con cuatro fertilizantes. ¿Cuál fertilizante conviene utilizar? Parcela Fertilizante.1 Fertilizante.2 Fertilizante.3 Fertilizante.4 1 320 314 165 202 2 244 277 171 121 3 229 244 128 124 4 301 301 209 212 5 293 311 182 208 6 260 292 153 197 7 293 300 200 152 8 295 316 130 212 9 232 235 178 165 10 310 281 181 201 11 322 349 247 218 12 315 306 160 219 16.2 Problema 4 Un biólogo pesquero desea establecer si el descarte de especies asociadas (by- catch) en la pesquería de merluza varía estacionalmente. Para ello realizó un seguimiento de 12 barcos de la flota pesquera comercial, determinando para cada uno de ellos el porcentaje en biomasa del descarte realizado en cada estación del año. ¿Existe una variación estacional en el descarte realizado por la flota merlucera? Sus resultados fueron: pesca %&gt;% pander(caption = &quot;Porcentaje de descarte en las cuatro estaciones.&quot;) Porcentaje de descarte en las cuatro estaciones. Barco Primavera Verano Otono Invierno 1 32 25 43 43 2 27 16 30 38 3 27 30 38 37 4 26 31 51 52 5 28 32 52 44 6 21 23 41 38 7 21 21 44 48 8 24 37 41 46 9 33 29 45 54 10 27 23 45 41 11 35 22 44 43 12 31 35 43 44 "],
["regresion.html", "Capítulo 17 Regresión 17.1 Regresión Lineal Simple 17.2 Regresión con Replicación 17.3 Transformaciones para corregir la falta de linealidad 17.4 Uso de los Residuales para Comprobar los Supuestos 17.5 Regresiones Múltiples", " Capítulo 17 Regresión En las ciencias naturales es usual querer explicar una variable con otras. Las variables que se quieren explicar son las variables dependientes y las que se usan para explicar son las llamadas variables explicatorias o también independientes. A estos modelos se los conoce como modelos de regresión. Aunque las variables estén relacionadas esto no implica que haya una relación causal entre ellas. Sin un modelo causal que explique la manera que las variables se relación se está incurriendo una falacia del tipo cum hoc ergo propter hoc. Por ejemplo, en la Figura 17.1 se muestra que la relación entre los limones frescos importados desde México (ton) y tasa de mortalidad total en autopistas de EE.UU. Según esta regresión al ¡aumentar la importación de disminuye la tasa de mortalidad! Este resultado carece de lógica ya que no hay una forma en que la importación de limones afecte la mortalidad. Por este motivo hay que ser cuidadoso en cuanto a las conclusiones que se realizan con los resultados. (ref:regresion-espuria) Ejemplo de regresión espuria. Limones frescos importados desde México (ton) y tasa de mortalidad total en autopistas de EE.UU. Figura 17.1: (ref:regresion-espuria) 17.1 Regresión Lineal Simple La regresión lineal simple se da cuando hay una variable aleatoria con distribución normal y solo una variable predictora. La varible predictora no es una variable aleatoria, sino que puede ser modificada por el investigador. El objetivo de esta técnica es obtener una ecuación lineal que explique el cambio de la variable aleatoria según el cambio de la variable predictora: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 X_i \\tag{17.1} \\end{equation}\\] Por ejemplo, el se tiene la edad y la longitud de alas de gorriones de varias edades en la Tabla ??. La edad es la variable independiente y la longitud de ala es la variable dependiente. Se busca ajustar un modelo como en el da la Ecuación (17.1). En la Figura 17.2 la recta que mejor ajusta a esto datos en azul. Sin embargo, el ajuste no es perfecto. La diferencia entre el valor ajustado o predicho y el valor observado es el error que se comete. Ese error se simboliza con la letra griega epsilon y debe ser incluido en el modelo: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\tag{17.2} \\end{equation}\\] Se asume que \\(\\epsilon_i\\sim N\\left ( 0, \\sigma^2 \\right )\\) y por lo tanto \\(\\sum \\epsilon_i = 0\\). El método que se usa para encontrar los estimadores de los \\(\\beta_i\\) consiste en minimizar el cuadrado de los errores. Por eso, el método se lo conoce como mínimos cuadrados o mínimos cuadrados ordinarios. Afortunadamente se pueden resolver de manera analítica por las siguientes ecuaciones: \\[\\begin{equation} \\begin{aligned} b_1 &amp;= \\frac{\\sum(X_i-\\overline{X})(Y_i-\\overline{Y})}{\\sum (X_i-\\overline{X})^{2}}=\\frac{\\sum X_iY_i-\\frac{\\left (\\sum X_i\\right)\\left ( \\sum Y_i\\right )}{n}}{\\sum X_i^2-\\frac{\\left (\\sum X_i \\right )^2}{n}}\\\\ b_0 &amp;= \\overline{Y} - b_1\\overline{X} \\end{aligned} \\tag{17.3} \\end{equation}\\] Para facilitar las formulas y hacerlas más compactas definimos: \\[\\begin{equation} \\begin{aligned} \\sum y^2 &amp;= \\sum \\left (Y_i - \\overline{Y} \\right )^2\\\\ \\sum x^2 &amp;= \\sum \\left (X_i - \\overline{X} \\right )^2\\\\ \\sum xy &amp;= \\sum \\left (X_i - \\overline{X} \\right )\\left (Y_i - \\overline{Y} \\right ) \\end{aligned} \\end{equation}\\] Tabla 17.1: Edad en días y longitud de ala en centímetros de 13 gorriones. Edad Longitud 3 1.4 4 1.5 5 2.2 6 2.4 8 3.1 9 3.2 10 3.2 11 3.9 12 4.1 14 4.7 15 4.5 16 5.2 17 5.0 (ref:gorriones-grafico) Gráfico de dispersión de edad en días y longitud de ala en centímetros de 13 gorriones. En azul línea de regresión estimada con mínimos cuadrados ordinarios. En rojo el error cometido por la regresión entre los valores estimados y los valores observados. Figura 17.2: (ref:gorriones-grafico) Una vez que están determinados los \\(\\beta_i\\) es necesario determinar si son signficativos. Es decir, diferentes de cero. El \\(\\beta_1\\) puede ser muy grande (e.g. 100) pero si la varianza asociada a este número es muy grande puede que no sea significativamente distinto de cero. Existen dos estadísticos que permiten probar la significancia de \\(\\beta_1\\). El primero es un estadístico F que se calcula a partir de una tabla de análisis de la varianza (ANOVA): \\[ F = \\frac{CM_{regresion}}{CM_{residual}} \\] Fuente de Variación GL Suma de Cuadrados (SC) Cuadrados Medios (CM) Regresión \\(1\\) \\(\\frac{\\left (\\sum xy \\right )^2}{\\sum x^2}\\) \\(\\frac{SC_{regresion}}{GL_{regresion}}\\) Residual \\(n-2\\) \\(SC_{total}-SC_{regresion}\\) \\(\\frac{SC_{residual}}{GL_{residual}}\\) Total \\(n-1\\) \\(\\sum y^2\\) Este estadístico F bajo la hipótesis nula \\(H_0: \\beta_1 = 0\\) se distribuye como \\(F_{1;n-2}\\). Entonces se compara con esa distribución y si el estadístico es mayor que el valor crítico al \\(\\alpha\\) especificado o si la probabilidad es de un valor mayor a estadístico es menor a \\(\\alpha\\) entonces se rechaza \\(H_0\\). Por ejemplo, con los datos de la tabla 17.1: Fuente de Variación GL Suma de Cuadrados (SC) Cuadrados Medios (CM) F P(&gt;F) Regresión 1 19.1322137 19.1322137 401.0875 0 Residual 11 0.5247093 0.0477008 Total 12 19.6569231 La otra forma de hacer esta prueba es usando el estadístico t: \\[ t = \\frac{b_1 - \\beta_1}{s_{b1}} \\tag{17.4} \\] En el caso de \\(H_0: \\beta_1 = 0\\) es claro que ese el valor que debe tomar \\(\\beta_1\\) en la Ecuación (17.4). Por otro lado, esta forma de probar la significancia del estimador \\(b_1\\) tiene la ventaja de poder probar a una cola izquierda o derecha y con distintos valores de \\(\\beta_1\\) (e.g. 1, 3. 100, etc.). La varianza de \\(b_1\\) se calcula como: \\[ s_b^2=\\frac{s^2_{YX}}{\\sum x^2} \\] Donde el estimador de \\(s^2_{YX}\\) es el \\(CM_{residual}\\) en la tabla de anovo. El coeficiente de determinación \\(r^2\\) es igual a la proporción de la varianza de Y explicada por la regresion: \\[ r^2 = \\frac{SC_{regresion}}{SC{total}} \\] La interpretación de la función de regresión es que \\(\\beta_0\\) es el valor que toma \\(Y\\) cuando \\(X=0\\). No siempre tiene sentido biológico este parámetro. Por ejemplo, si la variable \\(X\\) es el peso de un animal, ¡no existe un animal con peso 0!. Por otro lado, en el caso de los gorriones este valor representa la longitud media del ala de un gorrión al nacer. La interpretación de \\(\\beta_1\\) es cuanto aumenta (o disminuye) \\(Y\\) por cada unidad de \\(X\\). 17.1.1 Intervalos de confianza El intervalo de confianza para \\(b_1\\) es: \\[ b_1 \\pm t_{\\alpha(2);n-2} s_b \\] El intervalo de confianza para cada \\(Y\\) es distinto que el de para una media. Debido a que la pendiente \\(b_1\\) tiene varianza hace que los intervalos de confianza de \\(Y\\) sean mayores en los extremos que en el centro. Podemos imaginarlo como un subibaja donde el punto donde está sostenido es igual a \\(\\overline{X}\\). Por lo tanto la varianza de \\(\\hat{Y}\\) aumenta a medida que se aleja de \\(\\overline{X}\\) \\[ S_{\\hat{Y}}= \\sqrt{S^2_{YX}\\left[ \\frac{1}{n} + \\frac{\\left( X_i - \\overline{X} \\right)^2}{\\sum x^2} \\right ]} \\] Luego se construye normalmente como cualquier otro intervalo de confianza: \\[ \\hat Y \\pm t_{\\alpha(2);n-2}S_{\\hat Y} \\] En cambio, el intervalo de predicción para un nuevo \\(Y\\) depende de la cantidad de nuevas estimaciones que se quieran predecir: \\[ (S_{\\hat{Y}})_m= \\sqrt{S^2_{YX}\\left[\\frac{1}{m} + \\frac{1}{n} + \\frac{\\left( X_i - \\overline{X} \\right)^2}{\\sum x^2} \\right ]} \\] Donde \\(m\\) es el número de nuevas predicciones en en un mismo \\(X_i\\). 17.2 Regresión con Replicación La regresión lineal simple vista anteriormente es para cuando solo hay una observación por cada \\(X_i\\). Cuando tenemos varias observaciones por cada \\(X_i\\) es posible probar la hipótesis sobre la linealidad de los datos. Esto es posible de probar porque cada observación proviene de una distribución normal con media \\(\\beta_0 + \\beta_1X\\). Por lo tanto, la media de las réplicas de cada \\(X_i\\) debería coincidir con el valor de \\(\\hat{Y}\\) de la regresión. Por ejemplo, se tienen mediciones de la presión sistólica de hombres de diferentes edades, con varias mediciones para la misma edad en la Tabla @ref(tab:presión-sistolica). En la Figura ?? se graficaron estos datos. Además, se añadió la recta de regresión estimada. Tabla 17.2: Presión sistólica (mm Hg) en hombres de varias edades. edad presion 30 108, 110, 106 40 125, 120, 118, 119 50 132, 137, 134 60 148, 151, 146, 147, 144 70 162, 156, 164, 158, 159 Entonces, si tenemos réplicas por cada X es posible estimar las desviaciones de la linealidad. La hipótesis nula es que la población es linear vs la alternativa que no es linear: \\[ H_0 : \\text{la población es linear}\\\\ H_a: \\text{la población no es linear} \\] La forma de poner a prueba esta hipótesis es mediante un estadístico F. \\[ F = \\frac{CM_{desviaciones}}{CM_{dentro}} \\] La varianza total puede ser divididad entre la varianza residual o dentro de los grupos, y la varianza entre los grupos que además puede ser divida entre la que explicada por la regresión linear y la que no explicada por la regresión linear que se asume que es debida a desviaciones de la linealidad. Fuente de Variación GL Suma de Cuadrados (SC) Cuadrados Medios (CM) Entre grupos \\(I - 1\\) \\(\\sum\\frac{\\left(\\sum Y_ij\\right)^2}{n_i} - \\frac{\\left (\\sum\\sum Y_ij\\right )}{N}\\) Regresión Lineal \\(1\\) \\(\\frac{\\left (\\sum xy \\right )^2}{\\sum x^2}\\) Desviaciones de la linealidad \\(I - 2\\) \\(SC_{entre} - SC{regresion}\\) \\(\\frac{SC_{desviaciones}}{GL_{desviaciones}}\\) Dentro de grupos $N - I $ \\(SC_{total}-SC_{entre}\\) \\(\\frac{SC_{dentro}}{GL_{dentro}}\\) Total \\(N-1\\) \\(\\sum y^2\\) El estadístico F sigue una distribución F con I - 2 grados de libertad en el numerador y N-I en el denominador si \\(H_0\\) es cierta. Fuente de Variación GL Suma de Cuadrados (SC) Cuadrados Medios (CM) Entre grupos \\(4\\) \\(6751.93\\) Regresión Lineal \\(1\\) \\(6750.29\\) Desviaciones de la linealidad 3 \\(1.64\\) \\(0.55\\) Dentro de grupos \\(14\\) \\(117.27\\) \\(7.82\\) Total \\(19\\) \\(6869.20\\) El valor de \\(F = \\frac{0.55}{7.82} = 0.070 &lt; 1\\). Por lo tanto no se rechaza \\(H_0\\). Luego podemos proceder y comprobar si la regresión es significativa como se hizo para una regresión lineal simple sin replicas en la sección anterior. En R es posible poner a prueba la linealidad de forma simple con una prueba de anova. La formula general que se debe utilizar es: variable_respuesta ~ variable_explicatoria + as.factor(variable_explicatoria) Si no ponemos el segundo término a mano derecha, solo se hará una regresión lineal. Por eso, se agrega la función as.factor() para que utilice la segunda variable explicatoria como variable de agrupamiento y no como variable continua como ya fue utilizada anteriomente. Por ejemplo, para los datos de presión sistólica: summary(aov(presion ~ edad + as.factor(edad), data = presion_long)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## edad 1 6750 6750 863.45 1.13e-14 *** ## as.factor(edad) 3 2 1 0.07 0.975 ## Residuals 15 117 8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 El orden de los factores altera el producto. No es lo mismo poner: variable_respuesta ~ as.factor(variable_explicatoria) + variable_explicatoria Comparase el resultado siguiente con el anterior summary(aov(presion ~ as.factor(edad) + edad, data = presion_long)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(edad) 4 6752 1688.0 215.9 4.62e-13 *** ## Residuals 15 117 7.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ¡Y llegaremos a conclusiones totalmente opuestas! Una vez que no rechazamos la hipotesis de que la población es linear podemos comprobar que la regresion linear es significativa y estimar los parametros. En el el ejemplo: \\[ b = \\frac{\\sum xy}{\\sum x^2} = \\frac{5180}{3975} = 1.3031447 \\text{mm Hg/año}\\\\ a = \\overline{Y} - b \\overline{X} = 137.2- 1.3031447 52.5 = 68.7849057 \\] En R es posible ajustar un modelo lineal con la función lm() que funciona de forma similar a aov() (de hecho son lo mismo con cambios estéticos a la forma de presentar los datos). Necesita una fórmula y la ubicación de los datos. El resultado puedo verse con la función summary() y puede verse abajo. summary(lm(presion ~ edad, data = presion_long)) ## ## Call: ## lm(formula = presion ~ edad, data = presion_long) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0050 -1.9186 -0.4421 2.0264 4.0893 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 68.78491 2.21607 31.04 &lt;2e-16 *** ## edad 1.30314 0.04077 31.97 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.57 on 18 degrees of freedom ## Multiple R-squared: 0.9827, Adjusted R-squared: 0.9817 ## F-statistic: 1022 on 1 and 18 DF, p-value: &lt; 2.2e-16 summary.aov(lm(presion ~ edad, data = presion_long)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## edad 1 6750 6750 1022 &lt;2e-16 *** ## Residuals 18 119 7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Da casi toda la información necesaria. Call da información de como fue ejecutado lm(). Residuals un breve resumen de los residuales, mínimo, primer cuartil, mediana, tercer cuartila y máximo. Coefficients los coeficientes de los parámetros del modelo. (Intercept) es la ordenada al origen y los siguientes coeficientes corresponden a la pendiente de la variable nombrada. Indica el valor estimado, su error estándar, el valor del estadístico t y la probabilidad de un valor mayor. Abajo da también información del error estándar del residual. El r2 múltiple y ajustado. Y el estadístico F con sus grados de libertad y su valor de una probabilidad mayor. 17.3 Transformaciones para corregir la falta de linealidad Muchas veces las relaciones entre \\(X\\) e \\(Y\\) no son lineales. Anteriormente, vimos que se podía transformar \\(Y\\) para cumplir los supuestos de normalidad y homocedasticidad. Ahora veremos que también se puede transformar \\(X\\) para cumplir con la linealidad. En general, la transformación de \\(X\\) no cambia la distribución de \\(Y\\) por lo que puede trasnformarse el primero con impunidad. Sin embargo, hay que tener cuidado con la transformaciones de \\(Y\\). No deben hacerse en caso de que sí se cumplan los supuestos ya que se modificaría la distribución de \\(Y\\) y podrían dejar de cumplir estos supuestos. X Y s2Y 5 10.72, 11.22, 11.75, 12.31 0.4684667 10 14.13, 14.79, 15.49, 16.22 0.8100917 15 18.61, 19.5, 20.4, 21.37 1.4051333 20 24.55, 25.7, 26.92, 28.18 2.4452250 25 32.36, 33.88, 35.48, 37.15 4.2525583 X Y s2logY 5 1.03019, 1.04999, 1.07004, 1.09026 0.000668 10 1.15014, 1.16997, 1.19005, 1.21005 0.000665 15 1.26975, 1.29003, 1.30963, 1.3298 0.000665 20 1.39005, 1.40993, 1.43008, 1.44994 0.000665 25 1.51001, 1.52994, 1.54998, 1.56996 0.000666 17.4 Uso de los Residuales para Comprobar los Supuestos Incluso si no tenemos réplicas para cada \\(X\\) es posible comprobar los supuestos mediante el uso de gráficos de residuales. Una de las formas es graficando los residuos vs los valores ajustados. Si hay homocedasticidad y la regresión lineal es un buen modelo de ajuste a nuestros datos obtendremos un gráfico de ajuste como el que se muestra en la Figura 17.3a. En este gráfico, los residuales muestran la misma varianza a lo largo de la valores de \\(X\\) y están distribuidos de igual manera alrededor del cero. Si no se cumple el supuesto de homocedasticidad, es decir que hay heterocedaticidad, el gráfico de los residuos se verá como en la Figura 17.3b. En este caso la variabilidad de los residuales aumenta a medida que aumentan los valores predichos. También podría darse el caso inverso, que la varianza disminuya a medida que aumente la media, pero es mucho más raro. Si ocurre el caso de la Figura 17.3c es probable que falte una variable importante a nuestro modelo. Finalmente, si ocurre Figura 17.3: Gráficos de algunos de los posibles patrones que se pueden detectar al gráficar los residuales vs los valores ajustados. 17.5 Regresiones Múltiples "],
["ancova.html", "Capítulo 18 ANCOVA", " Capítulo 18 ANCOVA ANOVA (Análisis de la Varianza) y regresión lineal aparentemente no tienen relación entre sí. El primero se ocupa de variables explicatorias cualitativas, mientras que el segundo de variables explicatorias cuantitativas. El análisis de la Covarianza se ocupa de analizar datos con ambos tipos de variables. De esta forma se obtiene una regresión entre la variable de respuesta y la variable explicatoria cuantitativa por cada nivel de la variable explicatoria cualitativa. Supongamos que deseamos modelar el peso de acuerdo a la edad y el sexo. El modelo máximo tiene cuatro parámetros: dos ordenadas al origen, y dos pendientes (una para machos y otra para hembras). \\[ peso_{hembra} = a_{hembra} + b_{hembra} \\times edad\\\\ peso_{macho} = a_{macho} + b_{macho} \\times edad \\] Como debe cumplirse con el principio de parsimonia hay que simplificar el modelo lo más posible. En total hay seis modelos posibles, siendo el más complejo el de cuatro parámetros. Puede ser que los necesitemos todos ellos. El modelo de cuatro parámetros está representado en la Figura 18.1-a. Otra forma puede ser que solo necesitemos un solo parámetro de pendiente como en la Figura 18.1-b. Otra opción, es que la ordenada al origen sean iguales pero la pendientes sean diferentes, como en la Figura 18.1-c. Puede ser que la edad no tenga efecto sobre el peso entonces pero que el sexo si tenga efecto como en la Figura 18.1-d. Entonces este modelo tendrá solo dos parámetros. También puede darse el caso opuesto, que la edad tenga efecto sobre el peso pero no el sexo, como en la Figura 18.1-e. La última opción es que ni la edad ni el sexo tengan efecto sobre el peso, como en la Figura 18.1-f. Este último modelo solo tendrá un parámetro. Figura 18.1: Varios modelos de ANCOVA con un número diferente de parámetros. a) Dos ordenadas al origen y dos pendientes. b) dos ordenadas al origen y una pendiente común. c) Una ordenada al origen y dos pendientes. d) Dos ordenadas al origen, sin pendiente. e) Una ordenada al origen y una pendiente. f) Una ordenada al origen, sin pendiente. Como siempre, las decisión acerca de si conservar ambos parámetros o si son significativos se realiza con pruebas de hipótesis. Por ejemplo, se quiere ver cual es la capacidad de regeneración de las plantas y producir semillas luego del pastoreo. Antes del pastoreo, se registró el tamaño de la planta como el diámetro de la parte superior del rizoma. El pastoreo tiene dos niveles: con pastoreo y sin pastoreo. La respuesta es el peso de las semillas producidas. Es de esperar que las plantas más grandes produzcan más semillas. Y que las plantas pastoreadas produzcan menos semillas que las plantas no pastoreadas. compensation &lt;- read_csv(&quot;data/ipomopsis.csv&quot;) compensation ## # A tibble: 40 x 3 ## Root Fruit Grazing ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6.22 59.8 Ungrazed ## 2 6.49 61.0 Ungrazed ## 3 4.92 14.7 Ungrazed ## 4 5.13 19.3 Ungrazed ## 5 5.42 34.2 Ungrazed ## 6 5.36 35.5 Ungrazed ## 7 7.61 87.7 Ungrazed ## 8 6.35 63.2 Ungrazed ## 9 4.97 24.2 Ungrazed ## 10 6.93 64.3 Ungrazed ## # ... with 30 more rows Los datos tienen tres columnas, la primera Root corresponde al tamaño del rizoma en centímetros. Fruit corresponde al peso de semillas en gramos. Y por último, Grazing corresponde a si las plantas fueron pastoreadas Grazed o no Ungrazed. ggplot(compensation, aes(x = Root, y = Fruit)) + geom_point() Figura 18.2: Diagrama de dispersión entre el tamaño del rizoma y el peso de las semillas La Figura 18.2 muestra que las plantas más grandes produjeron más semillas, tal y como se esperaba. ¿Y qué pasó con el nivel de pastoreo? En la Figura 18.3 los resultados no son tan claros. La línea se corresponde a la media de cada grupo y muestra que ¡Las plantas pastoreadas producen más semillas que las no pasteoreadas! Un resultado completamente sorpredente, y contrario a lo esperado ¿Esto será así realmente o habrá alguna explicación? ggplot(compensation, aes(x = Grazing, y = Fruit)) + geom_point() + stat_summary(fun.y = mean, fun.ymin = mean, fun.ymax = mean, geom = &quot;crossbar&quot;, width = 0.2) Figura 18.3: Datos de niveles de pastoreo y peso de semillas, la barra representa la media. El modelo más complejo incluye cuatro parámetros. Dos ordenadas al origen y dos pendientes. \\[ Fruit_{grazed} = a_{grazed} + b_{grazed} \\times Root\\\\ Fruit_{ungrazed} = a_{ungrazed} + b_{ungrazed} \\times Root \\] Ajustando este modelo en R es parecido a hacer un modelo de regresión usando la función lm() semillas_completo &lt;- lm(Fruit ~ Grazing + Root + Grazing:Root, data = compensation) Solo vamos a ver modelos de ANCOVA simples con una variable cualitativa y otra cuantitativa. Como es el caso de una interacción entre una variable cualitativa y otra cuantitativa, el resultado es que haya varias pendientes. Una por cada nivel de la variable cualitativa. Veamos cual es el resultado de este análisis: summary(semillas_completo) ## ## Call: ## lm(formula = Fruit ~ Grazing + Root + Grazing:Root, data = compensation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.3177 -2.8320 0.1247 3.8511 17.1313 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -125.173 12.811 -9.771 1.15e-11 *** ## GrazingUngrazed 30.806 16.842 1.829 0.0757 . ## Root 23.240 1.531 15.182 &lt; 2e-16 *** ## GrazingUngrazed:Root 0.756 2.354 0.321 0.7500 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.831 on 36 degrees of freedom ## Multiple R-squared: 0.9293, Adjusted R-squared: 0.9234 ## F-statistic: 157.6 on 3 and 36 DF, p-value: &lt; 2.2e-16 Empezando a leer los resultados desde abajo, vemos que el modelo es significativo. Es decir, al menos uno de los parámetros es distinto de 0. Y que ajuste de los datos al modelo parece ser muy bueno, el R2 ajustado es mayor a 90%. Más arriba tenemos los coeficientes del modelo. Recordemos que en R obtenemos los coeficientes obtenemos son solo el valor real para la primera vez que aparece. Luego debemos obtener el valor del parámetro como la suma del parámetro principal y el efecto del nivel. Para aclarar, reemplazando la ecuación con los valores de \\(a\\) y \\(b\\) para cada una de las ecuaciones. El primer valor, (Intercept) corresponde a la ordenada al origen para Grazed Esto es así porque el primer nivel del un factor va a ser el que sea asignado a (Intercept) y va a correponder a nivel de base para la determinación de la pendiente. Por lo tanto, el coeficiente de Root que vemos corresponde también al nivel Grazed \\[ Fruit_{grazed} = -125.173 + 23.24 \\times Root\\\\ \\] Completar la ecuación para el nivel Ungrazed es un poco más complicado porque lo que vemos es el efecto del nivel y no el parámetro. El efecto de los otros niveles aparece como el nombre del factor seguido por el nombre del nivel. En nuestro ejemplo, Grazing es el factor, el nivel es Ungrazed y aparece como GrazedUngrazed en los coeficientes. Por otro lado, el efecto de Ungrazed sobre la pendiente aparece como la interacción entre GrazedUngrazed y Root. \\[ Fruit_{ungrazed} = \\left(-125.173 + 30.806\\right) + \\left( 23.24 + 0.756\\right)\\times Root \\\\ Fruit_{ungrazed} = \\left(-94.367\\right) + \\left( `r23.24 + 0.756`\\right)\\times Root \\\\ \\] Por otro lado, debemos considerar si debemos conservar todos esos parámetros o si podemos poner algún parametro en común. Empezemos por ver si las pendientes son distintas. Esto implica, realizar un nuevo modelo con una pendiente común para las plantas Grazed y Ungrazed: \\[ Fruit_{grazed} = a_{grazed} + b \\times Root\\\\ Fruit_{ungrazed} = a_{ungrazed} + b \\times Root \\] Ajustarlo en R es como hacerlo en el caso anterior pero eliminando la interacción en la formula. semillas_pendiente_comun &lt;- lm(Fruit ~ Grazing + Root, data = compensation) Para comprobar si el modelo más simple debe ser preferido usamos la función anova() para comprobar si el modelo más complejo es significativamente diferente del más simple. De no serlo, debemos descartarlo y quedarnos con el más simple. anova(semillas_completo, semillas_pendiente_comun) ## Analysis of Variance Table ## ## Model 1: Fruit ~ Grazing + Root + Grazing:Root ## Model 2: Fruit ~ Grazing + Root ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 36 1679.7 ## 2 37 1684.5 -1 -4.8122 0.1031 0.75 Vemos que diferencia es muy pequeña, con un estadístico F asociado muy pequeño de 0.1031 y una probablidad de un F mayor de 0.75. Por lo tanto, no hay diferencias significativas entre los modelos, descartando el más complejo. Para ver los coeficientes usamos summary() summary(semillas_pendiente_comun) ## ## Call: ## lm(formula = Fruit ~ Grazing + Root, data = compensation) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.1920 -2.8224 0.3223 3.9144 17.3290 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -127.829 9.664 -13.23 1.35e-15 *** ## GrazingUngrazed 36.103 3.357 10.75 6.11e-13 *** ## Root 23.560 1.149 20.51 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.747 on 37 degrees of freedom ## Multiple R-squared: 0.9291, Adjusted R-squared: 0.9252 ## F-statistic: 242.3 on 2 and 37 DF, p-value: &lt; 2.2e-16 Volviendo a leer los coeficientes vemos que todos son significativos ya que el valor del estadístico t asociado a cada uno es menor al 0.05. Por lo tanto, hay un efecto del tamaño del rizoma que es común a todas las plantas y hay diferencias significativas entre las plantas pastoreados y las no pastoreadas. ¿Cuales producen más semillas? Viendo el signo positivo del efecto de las plantas no pastoreadas (GrazingUngrazed) es posible deducir que ellas son las que producen más semillas. La formula para cada una de las rectas es: \\[ Fruit_{grazed} = -127.829 + 23.56 \\times Root \\\\ Fruit_{ungrazed} = \\left(-127.829 + 36.103\\right) + 23.56 \\times Root \\\\ Fruit_{ungrazed} = \\left(-91.726\\right) + 23.56 \\times Root \\] Figura 18.4: Gráfico de dispersión entre el tamaño del rizoma y el peso de las semillas. Se muestra la línea de regresión estimada por el modelo ANCOVA final. Viendo la Figura 18.4 resulta claro porque el resultado de que las plantas pastoreadas producian más semillas era engañoso. Las plantas pastoreadas eran más grandes, y el tamaño de la planta estaba oscurenciendo el resultado. Si no hubiesemos tenido en cuenta el tamaño, habríamos llegado a la conclusión equivocada. Realizando la prueba estadística sobre el modelo incorrecto llegariamos a la conclusión de que efectivamente el pastoreo aumenta la producción de semillas. anova(lm(Fruit ~ Grazing, data = compensation)) ## Analysis of Variance Table ## ## Response: Fruit ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Grazing 1 2910.4 2910.44 5.3086 0.02678 * ## Residuals 38 20833.4 548.25 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Usando el modelo correcto vemos que la media de las plantas no pastoreadas es mayor que las pastereadas (41.35 vs 77.46): summary(emmeans(semillas_pendiente_comun, ~Grazing)) ## Grazing emmean SE df lower.CL upper.CL ## Grazed 41.35888 1.989014 37 37.32875 45.38900 ## Ungrazed 77.46212 1.989014 37 73.43200 81.49225 ## ## Confidence level used: 0.95 Esto se obtiene usando la formula en la media de Root. \\[ Fruit_{grazed} = -127.829 + 23.56 \\times \\overline{Root} \\\\ Fruit_{ungrazed} = \\left(-91.726\\right) + 23.56 \\times \\overline{Root}\\\\ Fruit_{grazed} = -127.829 + 23.56 \\times 7.18115 \\\\ Fruit_{ungrazed} = \\left(-91.726\\right) + 23.56 \\times 7.18115\\\\ Fruit_{grazed} = 41.358894 \\\\ Fruit_{ungrazed} = 77.461894 \\] "],
["ordenacion-en-espacios-reducidos.html", "Capítulo 19 Ordenación en Espacios Reducidos 19.1 Análisis de componentes principales 19.2 Componentes principales de una matriz de correlación 19.3 ¿Cuantos componentes son significativos? 19.4 Mal uso de los componentes principales", " Capítulo 19 Ordenación en Espacios Reducidos En la Ciencias Naturales, generalmente, se poseen varias variables por cada objeto o unidad. Pero en un diagrama de dispersión es solo posible ver dos dimensiones, como máximo se podrán ver tres. Cuando queremos ver cuáles son las tendencias de variación en los objetos con respecto a todas las variables, estos gráficos se quedan cortos. Podríamos ver cada para de combinaciones, pero resulta tedioso y en general no es muy iluminador. Además, que podemos perdernos algunas relaciones interesantes entre varias variables que se manifiestan en más de dos dimensiones. Lo métodos para ordenación en espacio reducido permiten extraer información sobre la calidad de la proyección y el estudio de las relaciones tanto entre las variables como entre objetos. Existen varios métodos de ordenación resumidos en la tabla de abajo, más otros no incluidos que no vamos a ver en este curso. Método Distancia Preservada Variables Análisis de Componentes Principales Distancia Euclídea Datos Cuantitativos, Relaciones lineales (cuidado con los doble-ceros) Análisis de Coordenadas Principales, Escalamiento (multidimensional) métrico, escalamiento clásico Cualquier medida de distancia Cuantitativos, semicuantitativos, cualitativos, o mezclados Análisis de Correspondencias Distancia \\(\\chi^{2}\\) No-negativos, datos cuantitativos dimensionalmente homogéneos o binarios; abundancia de especies, o datos de presencia/ausencia Los métodos de ordenación pueden usarse para delinear grupos de objetos cuando la estructura de los datos no es continua (las variables si deben ser continuas). En particular, la ordenación puede ser usada siempre para complementar los análisis de agrupamientos. Esto es así porque mientras en análisis de agrupamiento investiga las relacionas finas entre objetos; la ordenación investiga la variabilidad entera de los datos y extrae los gradientes generales. En general, se usa la ordenación para estudiar las posiciones relativas de los objetos en un espacio reducido, es decir pasar de un espacio multidimensional a dos o tres dimensiones. Cuando la proyección de los datos en un espacio reducido representa una gran proporción de la variabilidad las distancias entre los objetos sean similares a la que existen en un espacio multidimensional. Cuando las proyecciones no son tan eficientes, la distancia entre los objetos es menor que en el espacio multidimensional. Se pueden dar dos casos: (1) que los objetos estén a distancias proporcionalmente similares en los dos espacios, entonces la proyección seguirá siendo útil (2) que las posiciones relativas de los objetos cambien entre los dos espacios, entonces la proyección es inútil. Por lo tanto, a veces es útil considera la ordenación aun cuando esta represente una pequeña parte de la variación total. 19.1 Análisis de componentes principales Supongamos que tenemos una distribución multivariada normal, el primer eje principal es la línea que atraviesa la mayor dimensión del elipsoide de densidad que describe la densidad. De la misma manera, los siguientes ejes principales (ortogonales entre sí, es decir en ángulo recto e incorrelados, y sucesivamente más cortos) atraviesan las siguientes dimensiones del elipsoide \\(p\\)-dimensional. Por lo tanto, pueden encontrarse un número \\(p\\) de ejes principales de una matriz de datos de \\(p\\) variables. Las relaciones entre las variables pueden representarse con una matriz cuadrada \\(\\mathbf{S}_{p \\times p}\\): \\[ \\mathbf{S} = \\begin{bmatrix} \\sigma_{11} &amp; \\sigma_{12} &amp; \\cdots &amp; \\sigma_{1p} \\\\ \\sigma_{21} &amp; \\sigma_{22} &amp; \\cdots &amp; \\sigma_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{p1} &amp; \\sigma_{p2} &amp; \\cdots &amp; \\sigma_{pp} \\\\ \\end{bmatrix} \\] Donde la diagonal es la varianza de una variable y fuera de ellas se encuentran las covarianzas Los ejes principales de una matriz de dispersión \\(\\mathbf{S}\\) pueden encontrarse resolviendo la ecuación: \\[ \\left( \\mathbf{S} - \\lambda_{k}\\mathbf{I} \\right)\\mathbf{u}_{k} = 0 \\] Cuya ecuación característica es: \\[ \\left| \\mathbf{S} - \\lambda_{k}\\mathbf{I} \\right| = 0 \\] Se usa para computar los autovalores. Los autovectores \\(\\mathbf{u}_{k}\\) asociados a los \\(\\lambda_{k}\\) se encuentran poniendo los distintos valores de \\(\\lambda_{k}\\) en la primera ecuación. Estos autovectores son los ejes principales de la matriz de dispersión \\(\\mathbf{S}\\). Los componentes principales tienen las siguientes propiedades: Dado que cualquier matriz de dispersión \\(\\mathbf{S}\\) es simétrica, sus ejes principales \\(u_{k}\\) son ortogonales entre sí. Es decir, que representan direcciones linealmente independientes en el elipsoide de densidad de la distribución de objetos. Los autovalores \\(\\lambda_{k}\\) de una matriz de dispersión \\(\\mathbf{S}\\) dan la cantidad de varianza que corresponde a cada uno de los sucesivos ejes principales. Dadas las dos primeras propiedades, el análisis de componentes principales puede resumir, en unas pocas dimensiones, la mayor parte de la variabilidad de una matriz de dispersión con un gran número de variables. También provee de una medida de la variabilidad explicada por cada uno de esos pocos ejes principales independientes. Un ejemplo sencillo usando solo dos variables, algo que en la práctica nunca sucede, pero resulta útil como ejemplo \\(Y = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; 4 \\\\ 5 &amp; 0 \\\\ 7 &amp; 6 \\\\ 9 &amp; 2 \\\\ \\end{bmatrix}\\) luego de centrar con las medias de las columnas \\(\\lbrack y - \\overline{y} \\rbrack = \\begin{bmatrix} - 3.2 &amp; - 1.6 \\\\ - 2.2 &amp; 1.4 \\\\ - 0.2 &amp; - 2.6 \\\\ 1.8 &amp; 3.4 \\\\ 3.8 &amp; - 0.6 \\\\ \\end{bmatrix}\\) Calculando la matriz de dispersión: \\[ S = \\frac{1}{n - 1}\\left\\lbrack y - \\overline{y} \\right\\rbrack^{&#39;}\\left\\lbrack y - \\overline{y} \\right\\rbrack = \\begin{bmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\\\ \\end{bmatrix} \\] La ecuación característica correspondiente es: \\[ \\left| S - \\lambda_{k}I \\right| = \\left| \\begin{bmatrix} 8.2 &amp; 1.6 \\\\ 1.6 &amp; 5.8 \\\\ \\end{bmatrix} - \\begin{bmatrix} \\lambda_{k} &amp; 0 \\\\ 0 &amp; \\lambda_{k} \\\\ \\end{bmatrix} \\right| = 0 \\] Tiene dos autovalores, \\(\\lambda_{1} = 9\\) y \\(\\lambda_{2} = 2\\). La varianza total es la misma, pero particionada de otra manera. La suma de la varianza en la diagonal de \\(\\mathbf{S}\\) es \\(8.2 + 5.8 = 14\\), y la suma de los dos autovalores es \\((9 + 5) = 14\\). El primer componente principal tiene el 64.3% de la varianza (\\(\\lambda_{1} = 9)\\) y el segundo el resto, 35.7%. Hay tantos autovalores como variables, pero cada autovalor tiene cada vez menos varianza. Con los valores de \\(\\lambda_{k}\\) podemos calcular los autovectores con la ecuación: \\[ \\left( \\mathbf{S} - \\lambda_{k}\\mathbf{I} \\right)\\mathbf{u}_{k} = 0 \\] Una vez que los vectores son normalizados (i.e. la longitud es uno, \\(\\mathbf{u}^{&#39;}\\mathbf{u} = 1\\)) se convierten en columnas de la matriz \\(\\mathbf{U}\\): \\[ U = \\begin{bmatrix} 0.8944 &amp; - 0.4472 \\\\ 0.4472 &amp; 0.8944 \\\\ \\end{bmatrix} \\] Los signos de la matriz son totalmente arbitrarios, si se multiplica por -1 se consigue una imagen especular que es igual de buena representando los datos. Los autovectores son ortogonales entre sí (incorrelados). Podemos comprobarlo con su producto cruzado \\(\\mathbf{u}_{1}^{&#39;}\\mathbf{u}_{2} = \\left( 0.8944 \\times \\left( - 0.4472 \\right) \\right) + \\left( 0.4472 \\times 0.8944 \\right) = 0\\). Además, los elementos de \\(\\mathbf{U}\\) son los cosenos del ángulo entre las variables originales. Usando esta propiedad, se puede ver que los ejes principales especifican una rotación de los ejes de \\(\\left( \\operatorname{}{0.8944} \\right) = 2634^{&#39;}\\). Los elementos de los autovectores también son pesos (loadings) de las variables originales. Por lo tanto, la posición del objeto \\(x_{i}\\) en el primer eje principal está dada por la siguiente función o combinación linear: \\[ f_{i1} = \\left( y_{i1} - {\\overline{y}}_{1} \\right)u_{11} + \\ldots + \\left( y_{ip} - {\\overline{y}}_{p} \\right)u_{p1} = \\left\\lbrack y - \\overline{y} \\right\\rbrack_{i}\\mathbf{u}_{1} \\] Los valores de \\(\\left( y_{ij} - {\\overline{y}}_{j} \\right)\\) son los valores del objeto \\(i\\) en las variables \\(j\\) centrados y los valores de \\(u_{i1}\\) son lo pesos de las variables en el primer autovector. Las posiciones de los objetos con respecto a los ejes principales están dadas en la matriz F de variables transformadas. Es llamada matriz de valores de componentes: \\[ \\mathbf{F} = \\left\\lbrack y - \\overline{y} \\right\\rbrack\\mathbf{U} \\] Para el ejemplo esto sería: \\[ \\mathbf{F} = \\begin{bmatrix} - 3.2 &amp; - 1.6 \\\\ - 2.2 &amp; 1.4 \\\\ - 0.2 &amp; - 2.6 \\\\ 1.8 &amp; 3.4 \\\\ 3.8 &amp; - 0.6 \\\\ \\end{bmatrix}\\begin{bmatrix} 0.8944 &amp; - 0.4472 \\\\ 0.4472 &amp; 0.8944 \\\\ \\end{bmatrix} = \\begin{bmatrix} - 3.578 &amp; 0 \\\\ - 1.342 &amp; 2.236 \\\\ - 1.342 &amp; - 2.236 \\\\ 3.130 &amp; 2.236 \\\\ 3.130 &amp; - 2.236 \\\\ \\end{bmatrix} \\] En este caso simple, con solo dos variables, los componentes principales son una representación perfecta de la variabilidad y es solo una rotación de estas dos variables. Cuando hay más de dos variables, como es usual, el análisis de componentes principales también realiza una rotación del sistema de variables-ejes, pero en un espacio multidimensional. En este caso, los componentes principales I y II definen un plano que permite la representación de la mayor variabilidad. Los objetos son proyectados en este plano de tal forma que conserven, lo más posible, las distancias euclídeas que tienen en el espacio multidimensional de las variables originales. La posición relativa de los objetos en el espacio p-dimensional rotado de los componentes principales son las mismas que en el espacio p-dimensional de las variables originales. Esto significa que las distancias euclídeas entre los objetos se conservan a través de la rotación de los ejes. Esta es una de las propiedades más importantes de análisis de componentes principales. La calidad de la representación en un espacio euclídeo reducido con m dimensiones puede ser estudiada con la relación \\[ \\left( \\sum_{k = 1}^{m}{\\ \\lambda_{k}} \\right)/\\left( \\sum_{k = 1}^{p}{\\ \\lambda_{k}} \\right) \\] Esta relación es equivalente al coeficiente de determinación (\\(R^{2}\\)) en el análisis de regresión. El análisis componentes principales se pueden usar para estudiar el rol de las variables en la conformación de los componentes principales. Esto se puede ver en varias maneras: matriz de autovectores, proyección en un espacio reducido (matriz \\(\\mathbf{U}\\mathbf{\\Lambda}^{1/2}\\)), y proyección en un espacio reducido (matriz U) La matriz de autovectores – Como la matriz U contiene los autovectores normalizados, la diagonal \\(\\mathbf{U}&#39;\\mathbf{U}\\) es igual 1 y los elementos fuera de la marginal es igual 0 porque los autovectores son ortogonales entre sí. \\[ \\mathbf{U}^{&#39;}\\mathbf{U} = \\mathbf{I} \\] Por lo tanto, las variables tienen longitud de unidad en el espacio multidimensional y están a 90° entre sí (ortogonales) dado que los autovectores son ortogonales entre sí. Esto es así porque el análisis de componentes es una rotación en el espacio multidimensional. Además, al normalizar los autovectores normaliza los ejes de las variables: \\[ \\begin{aligned} \\mathbf{U} = &amp;\\begin{bmatrix} u_{11}\\hphantom{0000} &amp; \\hphantom{00}\\cdots &amp; \\hphantom{000000}u_{1p}\\hphantom{000000} \\\\\\\\ \\vdots\\hphantom{0000} &amp; \\ &amp; \\hphantom{00000}\\vdots\\hphantom{000000} \\\\\\\\ u_{p1}\\hphantom{0000} &amp; \\hphantom{00}\\cdots &amp; \\hphantom{000000}u_{pp}\\hphantom{000000} \\\\ &amp;\\end{bmatrix} \\begin{matrix} \\sqrt{\\sum u_{1k}^{2}} = 1 \\\\\\\\ \\vdots \\\\\\\\ \\sqrt{\\sum u_{pk}^{2}} = 1 \\\\ \\end{matrix} \\\\ &amp;\\begin{matrix} \\sqrt{\\sum u_{j1}^{2}} = 1 &amp; \\cdots&amp;&amp; \\sqrt{\\sum u_{jp}^{2}} = 1 &amp; \\\\ &amp;\\end{matrix} \\\\ \\end{aligned} \\] Otra forma de estudiar la relación entre los predictores consiste en escalar los autovectores de tal forma que los cosenos de los ángulos entre los ejes de las variables sean proporcionales a su covarianza. Se logra escalando cada autovector \\(k\\) a una longitud igual a su desvío estándar \\(\\sqrt{\\lambda_{k}}\\). La distancia euclídea entre objetos no se conserva de esta manera. Usando la matriz diagonal de autovalores \\(\\mathbf{\\Lambda}\\) se puede computar la nueva de autovectores \\(\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\): \\[ \\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\mathbf{=}\\begin{bmatrix} 0.8944 &amp; - 0.4472 \\\\ 0.4472 &amp; 0.8944 \\\\ \\end{bmatrix}\\begin{bmatrix} \\sqrt{9} &amp; 0 \\\\ 0 &amp; \\sqrt{5} \\\\ \\end{bmatrix} = \\begin{bmatrix} 2.6633 &amp; - 1.000 \\\\ 1.3416 &amp; 2.000 \\\\ \\end{bmatrix} \\] En este escalamiento, la relación entre las variables es la misma que en la matriz de dispersión \\(\\mathbf{S}\\) \\[ \\begin{aligned} {\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}} = &amp;\\begin{bmatrix} \\hphantom{00}u_{11}\\sqrt{\\lambda_{1}}\\hphantom{0000000} &amp; \\hphantom{000}\\cdots &amp; \\hphantom{000000}u_{1p}\\sqrt{\\lambda_{p}}\\hphantom{000000000} \\\\\\\\ \\vdots\\hphantom{0000} &amp; \\ &amp; \\hphantom{00000}\\vdots\\hphantom{00000000} \\\\\\\\ u_{p1}\\sqrt{\\lambda_{1}}\\hphantom{0000} &amp; \\hphantom{000}\\cdots &amp; \\hphantom{000000}u_{pp}\\sqrt{\\lambda_{p}}\\hphantom{000000000} \\\\ &amp;\\end{bmatrix} \\begin{matrix} \\sqrt{\\sum\\left( u_{1k}\\sqrt{\\lambda_{k}} \\right)^{2}} = s_{1} \\\\ \\\\ \\vdots \\\\ \\\\ \\sqrt{\\sum\\left( u_{pk}\\sqrt{\\lambda_{k}} \\right)^{2}} = s_{p} \\\\ \\end{matrix} \\\\ &amp;\\begin{matrix} \\sqrt{\\sum\\left( u_{j1}\\sqrt{\\lambda_{1}} \\right)^{2}} = \\sqrt{\\lambda_{1}} &amp; \\cdots &amp; \\sqrt{\\sum\\left( u_{jp}\\sqrt{\\lambda_{p}} \\right)^{2}} = \\sqrt{\\lambda_{p}} \\\\ &amp;\\end{matrix} \\\\ \\end{aligned} \\] Proyección de las variables en un espacio reducido (matriz \\(\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\)): Lo interesante no es todo el espacio multidimensional, sino la proyección simplificada del mismo en un espacio reducido, en general dos dimensiones. Los elementos \\(u_{jk}\\sqrt{\\lambda_{k}}\\) de los autovectores, escalados en \\(\\sqrt{\\lambda_{k}}\\), son coordenadas de la proyección de las \\(j\\) variables en los diferentes ejes \\(k\\). Se grafican como flechas ya que son ejes. Sus proyecciones son más cortas o iguales que sus longitudes en el espacio multidimensional. Los ángulos entre las variables son proyecciones de sus verdaderos ángulos de covarianza. Por lo tanto, es importante solo considerar variables que estén bien representadas en el plano de proyección. Una forma de evaluarlo es verificando la longitud de la proyección. En un espacio reducido la longitud de la proyección del eje de la variable es \\(s_{j}\\sqrt{d/p\\ }\\). Esta expresión define la contribución en equilibrio de la variable en los varios ejes del espacio multidimensional. Esta medida nos sirve para comparar con la longitud de la variable y ver si su contribución en mayor o menor de lo esperado bajo la hipótesis de contribuciones iguales en todos los ejes principales. Para nuestro ejemplo, la longitud de las variables: \\[ \\text{longitud de la primera variable} = \\sqrt{{2.6833}^{2} + \\left( - 1.000 \\right)^{2}} = 2.8636 \\] \\[ \\text{longitud de la segunda variable} = \\sqrt{{1.3516}^{2} + {2.000}^{2}} = 2.4083 \\] Como el ejemplo tiene solo dos variables, las longitudes son iguales a las contribuciones en equilibrio: \\[ \\text{proyeccion en equilibrio de la variable 1} = s_{1}\\sqrt{2/2} = \\sqrt{8.2}\\sqrt{2/2} = 2.8636 \\] \\[ \\text{proyeccion en equilibrio de la variable 2} = s_{2}\\sqrt{2/2} = \\sqrt{5.8}\\sqrt{2/2} = 2.4083 \\] La matriz de correlación \\(\\mathbf{R}\\) está conectada a la matriz de dispersión \\(\\mathbf{S}\\) por la diagonal de matriz de desvíos estándar \\(\\mathbf{D}(s)\\). El coseno del ángulo \\(\\alpha_{jl}\\) entre dos variables \\(y_{j}\\) e \\(y_{l}\\) en el espacio multidimensional, está por lo tanto relaciona a la correlación (\\(r_{jl}\\)); puede demostrarse que \\(\\cos\\left( \\alpha_{jl} \\right) = r_{jl}\\). Este ángulo es igual a la covarianza, porque la estandarización cambió las longitudes de las variables a la unidad. En el ejemplo, la correlación entre las dos variables es igual a \\(\\frac{1.6}{\\sqrt{8.2 \\times 5.8}} = \\ 0.232\\) . El ángulo correspondiente es \\(\\operatorname{}{0.232} = 7635&#39;\\). Igualmente, el ángulo entre la variable \\(j\\) y el eje principal \\(k\\), en el espacio multidimensional, es el arco coseno de la correlación entre la variable \\(j\\) y el eje principal \\(k\\). Esta correlación es el elemento \\(jk\\) de la nueva matriz de autovectores: \\[ u_{jk}\\sqrt{\\lambda_{k}}/s_{j} \\] Es decir que la correlación es calculada pesando cada elemento de los autovectores por la relación del desvío estándar del eje principal al de la variable. Para el ejemplo, los desvíos estándar \\(s_{1} = 2.8636\\ s_{2} = 2.4083\\): \\[ \\left\\lbrack \\frac{u_{jk}\\sqrt{\\lambda_{k}}}{s_{j}} \\right\\rbrack = \\begin{bmatrix} 0.9370 &amp; - 0.3492 \\\\ 0.5571 &amp; 0.8305 \\\\ \\end{bmatrix}\\begin{bmatrix} 2026&#39; &amp; 11026&#39; \\\\ 5609&#39; &amp; 3351&#39; \\\\ \\end{bmatrix} \\] Una consecuencia importante de esto es que las variables con correlación más alta, en valor absoluto, son las que más contribuyen a cada autovector. Sin embargo, no se puede hacer la prueba estadística de Pearson para los coeficientes de correlación porque los componentes principales son combinaciones lineales de las variables. Cuando los ejes de las variables de \\(\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\) están estandarizados a longitud unidad sus proyecciones en el espacio principal no es recomendada porque los autovectores re-escalados no son necesariamente ortogonales y pueden tener cualquier longitud. La matriz de las proyecciones de los ejes de las variables de la matriz puede ser examinada con respecto a los siguientes puntos: Las proyecciones de las coordenadas de los ejes de las variables especifican la posición de los ápices de ese eje de variable en el espacio reducido. Se recomienda usar flechas para representarlos. La proyección de los \\(u_{jk}\\sqrt{\\lambda_{k}}\\) del eje de la variable \\(j\\) en el eje principal \\(k\\) muestra su covarianza con respecto al eje principal, y su signo. Verificar variables cuyas longitudes proyectadas alcancen o excedan los valores de sus respectivas contribuciones en equilibrio. Los ejes de variables que sean claramente más cortos que esto contribuyen poco a la formación del espacio reducido bajo estudio y, por lo tanto, contribuyen poco a la estructura que se puede ser encontrada para los objetos en ese espacio reducido. La correlación entre las variables está dada por el ángulo entre los ejes de las variables y no por la proximidad entre los ápices de los ejes. Hay que tener en cuenta que, la proyección de los ejes en el espacio reducido puede no ser la representación completa de la correlación espacio multidimensional. Puede ser mejor agrupar variables, en el espacio reducido del gráfico, con respecto al espacio multidimensional, realizando un método de análisis de agrupamiento. Los objetos pueden ser proyectados en ángulo recto sobre los ejes de las variables de acuerdo a sus valores en esas variables. Sin embargo, las distancias entre los objetos no son aproximaciones de sus distancias euclídeas. Proyección de las variables en un espacio reducido (matriz \\(\\mathbf{U}\\)): Difiere de lo anterior en los autovectores no han sido escalados a las longitudes de sus desvíos estándar. Los ángulos entre los ejes de las variables y los ejes principales son proyecciones de sus ángulos de rotación. Por ejemplo: \\[ U = \\begin{bmatrix} 0.8944 &amp; - 0.4472 \\\\ 0.4472 &amp; 0.8944 \\\\ \\end{bmatrix}\\begin{bmatrix} 2634&#39; &amp; 11634&#39; \\\\ 6326&#39; &amp; 2634&#39; \\\\ \\end{bmatrix} \\] En esta proyección no es posible interpretar las correlaciones entre las variables ya que siempre son ortogonales en esta representación, donde los autovectores están escalados a 1. La proyección de \\(u_{jk}\\) de una variable \\(j\\) en el eje principal \\(k\\) es proporcional a la covarianza de ese descriptor con el eje principal. Se puede comparar la proyección de diferentes ejes de variables en el mismo eje principal. Se puede probar que una proyección isogonal (con ángulos iguales) de \\(p\\) ejes ortogonales de longitud uno da una longitud de \\(\\sqrt{\\frac{d}{p}}\\) en cada eje de un espacio \\(d\\)-dimensional. Se puede dibujar un círculo de equilibrio de las variables como referencia para evaluar la contribución de cada variable a la conformación del espacio reducido. Variable centrada \\(j\\) Escalado de los autovectores \\(\\sqrt{\\lambda_{k}}\\) 1 Longitud total s_{j} 1 Ángulos en el espacio reducido Proyección de las covarianzas (correlaciones) 90°, rotaciones rígidas del sistema de ejes Longitud de la contribución en equilibrio \\(s_{j}\\sqrt{d/p}\\) Círculo con radio \\(\\sqrt{\\frac{d}{p}}\\) Proyección en el eje principal \\(k\\) \\(u_{jk}\\sqrt{\\lambda_{k}}\\) (la covarianza con el componente k) \\(u_{jk}\\) (proporcional a la covarianza con \\(k\\)) Correlación con el eje principal \\(k\\) \\(u_{jk}\\sqrt{\\lambda_{k}}/s_{j}\\) \\(u_{jk}\\sqrt{\\lambda_{k}}/s_{j}\\) 19.1.1 Biplots Se le llama biplot al gráfico de componentes principales en donde se grafican al mismo tiempo los ejes de las variables y los objetos en el espacio reducido. Existen dos tipos de gráficos biplots según el escalamiento que se use: los de distancia se hacen con la yuxtaposición de la matriz \\(\\mathbf{U}\\) (los autovectores escalados a longitud unidad) y \\(\\mathbf{F}\\) (donde cada componente principal \\(k\\) está escalado a la varianza = \\(\\lambda_{k}\\)), los de correlación usan la matriz \\(\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\) para las variables (cada autovector escalado a longitud \\(\\sqrt{\\lambda_{k}}\\)) y la matriz \\(\\mathbf{G} = \\mathbf{F}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\) para los objetos, cuyas columnas tienen varianzas de unidad. Las principales propiedades de los biplot de distancias son: La distancia entre los objetos del biplot son aproximaciones de sus distancias euclídeas en el espacio multidimensional. La proyección del objeto en ángulo recto sobre la variable da la posición aproximada del objeto en esa variable. Dado que las variables tienen longitud 1 en el espacio multidimensional, la longitud de su proyección sobre el espacio reducido indica cuanto contribuye a la formación de ese espacio. Los ángulos entre los vectores de las variables no tienen interpretación En los biplots de correlación: Las distancias entre los objetos del biplot no son aproximaciones de sus distancias euclídeas en el espacio multidimensional. La proyección del objeto en ángulo recto sobre la variable da la posición aproximada del objeto en esa variable. Dado que la longitud de las variables es \\(s_{j}\\) en un espacio multidimensional completo, la longitud de la proyección de la variable en el espacio reducido es una aproximación de su desvío estándar. Lo ángulos entre las variables reflejan su correlación. En cualquiera de los dos casos, los objetos o variables pueden ser multiplicados por una constante para producir un gráfico claro. 19.2 Componentes principales de una matriz de correlación También puede realizar este análisis sobre una matriz \\(\\mathbf{R}\\) de correlación, ya que las correlaciones son las covarianzas estandarizadas de las variables. La suma de autovalores de \\(S\\) es igual a la suma de varianzas, mientras que la suma de autovalores de \\(\\mathbf{R}\\) es igual \\(p\\), por lo que los autovalores y por lo tanto los autovectores son diferentes. Esto se debe a que las distancias entre los objetos no son las mismas en los dos casos. En el caso de las correlaciones, las variables están estandarizadas. Por lo tanto, las distancias entre los objetos son independientes de las unidades de medición, por otro lado, las que están en el espacio original de medida cambian de acuerdo a su cambio en unidad de medida. Cuando todas las variables son del mismo orden de magnitud y tienen las mismas unidades conviene usar la matriz \\(\\mathbf{S}\\). En ese caso, los autovectores y los coeficientes de correlación entre las variables y los componen proporcionan información complementaria. El primero da la ponderación de las variables y el segundo cuantifica su importancia relativa. Cuando las variables son de naturaleza diferente, puede ser necesario usar la matriz \\(\\mathbf{R}\\) en vez de \\(\\mathbf{S}\\). ¿Cuándo usar una \\(\\mathbf{S}\\) o \\(\\mathbf{R}\\)? Si uno quiere agrupar los objetos en el espacio reducido ¿El agrupamiento debe hacerse con respecto a las variables originales, por lo tanto, preservando sus diferencias en magnitud? ¿O las variables deberían contribuir de igual forma al agrupamiento de los objetos, independientemente de su varianza? En el segundo caso uno debería proceder con la matriz de correlación Otra forma de ver esto es: Considere que la distancia euclídea es la que se conserva entre los objetos con el análisis de componente principales. ¿Qué es más interesante de interpretar en términos de la configuración espacial de las distancias euclídeas? La covarianza (los datos crudos) o las correlaciones (los datos estandarizados) Igual que con el caso anterior, el análisis de componentes principales es una rotación del sistema de ejes. Pero, como ahora las variables están estandarizadas, los objetos no están posicionados de la misma forma que si las variables fuesen solo centradas. Las conclusiones de lo visto anteriormente no cambian, solo hay que reemplazar matriz de dispersión \\(\\mathbf{S}\\) por matriz de correlación \\(\\mathbf{R}\\), covarianza por correlación y \\(s_{jl}\\) por \\(r_{jl}\\). Por lo tanto, las varianzas y desvíos estándar son iguales a 1. Lo que da lugar a ciertas propiedades especiales para la matriz \\(\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\). Primero, \\(\\mathbf{D}\\left( s \\right) = \\mathbf{I}\\), por lo que \\(\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}} = \\mathbf{D}\\left( s \\right)^{- 1}\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\), esto significa que los coeficientes \\(u_{jk}\\sqrt{\\lambda_{k}}\\) son los coeficientes de correlación entre las variables \\(j\\) y los componentes \\(k\\). La contribución en equilibrio, en el espacio reducido de \\(\\mathbf{U}\\mathbf{\\Lambda}^{\\frac{1}{2}}\\), es \\(s_{j}\\sqrt{\\frac{d}{p}} = \\sqrt{\\frac{d}{p}}\\) (\\(s_{j} = 1\\)). Por lo tanto, es posible juzgar si la contribución de una variable es mayor o menor a lo esperado comparando la longitud de las proyecciones a un círculo de equilibrio con radio \\(\\sqrt{\\frac{d}{p}}\\). Las propiedades principales de un variable estandarizada se dan en la tabla de abajo. Variable estandarizada \\(j\\) Escalado de los autovectores \\(\\sqrt{\\lambda_{k}}\\) 1 Longitud total 1 1 Ángulos en el espacio reducido Proyección de las correlaciones 90°, rotaciones rígidas del sistema de ejes Longitud de la contribución en equilibrio \\(\\sqrt{d/p}\\) Círculo con radio \\(\\sqrt{\\frac{d}{p}}\\) Proyección en el eje principal \\(k\\) \\(u_{jk}\\sqrt{\\lambda_{k}}\\) (la covarianza con el componente k) \\(u_{jk}\\) (proporcional a la covarianza con \\(k\\)) Correlación con el eje principal \\(k\\) \\(u_{jk}\\sqrt{\\lambda_{k}}\\) \\(u_{jk}\\sqrt{\\lambda_{k}}\\) 19.3 ¿Cuantos componentes son significativos? Una propiedad de los componentes es que cada uno representa una cantidad cada vez menor de la varianza total. Por lo tanto, un problema es cuantos componentes son significativos en términos biológicos. La misma pregunta vista de otra manera es, cuantas dimensiones tendría que tener el espacio reducido. La mejor manera de ver esto es con diagrama de Shepard. Sin embargo, dado que el análisis de componentes principales es una forma de partición de la varianza uno podría realizar una prueba formal para la varianza asociada con los sucesivos ejes principales. Existen varias pruebas clásicas para contestar esta pregunta, pero el problema que tienen es que requieren normalidad en las variables, una condición que rara vez se cumple por datos ecológicos. Hay una regla empírica que sugiere que solo se deben interpretar los componentes principales si su autovalor \\(\\lambda\\) es mayor que la media de los \\(\\lambda\\). Este es llamado el criterio Kaiser-Guttman. Otra forma, también empírica, es comparar los valores decrecientes de los autovalores con los valores de modelo de bastón roto. Considere que la varianza es un recurso embebida en un bastón de longitud 1. Si los componentes principales dividieran la varianza al azar entre los ejes principales, las fracciones de la varianza explicada por los ejes principales tendría la misma longitud relativa que las piezas obtenidas al romper el bastón en puntos al azar en tantas piezas como ejes. Si un bastón de unidad es roto en al azar en \\(p = 2,\\ 3,\\ \\ldots\\) piezas, los valores esperados (E) de las longitudes relativas de las piezas sucesivamente menores (j) están dados por la ecuación: \\[ E\\left( \\text{pieza}_{j} \\right) = \\frac{1}{p}\\sum_{x = j}^{p}\\frac{1}{x} \\] Los valores esperados son iguales a la media de las longitudes que fuesen obtenidas al romper el bastón al azar muchas veces y calcular la media de la pieza más grande, la segunda más grande, etc. No tendría sentido interpretar los ejes principales que explican una fracción de la varianza menor o igual que la predicha por el modelo del bastón roto. Puede comprobarse que ejes deben interpretarse consultando una tabla y seleccionando los autovalores que son mayores que las predicciones del modelo. O comparar la suma de los autovalores de 1 hasta \\(k\\) con la suma de los valores de 1 hasta \\(k\\) en el modelo. Esta prueba generalmente selecciona los primeros dos o tres componentes principales. 19.4 Mal uso de los componentes principales Los errores más comunes son: uso de las variables para las cuales la covarianza no tiene sentido y la interpretación de la relación entre variables, en el espacio reducido, basa en las posiciones relativas de los ápices de los ejes en vez de los ángulos entre ellas. El análisis de componentes principales fue definido originalmente para el estudio de datos con distribución multinormal, por lo que para usarlo óptimamente es necesario normalizar los datos. Las desviaciones de la normalidad no afectan necesariamente el análisis. Hay que tener cuidado con las distribuciones sesgadas, los primeros ejes principales solo van a separar los pocos objetos con valores extremos del resto, en vez de mostrar los ejes principales de variación de todos los objetos en estudio. El método debe ser usado con una matriz de varianzas o correlaciones con las siguientes propiedades: a) la matriz S o R ha sido calculada entre variables b) que son cuantitativos y c) para las cuales estimadores validos de la covarianza pueden ser obtenidos. Esto se viola bajo las siguientes condiciones: Una matriz de dispersión no puede ser estimada cuando el número de observaciones n es menor o igual al número de variables p. El número de objetos de ser mayor a p para obtener estimadores validos de la matriz de dispersión. Sin embargo, los primeros ejes principales son poco afectados por cuando la matriz no es rango completo. Por lo que no debería haber interpretaciones incorrectas de las ordenaciones en espacio reducido. Algunos autores han transpuesto las matriz original y computado correlaciones entre objetos en vez de entre variables. Esto no tiene sentido porque el análisis produce información tanto de los objetos como de las variables. Además, la covarianza entre objetos no tiene sentido. Y la correlación implica estandarización de los vectores, y solo tiene sentido para datos dimensionalmente homogéneos. Las covarianzas y correlaciones solo están definidas para variables cuantitativas. Sin embargo, el análisis de componentes principales es muy robusto a variaciones de precisión de los datos. Las variables pueden ser recodificadas en pocas clases sin cambiar notablemente los resultados. Los coeficientes de correlación usando datos semicuantitativos son equivalentes al coeficiente de correlación de rangos de Spearman. Cuando se calcula en conjuntos de datos con muchos doble ceros, los coeficientes de covarianza o correlación dan ordenaciones que producen estimadores inadecuados de las distancias entre objetos. Con este tipo de datos solo se debe usar componentes principales en gradientes pequeños. "]
]
